{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/don/ML-tests/lib/python3.8/site-packages/haiku/_src/data_structures.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.\n",
      "  PyTreeDef = type(jax.tree_structure(None))\n",
      "/home/don/ML-tests/lib/python3.8/site-packages/chex/_src/pytypes.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.\n",
      "  PyTreeDef = type(jax.tree_structure(None))\n",
      "2022-09-19 09:42:14.781472: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-19 09:42:15.383655: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64\n",
      "2022-09-19 09:42:15.383762: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64\n",
      "2022-09-19 09:42:15.383770: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# First going to work out some ideas in a jupyter notebook - Later I will clean this\n",
    "# up and and create some proper code.\n",
    "\n",
    "from typing import Iterator, NamedTuple\n",
    "\n",
    "# Getting some tensorflow warnings, but don't care about those right now\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import pathlib\n",
    "import string\n",
    "import glob2\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dataclasses\n",
    "\n",
    "tf.config.experimental.set_visible_devices([], \"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The IAM On-Line Handwriting Database (IAM-OnDB) is required for this project, so you will need to ask for\n",
    "# permission for that and download. https://fki.tic.heia-fr.ch/databases/iam-on-line-handwriting-database \n",
    "# Here we will create a wrapper class to give us some tensorflow dataset summaries of just the writing portion.\n",
    "\n",
    "class WritingGenerator():\n",
    "    def __init__(self, f_name, batch_size=32):   \n",
    "        self.all_x = []\n",
    "        self.all_y = []\n",
    "\n",
    "        # How we might pad each stroke to a consistent length and batch it for fitting - With the amount \n",
    "        # of data in the database and the complexity of a Transformer, this should probably be kept to\n",
    "        # under 400 strokes and 20 characters. There is an issue here though that different people use \n",
    "        # different amounts of strokes/char and this can confuse the network. I am not sure there is an\n",
    "        # elegant way to handle that problem with a Transformer network, as we would need about 1500\n",
    "        # tokens to read in every writing sample fully with padding and this is beyond a standard \n",
    "        # Transformer. Perhaps a PerceiverAR is next?\n",
    "        self.MAX_STROKE_LEN = 250\n",
    "        self.MAX_CHAR_SEQ_LEN = 20         \n",
    "        \n",
    "        self.f_name = f_name\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.padding_value = -1.\n",
    "        self.char_padding_value = 0.\n",
    "\n",
    "        # You will need to change this and point to your own database where you unzipped all of the \n",
    "        # IAM-OnDB strokes and corresponding ascii\n",
    "        base_dir_strokes='../IamONDB/lineStrokes'\n",
    "        base_dir_ascii='../IamONDB/ascii'\n",
    "        \n",
    "        try:\n",
    "            f = open(f_name, 'r')\n",
    "        except IOError:\n",
    "            print(\"Error opening file\")\n",
    "            return 0\n",
    "      \n",
    "        f_train = list(f)\n",
    "        f.close()\n",
    "        \n",
    "        self.n_samp = len(f_train)\n",
    "\n",
    "        print('Reading ' + str(self.n_samp) + ' files')\n",
    "\n",
    "        # This will contain a list of all stroke files\n",
    "        self.f_sub_list_strokes = []\n",
    "        # This will contain a list of corresponding ascii line files\n",
    "        self.f_sub_list_ascii = []\n",
    "\n",
    "        # First create a list of all subfiles in the .txt list - we are going to treat each line as a separate sample here\n",
    "        for i, fname in enumerate(f_train):\n",
    "            path_stroke = glob2.glob(base_dir_strokes + '/' + fname.strip()[0:3] + '/' + fname.strip()[0:7] + '/' + fname.strip() + '-*.xml')    \n",
    "              \n",
    "            self.f_sub_list_strokes += path_stroke\n",
    "\n",
    "            path_line = glob2.glob(base_dir_ascii + '/' + fname.strip()[0:3] + '/' + fname.strip()[0:7] + '/' + fname.strip() + '.txt')   \n",
    "            # We want a 1 to 1 matching of strokes to ascii.  We will pull out the appropriate line when we create the dataset\n",
    "            self.f_sub_list_ascii += path_line * len(path_stroke)\n",
    "                        \n",
    "        # list datasets\n",
    "        self.list_ds_strokes = tf.data.Dataset.from_tensor_slices(self.f_sub_list_strokes)\n",
    "        self.list_ds_ascii = tf.data.Dataset.from_tensor_slices(self.f_sub_list_ascii)\n",
    "\n",
    "        # Text helper functions and variables.  All lines of text need to be one-hot-encoded for proper integration into the attention\n",
    "        # mechanism of the model\n",
    "        self.vocab = string.printable\n",
    "\n",
    "        # I am adding 1 to all character enumerations so that 0 is reserved for padding only and can be ignored in the model\n",
    "        self.char2idx = {u: i+1 for i, u in enumerate(self.vocab)}\n",
    "        self.idx2char = {i+1: u for i, u in enumerate(self.vocab)}\n",
    "\n",
    "        self.invert_one_hot = lambda x: tf.argmax(x, -1).numpy()\n",
    "\n",
    "        self.text_to_int = lambda x: np.array([self.char2idx[c] for c in x])\n",
    "        self.int_to_text = lambda x: ''.join(np.array([self.idx2char[i] for i in x]))\n",
    "\n",
    "        # Combine\n",
    "        self.list_ds = tf.data.Dataset.zip((self.list_ds_strokes, self.list_ds_ascii))\n",
    "        \n",
    "        # Create a datbase of tuples (strokes, matching ascii)\n",
    "        self.labeled_ds = self.list_ds.map(lambda x, y: tf.py_function(self.process_stroke, (x, y), (tf.float32, tf.float32)))\n",
    "        \n",
    "        self.cached_example_dataset = self.labeled_ds.shuffle(buffer_size=1024).cache().take(1024)\n",
    "        \n",
    "    # Create a dataset of strokes and matching lines - As mentioned before, each line is a training sample in this version\n",
    "    def process_stroke(self, file_path_stroke, file_path_lines):\n",
    "        line_num = int(file_path_stroke.numpy()[-6:-4])\n",
    "        strokes = self.get_strokes(file_path_stroke.numpy())\n",
    "        # Not sure the best way to combine two files\n",
    "        lines = self.get_ascii(file_path_lines)\n",
    "\n",
    "        U = lines[line_num-1]\n",
    "        U = U[:self.MAX_CHAR_SEQ_LEN]\n",
    "        U_conv = tf.keras.backend.one_hot(self.text_to_int(U), len(self.vocab)+1)\n",
    "\n",
    "        return strokes, U_conv\n",
    "\n",
    "    # Returns only the strokes of the dataset as a tuple with a label of the same data 1 timestamp ahead\n",
    "    @property\n",
    "    def batched_set(self):\n",
    "        # We don't care about the line data for this version, so remove that first\n",
    "\n",
    "        stroke_only_ds=self.labeled_ds.map(lambda x, y: x)\n",
    "        \n",
    "        # All sequences will be strictly right padded so that tensorflow will run them on a GPU\n",
    "        batched_dataset = stroke_only_ds.padded_batch(self.batch_size, padded_shapes=([self.MAX_STROKE_LEN, 3]), \n",
    "                                                      drop_remainder=True, padding_values=self.padding_value)\n",
    "\n",
    "        return batched_dataset.map(self.dense_1_step).cache()\n",
    "        return batched_dataset.cache()\n",
    "\n",
    "    # Returns only the strokes of the dataset as a tuple with a label of the same data 1 timestamp ahead\n",
    "    # This one also returns the character sequence U being written as a one-hot-encoded tensor\n",
    "    @property\n",
    "    def batched_onehot_set(self):\n",
    "        batched_dataset_one_hot = self.labeled_ds.padded_batch(\n",
    "            self.batch_size, padded_shapes=([self.MAX_STROKE_LEN, 3], \n",
    "                                            [self.MAX_CHAR_SEQ_LEN, len(self.vocab)+1]), \n",
    "                                            drop_remainder=False, padding_values=(self.padding_value, self.char_padding_value))        \n",
    "\n",
    "        return batched_dataset_one_hot.map(self.dense_1_step)\n",
    "\n",
    "    # We will make our prediction 1 step ahead\n",
    "    def dense_1_step(self, batch_stroke, batch_char_seq):\n",
    "        # Shift features and labels one step relative to each other.\n",
    "        return (batch_stroke[:, :, :], batch_char_seq ), batch_stroke[:, :, :]\n",
    "    \n",
    "    def get_examples(self, num_examples):\n",
    "        example_dataset = self.labeled_ds.shuffle(100).take(num_examples)\n",
    "        \n",
    "        #example_dataset = self.labeled_ds.batch(1).shuffle(100).take(num_examples)\n",
    "        \n",
    "        return example_dataset\n",
    "        \n",
    "    def get_strokes(self, fname):\n",
    "        root = ET.parse(fname).getroot()\n",
    "\n",
    "        # Parse one xml file\n",
    "        strokeset = root.find('StrokeSet')\n",
    "\n",
    "        x_samp = []\n",
    "\n",
    "        for stroke in strokeset.iter('Stroke'):\n",
    "            for child in stroke:\n",
    "                x_samp.append([float(child.attrib.get('x')), -1*float(child.attrib.get('y')), 0.])\n",
    "\n",
    "            # As in Graves, 2013, we add a binary vector indicating the end of a stroke\n",
    "            x_samp[-1][-1]=1.0\n",
    "\n",
    "        x_samp = np.asarray(x_samp)\n",
    "        x_samp = x_samp[:self.MAX_STROKE_LEN, :]\n",
    "\n",
    "        # We want the data as offsets though, not raw strokes - easier to train a network to predict small changes in the next timestamp\n",
    "        x_off = np.hstack(([x_samp[1:, :2]-x_samp[:-1, :2], x_samp[1:, 2:3]]))\n",
    "        x_off = np.vstack(([0, 0, 0], x_off))\n",
    "\n",
    "        x_off[:, 0] /= np.std(x_off[:, 0])\n",
    "        x_off[:, 1] /= np.std(x_off[:, 1])\n",
    "\n",
    "        return x_off\n",
    " \n",
    "    # Read an ascii file form the iamONDB and return all of the lines as strings\n",
    "    def get_ascii(self, fname):\n",
    "        text_file = open(fname.numpy(), \"r\")\n",
    "        lines = text_file.read()\n",
    "        lines = lines.split('CSR:')\n",
    "\n",
    "        return lines[1].strip().split('\\n')       \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Writing Dataset for: {self.f_name}'\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing function for strokes\n",
    "# TODO: This should be in the writing class\n",
    "\n",
    "def plot_stroke(offsets, lines):\n",
    "    plt.figure(num=None, figsize=(15, 4))\n",
    "    strokes=np.array([np.cumsum(offsets[:,0]), np.cumsum(offsets[:,1]), offsets[:,2]]);    \n",
    "    stroke=[]\n",
    "\n",
    "    strokes[-1, -1] = 1\n",
    "\n",
    "    for x, y, eos in strokes.T:\n",
    "        stroke.append([x, y])\n",
    "        if eos > 0.1:\n",
    "            stroke=np.asarray(stroke);\n",
    "            #print(stroke.shape)\n",
    "            plt.plot(stroke[:,0], stroke[:,1], 'k')\n",
    "            stroke = []\n",
    "\n",
    "    clean_txt = lines\n",
    "\n",
    "    clean_txt = np.delete(clean_txt, np.argmax(clean_txt, -1) == 0.0, axis=0)\n",
    "\n",
    "    # TODO: This should be passed in\n",
    "    plt.title(train.int_to_text(train.invert_one_hot(clean_txt)))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1726 files\n"
     ]
    }
   ],
   "source": [
    "# I concatenated all data into one set as we just want the maximum amount of data to train the \n",
    "# network to write and don't really care about evaluation or test sets for this project\n",
    "train = WritingGenerator('../IamONDB/trainset_d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2kAAAEICAYAAADFv7xwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABldklEQVR4nO3dd3xT5fcH8M/TTQuFQqGUIXu1RZAlyJKhIEvcLOELyJIlojgQ/SECbkBlKSCigAgKIqMFWaUVSssoo2WVUcospaV75vn90Sa00JGkN7lJ+nm/Xrxsk9x7Dw2mOTnnOY+QUoKIiIiIiIgsg53aARAREREREdEDTNKIiIiIiIgsCJM0IiIiIiIiC8IkjYiIiIiIyIIwSSMiIiIiIrIgTNKIiIiIiIgsCJM0IiLSEUL8TwgRpHYcACCE+D8hxG9qX0sI8bQQIsYMMUghREMzXMdinmMiIiockzQiIiIiIiILwiSNiIhMQgjhoHYMRERE1ohJGhFRGSSEqC2E+EsIESuEiBNC/PDQ/V8LIeKFEJeFEM/lu32kECJSCJEkhLgkhBiX776nhRAxQoj3hBC3APwshCgnhPgl71yRQogZ+VsHhRA1hBB/5sVxWQgx5aFQnYQQa/Kud0YI0Sbfse8LIaLy7osQQryQ777/CSGCivl71BNCHMg7djcATwN+doXGnHd7mhCicr7HPiGEuCuEcMz7flTezyFeCBEghKij5zX1+blPF0LcEULcFEKMzHd/FSHEViFEohDiCIAG+v5diYhIHUzSiIjKGCGEPYBtAK4CqAugJoDf8z3kSQDnkJu4fAlgpRBC5N13B0A/AO4ARgJYIIRole/Y6gAqA6gDYCyAT/KuUR/AMwCG5YvDDsA/AMLzYugB4C0hRK985xuQF1slAFsB5E8mowB0BlARwGwAvwkhvPX8e6wDcDTvvjkARhT188qvuJillDcAHALwUr5DhgDYJKXMEkI8D+BDAC8CqArgIID1+lwX+v3cK+bFNBrAYiGER959iwGkA/AGMCrvDxERWTAmaUREZU87ADUAvCulTJFSpksp8w+SuCql/ElKmQPgF+S+ufcCACnldilllMx1AMAu5CZKWhoAn0gpM6SUaQBeBTBPShkvpYwB8F2+x7YFUFVK+amUMlNKeQnATwAG5XtMkJRyR14svwJoob1DSrlRSnlDSqmRUm4AcCHv71bs30MI8VjetWflxRmI3MRLHyXFvA7AYADISwgH5d0GAOMBzJdSRkopswHMA9BSn2qaHj/3LACfSimzpJQ7ACQDaJKXkL8E4OO85/p03s+CiIgsGJM0IqKypzZyE5jsIu6/pf1CSpma92V5ABBCPCeEOCyEuCeESADQBwVbBWOllOn5vq8B4Fq+7/N/XQdADSFEgvYPcitNXoXFAiAVgIt2rZsQYrgQ4kS+Y/0eiqWov0cNAPFSypR8j7366I+hUCXF/CeADnkVvS7ITVoP5jt2Ub7j7gEQyK1+FUuPn3vcQ89nat7ftSoABxT8uev7dyUiIpVwUTcRUdlzDcBjQgiHYhK1RwghnJGbhAwH8HdeC98W5CYaWvKhw24CqAUgIu/72g/FcVlK2cjA+JFXffoJue2Gh6SUOUKIEw/FUpSbADyEEG75ErXHCom9MMXGLKWMF0LsAvAagGYAfpdSynzHzpVSrtXjOjp6/tyLEgsgG7k/97N5tz1myPWJiMj8WEkjIip7jiA3UflcCOEmhHARQnTU4zgnAM7Ie+OfN4jj2RKO+QPAB0IIDyFETQCTHoojKW/QSDkhhL0Qwk8I0VaPWNyQm1TFArmDNZBbSSuRlPIqgDAAs4UQTkKITgD663OsnjGvQ25C9TIetDoCwDLk/ix882KuKIR4RY9rGvNzBwDktXr+BeD/hBCuQggf6Ln+joiI1MMkjYiojMl7494fQEMA0QBikFv5Kem4JABTkJt4xSN3KMbWEg77NO/8lwH8C2ATgIx8cfQD0DLv/rsAViB3AEZJsUQA+Aa5gzpuA2gOILik4/IZgtzBIveQO9xkjT4H6RnzVgCNANySUobnO3YzgC8A/C6ESARwGsBzKIGRP/f8JiG39fEWgNUAfjbgWCIiUoF40IVBRERkWkKICQAGSSm7qh0LERGRpWIljYiITEYI4S2E6CiEsBNCNAEwHcBmteMiIiKyZBwcQkREpuQEYDmAegASkLvn2RI1AyIiIrJ0bHckIiIiIiKyIGx3JCIiIiIisiCqtDt6enrKunXrqnFpIiIiIiIi1R09evSulLJqYfepkqTVrVsXYWFhalyaiIiIiIhIdUKIq0Xdx3ZHIiIiIiIiC8IkjYiIiIiIyIIwSSMiIiIiIrIgTNKIiIiIiIgsCJM0IiIiIiIiC8IkjYiIiIiIyIIwSSMiIiIiIrIgTNKIiIiIyCzOnTuHxYsXIyMjQ+1QiCwakzQiIiIiMqnw8HC89tpraNasGSZNmoT33ntP7ZCILBqTNCIiIhuXlJSE7OxstcOgMujw4cPo378/WrZsiZ07d+K9997DmDFjsGjRImzdulXt8IgsFpM0IiIiG5aUlIR69erhsccew/z589UOh8oAKSX27duHnj17okOHDvjvv//w6aef4urVq5g/fz6+//57tGrVCiNHjsS1a9fUDpfIIjFJIyKLdevWLbRs2RJ9+/bFzp07odFo1A6JyOqsXbsWcXFxcHJywieffML/j8hkpJTYvn07OnbsiO7du+PMmTP46quvcPXqVcyaNQseHh4AAGdnZ/z+++/IzMzE4MGDWeUlKgSTNCKySGlpaXj++edx4cIFHDt2DH369EGTJk2wcOFCJCQkqB1emZCRkYGpU6di/fr1aodCRpJSYtmyZXjiiSfg5+eHJk2awM6Ov/pJebGxsWjXrh369euH69evY/Hixbh8+TLeeecdlC9f/pHHN2rUCMuXL0dwcDD+7//+z/wBE1k4vlITkcXRaDT43//+h9DQUKxduxZXr17F+vXrUa1aNUybNg21atXChAkTcPr0abVDtWlvv/02vvvuOyxZskTtUMhIISEhCA8Px/jx43Hq1Ck8/vjjaodENkij0WDYsGE4deoUVq5ciYsXL+LNN9+Ei4tLsccNGTIEo0aNwrx58/Dvv/+aKVoi68AkjYgszuzZs/HHH3/giy++wMCBA+Hk5IRBgwYhODgYR48exauvvoqff/4ZzZs3x3fffad2uDbpyJEjWLp0KYDcT7zJOi1btgwVKlRAnz59EB0djebNm6sdEtmgefPmYdeuXfj+++8xatQoODo66n3sd999h6ZNm2LYsGG4ffu2CaMksi5M0ojIoqxduxaffvopRo0ahXfeeeeR+1u1aoVVq1YhJiYGzz77LGbOnIlbt26pEKntys7Oxvjx4+Ht7Q0XFxc4OzurHRIZ4d69e9iwYQOGDRuGK1euAACTNFLcvn378Mknn2Do0KF44403DD7ezc0Nf/zxB+7fv4/hw4dzzSQp7ujRo4iKilI7DIMxSSMii+Lh4YH+/ftj6dKlEEIU+ThPT0/88MMPyMjIwEcffWTGCG3f4sWLcfz4cSxcuBBOTk5M0qzUmjVrkJ6ejnHjxuHUqVMAwHZHUtStW7cwePBgNG7cGMuWLSv2Nbs4fn5+WLRoEXbt2oUvv/xS4SiprBsyZAg+/PBDtcMwGJM0IrIoffr0wdatW+Hk5FTiYxs1aoQpU6Zg1apVOH78uBmis33Xr1/HrFmz0Lt3b7z88stIT08vcV0JWR7twJAOHTqgRYsWOHnyJCpWrIhatWqpHRrZiJycHAwZMgSJiYnYuHFjocNBDDFmzBi8+uqr+OijjxAcHKxQlES5XQVVqlRROwyDMUkjIqv20UcfoUqVKpg2bRqklGqHY/WmTZuGrKws/PDDDwCAzMxMVtKs0IEDB3Du3DmMHz8eAHDq1Ck0b97c6EoH0cNmz56Nffv2YcmSJfDz8yv1+YQQ+PHHH1GnTh0MGTIEWVlZCkRJZZ1Go8G9e/dQuXJltUMxGJM0IrJqlSpVwpw5c3DgwAFs3rxZ7XCsmr+/PzZu3IiZM2eiQYMGyMzMBAAmaVZo2bJl8PDwwCuvvAIpJSc7kqJ27dqFzz77DCNHjsT//vc/xc5bsWJFfPzxx4iOjsaFCxcUOy+VXUlJSdBoNEzSiIjU8MYbb8DPzw/vvPMOMjIy1A7Har377rto3Lgx3n33XQDQ/SyZpFmX27dv46+//sL//vc/lCtXDtHR0UhMTOTQEFLE9evXMWzYMPj6+uoq7krS/juNjIxU/NxU9sTFxQEAkzQiIjU4ODhg4cKFuHz5MhYtWqR2OFbp0qVLOH36NCZOnKhLypycnPDDDz+gR48eKkdHhvj555+RlZWFcePGAYBuaAiTNCqt7OxsDB48GKmpqdi4cSNcXV0Vv0bTpk0hhEBERITi56ay5969ewDANWlERGrp0aMHBgwYgM8++4x77RghICAAANC7d2/dbS4uLpg4cSJatmypUlRkjHXr1qFLly5o0qQJgAdJmhLrhqhsmzVrFg4ePIjly5ejadOmJrmGq6sr6tSpwySNFKFN0sp0JU0IYS+EOC6E2KbUOYnKgvnz5+PPP/9UOwyb8PXXXyM9PR2zZs1SOxSr4+/vj3r16nHjaiuXlJSE06dPo3v37rrbTp06hTp16qBixYoqRkbWbseOHfj8888xduxYDB061KTX8vHxYZJGimCSlmsqADYQExnou+++01UxqHQaNWqEyZMnY8WKFThx4oTa4ViNzMxM7NmzB8899xyn/1m5sLAwSCnRrl073W0nT55kqyOVSnR0NF5//XW0aNECCxcuNPn1fHx8cO7cOeTk5Jj8WmTbynySJoSoBaAvgBVKnI+oLElOTi71/jL0wKxZs1C5cmW89dZbHMmvp6CgIKSkpBRodSTrFBISAgC6JC0jIwPnzp3jZEcyWmZmJl577TVkZWVh48aNKFeunMmv2axZM2RkZODy5csmvxbZNg4OARYCmAFAU9QDhBBjhRBhQoiw2NhYhS5LZN00Gg1SUlKYpCko/0j+LVu2qB2OVfD394ejoyO6deumdihUSkeOHEHDhg11i+SPHz+O7OxstGrVSuXIyFqlpqaiQoUKWLFihdnaoX18fACALY9Uavfu3UOFChXg6OiodigGK3WSJoToB+COlPJocY+TUv4opWwjpWxTtWrV0l6WyCakpaVBSskkTWFjxoyBn58fPvzwQ7VDsQr+/v7o3Lkz/x3agJCQEDz55JO674ODgwEAHTt2VCsksnKVKlVCQEAAXn31VbNds1mzZgA4hp9Kz1o3sgaUqaR1BDBACHEFwO8AugshflPgvEQ2LyUlBQDg5uamciS2xcHBAa+//jrOnj2LhIQEtcOxaNevX8epU6fY6mgDYmJicOPGjQLr0YKCgtCwYUNUr15dxcjI2pl7rWrFihVRs2ZNVtKo1Mp0kial/EBKWUtKWRfAIAB7pZTDSh0ZURmg3SzYxcVF5Uhsj6+vLwC2y5SksNH7ZJ2069G0lTQpJYKDg9GpUyc1wyIySrNmzfj6TaVWppM0IjKeRpO7jNPOjv8rKo1rGvTj7++PmjVrcg8tG3DkyBE4Ojrq9rW7cOECYmNj2epIVsnHxweRkZEcAEWlEhcXZ5UbWQMKJ2lSyv1Syn5KnpPIlmnHCzNJU16dOnXg6uqKM2fOqB2KxcrOzsbu3bvRu3dvjt63ASEhIWjZsiWcnZ0B5LY6AmAljaySj48PUlJScO3aNbVDISt27949eHh4qB2GUfjOkEhFrKSZjp2dHdtlShASEoKEhAS2OtqAnJwchIWFFRgaEhQUhCpVqqBJkyYqRkZkHHZDUGlJKREfH892RyIynDZJs7e3VzkS2+Tj48Nf8MXw9/eHvb09evbsqXYoVEpnzpxBSkrKI0lax44dWSUlq6Sd8MjXcDJWamoqsrOzUalSJbVDMQqTNCIVsZJmWr6+voiJicH9+/fVDsUi+fv7o3379lb7C4weOHLkCIAHQ0Pu3LmDCxcusNWRrJanpyeqVq3KMfxktPj4eABguyMRGY5r0kxL2y7DX/KPunPnDsLCwtjqaCNCQkLg4eGBhg0bAniwPxqTNLJm7Iag0tBuwWOtH0TynSGRitjuaFpc01C03bt3A+DofVsREhKCdu3a6Vobg4KC4OLiglatWqkcGZHxtOuKOeGRjMEkjYiMxnZH06pbty7KlSvHCY+F2LlzJ6pWrco38TYgOTkZZ86cKbAeLTg4GG3bttVNeiSyRj4+PkhISMDt27fVDoWsEJM0IjIa2x1Ny97eHk2bNmUl7SEajQYBAQHo1asX/+3ZgKNHj0Kj0aBdu3YAchfLHz16lK2OZPXYDUGlwTVpRGQ0tjuaHtc0POrYsWO4e/cuWx1tREhICADokrQjR44gOzubSRpZPe3o9MTERJUjIWvEShoRGY3tjqbn6+uL6OhoJCUlqR2KxfD394cQAs8++6zaoZACQkJCUL9+fVStWhXAg02sO3TooGZYRKWWmpoKAHBzc1M5ErJG2iStYsWK6gZiJL4zJFIR2x1NjxMeH+Xv74/WrVvr3tSTdTty5Mgj69H8/PystsWHSCslJQUA4OrqqnIkZI0SEhJQvnx5ODo6qh2KUfjOkEhFbHc0PV9fXwDg8JA88fHxOHToEFsdbcSNGzcQExOja3XMycnBf//9x1ZHsgnaJI2VNDJGfHy81bY6AkzSiFTFdkfTq1evHpydnbkuLc/Bgweh0WjwzDPPqB0KKUC7Hk1bSTt9+jQSExOZpJFNYJJGpZGQkMAkjYiMw3ZH09NOeGQlLVdQUBCcnJx0lReybkeOHIGDgwOeeOIJAA82se7YsaOaYREpgmvSqDSYpBGR0VhJMw9PT09OB8sTFBSENm3awMXFRe1QSAEnT55Es2bNdM9nUFAQatasiTp16qgcGVHpcU0alUZ8fLxVr83lO0MiFXFNmnloNBomwgDS0tIQFhbGVjgbEhERoVt3CeQmaZ06dYIQQsWoiJTBdkcqDVbSiMhorKSZB5O0XKGhocjKymKSZiNSUlJw9epV3QTT6OhoXLt2ja2OZDNSUlLg6OhotdP5SF1M0ojIaFyTZh45OTmsVuLB/llPPfWUypGQEs6dOwcpJZo1awYA2LdvHwAwCSebkZqaylZHMopGo8H9+/fZ7khExmG7o3mwkpYrKCgIPj4+qFKlitqhkAK0E0u1lbTNmzejdu3aaNmypYpRESknJSWFrY5klMTEREgpWUkjIuOw3dE8mKTl/gy4f5ZtiYiIgIODAxo2bIjk5GQEBATgxRdf5Ho0shlM0shYCQkJAMAkjYiMw3ZH88jJySnzP+MzZ87g/v37TNJsSGRkJBo2bAgnJyfs3LkT6enpePHFF9UOi0gxTNLIWNokje2ORGQUtjuah0ajKfM/Y+16NCZptiMiIkLX6vjnn3+iWrVqHBpCNoVr0shY8fHxAFhJIyIjZWdnA2CSZmpsd8xN0mrUqIG6deuqHQopICMjAxcvXoSPjw/S09Oxfft2DBw4kK8lZFNYSSNjsd2RiEolLS0NAFCuXDmVI7FtbHfk/lm25sKFC9BoNPDx8cHu3buRnJzMVkeyOUzSyFhM0sisEhMTMWLECFy+fFntUEgh6enpAJikmVpZb3eMjo5GdHQ0Wx1tiHayY7NmzfDXX3+hUqVK6Natm8pRESkrJSWF7Y5kFK5JI7P6+OOP8euvvyI2NlbtUEgh2kqai4uLypHYtrLe7hgcHAyA69FsSUREBIQQaNCgAbZu3Yr+/fvDyclJ7bCIFJWamspKGhklPj4eQghUqFBB7VCMVnbftViZyMhIfP/995gwYQLatWundjikELY7mkdZb3cMCgpChQoV0Lx5c7VDIYVERkaifv36CAkJwb1799jqSDaJ7Y5krISEBFSsWNGqf/dbb+RlzMmTJ6HRaDBhwgS1QyEFadsdnZ2dVY7EtpX1dsegoCB06NABDg4OaodCComIiNC1Orq6uqJXr15qh0SkKCklkzQyWkJCglW3OgJM0qyGdj8ttrPYlrS0NLi4uHCYg4mV5XbHhIQEnDp1iqPZbUh2djbOnTuHpk2bYvPmzejTpw+r8VZk165dCAgIUDsMi5eRkQGNRsM1aWSU+Ph4qx4aAjBJsxraUe38JNy2pKencz2aGZTldsdDhw5BSsn1aDYkKioKWVlZuH//Pm7dusVWRyshpcS3336L5557DnPnzoWUUu2QLFpKSgoAsJKmp6SkJOzfv1+3P1hZl5CQwCSNzINJmm1KS0vjJ+BmUJbbHYOCgmBvb48nn3xS7VDKtICAAPz++++KnCsyMhIAcPbsWTg5OaFv376KnJdMa8KECZg+fTpeeOEF7Nixgx0UJUhNTQXAJE1fERER6NatG/777z+1Q7EITNIACCFqCyH2CSEihBBnhBBTlQiMCmKSZpvS09OZpJlBWW53DA4ORqtWrfhGR2W9e/fG4MGDFTmXdvx+REQEnnnmGbi7uytyXjKdW7duYfny5Rg3bhz++OMPlC9fXu2QLJ62ksZ2R/3ExcUBAKpUqaJyJJaBa9JyZQOYLqX0AdAewEQhhI8C56V8tElaWa0G2CrtmjQyrbLa7piZmYmQkBC2OupBo9HgzJkzJjn3rVu3FD2fNkmLi4vDSy+9pOi5yTT2798PABg9enSZfC0yBtsdDcMkrSCuSQMgpbwppTyW93USgEgANUt7XipIOziElTTbwkqaeZTVStqxY8eQnp7OJE0PtWrVgp+fHy5duqT4ubVtjsOHD1fkfNp2R3t7e/Tv31+Rc5Jp7du3D+7u7njiiSfUDsVqMEkzDJO0B7KyspCSksIkLT8hRF0ATwAIUfK8xHZHW8VKmnmU1TVpQUFBAMDJjnq4efMmAKBu3bqKn/vHH38EAIwcOVKR8yUlJQEAunbtCk9PT0XOSaa1b98+dOnShb/DDXDnzh0ATDr0FRcXBzs7O6tPTJRw//59ALD6dkfFXi2EEOUB/AngLSllYiH3jwUwFgAee+wxpS5bZjBJs01paWn8lNAMymol7eDBg/Dy8rK4NzlSSuzevVuXbDRp0gR+fn56HXvt2jUcPnwYzz33nGLresLCwnRfm+Lfibby1a5dO0XOd/fuXQBgq6OVuH79Oi5cuIDx48erHYpV0Va169evr3Ik1uHevXvw8PAok7/rHqZN0qx9va4i7/iFEI7ITdDWSin/KuwxUsofAfwIAG3atOHcWQMxSbNN6enpFvcG2haVxTVpKSkp2LVrF9LT07Flyxa8/PLLaoeks3v37gKbL9euXRvR0dF6HRscHIzBgwcjMjISTZs2VSSexYsXAwDq1aunyPny07aqA8oNQNCO2B44cKAi5yPT2rdvHwCgW7duKkdiXS5fvowqVaqgYsWKaodiFeLi4vh+Ik9ycjIAoEKFCipHUjpKTHcUAFYCiJRSflv6kKgwHBximziC3zzKWrtjTEwMOnXqhPT0dPTs2dOiEjQAWLt2LSpWrIjjx4/jtddeQ1pamt7HaivP2l/CpZWSkoL169frvlaatopmCjVq1DDZuUk5+/fvh4eHB1q0aKF2KFbl0qVLrKIZgEnaA9rfD9Y+RVWJj5Y7AngdQHchxIm8P30UOC/lo/00tiy90SwLuJm1eWRkZMDJyUntMMzi6NGjePLJJ3Hx4kUAuaPfLUlaWho2b96Ml156CS1btjT4TYU2SVMqoTp06BAyMjLQrl07xMbG6j4QU0pgYCAAYOjQoYqcL/9gE26GbB327duHrl27lrlqfmkxSTNMXFwcKleurHYYFkGbpFn7chIlpjsGSSmFlPJxKWXLvD87lAiOHsjOzoa9vT03v7QxrKSZXnp6OjIyMsrEYmopJaZMmQJHR0fdREFLGyyxfft2JCUl6fYMu3nzpkG/SJVO0g4cOAB7e3u89NJLkFLqhhUoZevWrQCAzp07K3K+zZs3677WrrsgyxUdHY1Lly6x1dFAOTk5uHLlCpM0A7CS9oD29wMraWQW2dnZXI9mgziC3/QSEhIAoEwkaUIIbNiwASEhIbpf1lWrVlU5qoLWrVsHLy8vdOvWDYmJidixYwcGDBig9/GmSNJatWqFxo0bA3gw5VEpAQEBAABvb+9Sn0tKibVr1+q+1w4QIct18OBBAMDTTz+tbiBWJiYmBtnZ2UzSDMAk7QG2O5JZaStpZFs4gt/0ylKSBuTu9+Xl5YXY2FgAlpWkJSQkYMeOHRg0aBDs7e2xZcsWZGRk6Kpq+lAySUtLS0NISAi6du2qS6KUTNJu376t+7p69eqlPt/+/ftx/PhxvPDCCwCge47Jcp09exb29vaKDbkpK7RtvaYY5mOLMjIykJKSwiQtD5M0MquyNvigLMjJyUFmZiYraSamnYRn7fulGEpbZbGkdsfNmzcXSMrWr1+PunXron379nqfQ8kkLSQkBJmZmSZL0r744gvd115eXqU+31dffYVq1arh7bffBsAkzRqcP38e9erVKzNrYpXC8fuG4UbWBTFJI7MqiyPEbV1GRgYAsJJmYmWtkqZliZW09evXo379+rohHbt378agQYMMWmur/aWrxHTH/fv3QwiBTp066SpdSiVpN27cwNKlS3XflzZJO336NHbu3InJkyejVq1aANjuaA0uXLiga6VV0w8//KAbYmMNLl26BHt7e9SuXVvtUKwCk7SCkpOTIYSw+g/B+a7fSrCSZnu0Y8et/UXE0pXlJM3FxcViplvdunULe/bsweDBgyGEwMaNG5GTk4MhQ4YYdJ5y5cpBCKFIJe3AgQNo2bIlKlWqBCcnJ1SpUgW3bt0q9XkBYN68ecjOzka/fv1QsWLFUn8Y880338DV1RUTJkzQJd6spFk2KSXOnz+PRo0aqRrHjh07MHnyZPTu3RuHDh1SNRZ9Xbp0CXXq1OFafD3du3cPAJM0rZSUFLi5uVn9sD0maVaClTTbk56eDoCVNFMrq0na3bt34enpaTG/pDZu3AiNRqNLytavXw9fX180b97coPMIIeDq6lrqJC0jIwOHDx9G165ddbfVqlULV69eLdV5gdyJfj/99BNGjRqFcuXKlXo92o0bN7B27VqMGjUKVapUgZubG8qVK8ckzcLdvHkTKSkpqlbS0tPTMWXKFDRp0gTu7u745ptvVIvFEBy/b5iy+nuuKMnJyVbf6ggwSbMarKTZHlbSzEO7Jq2s/fKKjY21qFbHdevW4fHHH4ePjw+io6MRFBRk0MCQ/CpWrKhr7zHWkSNHkJ6eXiBJa9asGc6cOVOq8wLA3LlzAQAfffQRbt++Xeok7bvvvkNOTg6mTZumu83T05Ptjhbu/PnzAKBqkvbFF18gKioKS5Ysga+vr+LTS02FSZphtO8nXF1dVY7EMjBJI7PSaDSspNkYbSWNSZppJSQkwMXFpcxVLC0pSYuJicHhw4d1SdmGDRsAAIMGDTLqfL6+vjh58mSpYjpw4ACAgvuX+fr64urVq6Va73bp0iWsWrUKY8eORe3atXHr1q1SrUdLSkrCsmXL8NJLLxV401q1alVW0iyc2klaVFQU5s+fj0GDBqF79+6oVq2a4vsAmkJSUhLu3r3LJM0A/NC3ICZpZFZsd7Q9mZmZAABHR0eVI7FtCQkJZa6KBuS2O1pKknb48GEAQM+ePQHkVtWefPJJNGjQwKjztWrVCqdPn9YN3zHG/v370bx58wJrOHx9fQEAkZGRRp93zpw5cHBwwAcffAAgdy1eaSppK1aswP379/Huu+8WuJ1JmuU7f/48XFxcdINezCn/xvZff/01AFhNknb58mUAnOxoCG2SVtY+jCxKcnKyxazHLg2+67cSbHe0XZayZshWldUkLTY21mLG74eGhsLJyQmPP/44IiMjceLECaNbHYHcJC0rK8vo1sT09HQEBwejR48eBW7XJmnGnnfTpk345Zdf8Oabb6JGjRpIS0tDYmKi0ZW0rKwsLFiwAF26dEHbtm0L3Md2R8t34cIFNGzY0KgPWDUaTakqulu3bsWOHTswe/Zs1KxZE0DuhNHExERdF4el4vh9w7EzpyBW0sis2O5IZJyymKRlZGQgKSnJYippoaGhaNGiBZycnPDll1/CxcXF6FZHIDdJA4Bjx44Zdfx///2H9PT0R5K0Bg0awNnZ2agkbd++fRg6dCieeuopfPbZZwAebGZtbCVt48aNuHbt2iNVNICVNGtw/vx5g1sdo6KiMGzYMHh7e6Nq1aqIiYkx+LoajQbvvPMOfH19MXnyZN3t1apVAwCLr6YxSTMcK2kFpaSkMEkj82G7I5lacnIyDh48CCml2qEoKj4+vswladoKiyUkaRqNBmFhYWjbti0uXLiANWvW4M033yzVOq369evD3d3d6CRt7969sLe3R5cuXQrcbm9vj6ZNmyI0NNSg84WHh2PgwIFo2LAhtm7dqvs0WzvO35gkTUqJr776Ck2bNkWfPn0eub9q1apITk62+KpIWZWdnY2oqCiDk7S3334bmzdvRqdOnZCeno4tW7YYfO39+/fj4sWL+PDDDwu001tTklapUiV4eHioHYrVSE9Ph52dHZdP5GEljcyK7Y5kKlJKbNy4EU2bNkWXLl2wYsUKtUNSVEJCQpn7Za+tsFhCu+O5c+eQlJSEtm3b4tNPP4WLiwtmzJhRqnPa2dnhiSeeMDpJ27NnD9q2bQt3d/dH7hs0aBAOHDig98a/ly9fRu/eveHu7g5/f39UrlxZd19pNhTfs2cPTpw4gXfeeafQD+i4V5plu3r1KrKysgxK0lJSUrBr1y6MHj0af/75J5o2bYq///7b4GuvWLEClSpVwgsvvFDgdmtK0lhFM0xaWppuD0likkZmxnZHMgUpJd5//328+uqrqFq1Kjp27IgpU6YoMobcUpTFdsfSJAdK01alKlSogHXr1mHSpEmlqqJptW7dGuHh4cjOzjbouMTERISGhj7S6qg1depU1KpVC++88w40Gk2x54qNjUWvXr2QkZGBgIAA1K5du8D9pdm76Ouvv4aXlxeGDh1a6P3NmjUDAISEhBh8bjI9YyY7BgQEID09XZdcDRgwAPv379f9O9LHvXv38Ndff2HYsGGPrE/SJmnaNlxLxSTNcGlpaWx1zIeDQ8is2O5IhTl//jyuX79u9PFz5szBl19+iXHjxiEsLAybNm2Ch4cH9u3bp2CU6pFSlskkzZLaHUNDQ+Hm5oYNGzbA1dW10PVVxmjVqhXS09Nx9uxZg447cOAAcnJyikzSypUrh7lz5yI0NFS3VUBhjhw5gq5du+LatWvYtm0bfHx8HnnM/fv3AeTu62aIkydPIiAgAFOmTCnyjVf79u3h4eGB7du3G3RuMo8LFy4AABo1aqT3MVu2bIGHh4duW4iuXbsiOzsbERERep9j7dq1yMjIwOjRox+5T/vhiCVX0jQaDS5fvswkzUDp6ekcGpInKysLGRkZrKSR+bDdkR524cIFtG7dGi+++KJRx3/99df45JNPMGLECCxZsgT29vaoXr06IiMjMWnSJIWjVUdqaiqys7PLXJKWmpoKwDIWkYeGhsLFxQWbNm3ClClTFGvBNHZ4yN69e+Hi4oIOHToU+Zhhw4ahZcuW+OCDD3Q/S62UlBS8/fbb6NChAxITE7Ft2zY89dRThZ7H2CTtm2++gZubG8aPH1/kYxwcHNCrVy/s2LGjxIofmd/58+dRsWJFvT8oycrKwrZt29C/f384ODgAgK4dNyUlRa9zSCmxcuVKtG7dGi1btnzkfjc3Nzg4OBhUmTO3GzduIDMzE/Xq1VM7FKvCStoD2v9fmKSR2bCSRvllZGTgtddeQ3JyMo4cOWLwvk4nTpzAu+++i1dffRUrV64s8G/L0DeUliw+Ph6Ace1m1uyxxx4DkLsuRk2ZmZk4fvw44uLiUL58eUyfPl2xczdu3Bjly5c3uOq7Z88edOzYsdg3NHZ2dvjmm29w9epV1KhRA0OHDsWmTZuwY8cONG/eHAsWLMC4ceMQERFRZEUOyE3SnJ2d4ezsrHd8MTExWLduHUaPHl1gfVth+vbtizt37uDo0aN6n5/MQzvZUd81QoGBgYiPj8fAgQN1t7m6ugLAIx8UFOXYsWMIDw8vtIoG5CZx2dnZBv17NDdOdjQOK2kPMEkjs2MljfJ79913cfz4cfz000+wt7fHmjVrDDp+3bp1cHBwwNKlS23635X2E+OyNjhEu0l0VFSUqnGcOnVKt2n7tGnTSkw6DGFvb4/Bgwfj999/R1xcnF7H3LlzB6dOnUL37t1LfGz37t3x77//4qWXXkJAQABeeeUV9O3bF46Ojjhw4ACWLFlS6OCR/O7fv2/whx6LFi2CRqPBtGnTSnxs7969IYRgy6MFMnT8/pYtW1CuXDn06tVLd5t2TY2+lbQVK1agXLlyRe5BqN383ZIrLsePHwfwYM0l6YeVtAe0+wsySSOz4eAQ0oqKisIPP/yAiRMn4o033kDv3r3x66+/IicnR6/jNRoNNmzYgF69ein6ptkS3bt3D4BtVQf18dhjj8HBwUH1JC3/KHt9kg5DTZkyBenp6XpPJJ09ezYAFDrSvjA9evTAypUrcevWLezbtw+rVq1CeHj4I6P7i2JoknblyhUsXrwYr732GurWrVvi4z09PdG+fXsmaRYmKysL0dHRug9LSiKlxJYtW/Dss8/qqmeAYZW01NRUrFu3Di+//HKRnQPa7RoMfTO/cuVKjB071izbs+zfvx/169d/ZAhPWXPp0iUEBwfr/XiNRsPJjnm0SRoHh5DZsN2RtH788UfY2dnhgw8+AJC7fub69es4cuSIXscfPnwY0dHRpdpM2FocPnwYAAod6mDLHBwcUKdOHdWTtGXLlgHITY5M0XLq5+eH7t27Y/HixSVOefzzzz+xZMkSTJ8+vdD1OsVxcHDA008/jZEjRxr0BteQJE1KicmTJ0MIgc8//1zva/Tt2xdhYWEWP7GvLLl9+zaklKhZs6Zejz969ChiYmIKtDoCD95k6pOkbdq0CYmJiXjjjTeKfIwxSZqUEp9//jl++ukngzs2DKXRaHDgwAE8/fTTJr2ONVi6dCmeffZZvR/v6empGxhV1rGSRmbHdkcCcttVVq1ahQEDBujeALRt2xYA9J4Atn79eri4uOD55583WZyWYvv27WjRogVq1aqldihm16BBA9WTtPDwcADAW2+9ZbJrTJkyBdeuXSt2P6krV65g9OjRaNeuHebNm2eyWB5mSJL2999/Y9u2bZg9e7ZuTaE++vbtCwDYuXOnUTGS8m7cuAEAqFGjhl6P37JlC+zs7NC/f/8Ct2srafq0O65cuRKNGjXSTYYsjDZJM2RNWmRkJC5evAhXV1e8/fbbJt2X79SpU4iPj2eShtzN0LUDZPRRrVo17pmYh0kamR3bHQnIrQbcvXu3wNS3unXrwsnJCefOnSvx+OzsbPzxxx/o168fKlSoYMpQcfXqVWzbts3g4+7fv68bX10aCQkJCA4O1r2JLWvUTtK0Az0cHR1LXLtVGv369UO9evXw3XffFXp/VlYWBg8eDCkl1q9fDycnJ5PF8rDExES9krTk5GRMmTIFzZs3x9SpUw26RosWLVCzZk22PFoQY5K0Ll26oEqVKgVud3Z2hp2dXYmVtPPnzyMwMBCjR48utuXNmErali1bAOR+iJCUlIS3335b72MNtX//fgC5Ww+UddnZ2QZ9MF+1alUkJSUhLS3NhFFZBw4OIbNjuyMBuS0QDRo0QM+ePXW32dvbo3HjxnrtF3XgwAHcuXPHLK2Oa9euRf/+/fWeTJaUlIS5c+eibt268PPzw61bt0p1/V27diEnJ0fv9Ue2pkGDBoiPj9dNuDQ3bVXA1C1S9vb2mDRpEgIDA3HixIkC96WmpuLFF1/E4cOH8dNPP5l9Ypy+lbTZs2fj2rVrWLp0KRwdHQ26hhACffr0wa5du5CVlWVsqKQgQ5K0Cxcu4MyZM4+0OgK5z62rq2uJlbScnBy89NJLGDFiRLGPMzZJe/LJJ9GzZ0+8//77+O2337B37169jzeEdj2aIZVkW5WTk2NwJQ0Aq2lgJY1UwHZHOn36NIKCgjBu3LhHEvamTZvqlaT9/vvvKF++vFkSlzp16gAoeQy8lBKLFi1C3bp18dFHH6F169bIzMzE+vXrS3X97du3o3Llymjfvn2pzmOt1JzwGBwcrHtj2a1bN5Nfb9SoUXBzc8Mrr7yCv/76C1JKxMXFoWfPnti+fTuWLFmCV1991eRxPCw1NbXEPcxOnTqFBQsW4I033kDHjh2Nuk7fvn2RmJiIoKAgo44nZd28eRN2dnZ67ZH277//AkCRFX83N7cSP+hq1qwZNm3ahOrVqxf7OEOTtJiYGISGhuoSyA8//BB2dnYmSdK069HM8XphDQytpDFJe0CbpOUfwmOtmKRZCVbSaNmyZXBycsLIkSMfua9p06a4dOmSbtx5YTIzM/Hnn39i4MCBZtlPRTudrqQk7euvv8Zbb72F1q1b48iRI/j333/Rtm1b/PLLL0ZfW6PRYOfOnejdu3eZ/XBDrSTtzp07GDp0KACgSpUq8PLyMvk1K1WqhL///huOjo546aWX0LFjR3Tu3BnHjh3Dpk2bMGHCBJPHUJg+ffpg06ZNRVYzNRoNxo8fDw8PD4OGhTysR48ecHJyYsujhbhx4waqV6+u12tPYGAgatSoUeQkSH0qafoyNEnbunUrAOiSNBcXF7i7uyMxMVGRePI7efIk16PlY2wl7c6dO4/cJ6VEcHCwRW9iriTtEClDuxIsEd/1WwmuSTMfKSWuXLlikl9ExkpOTsaaNWvwyiuvwNPT85H7mzRpgpycnGLfkGs3SzWmonDkyBGMHDnSoAly2iTtypUrRT7m999/x4wZM/Daa6/B399fNwRl+PDhCA8P1w2eMFRYWBhiY2PLbKsj8GAzWHMmaenp6XjhhRd0bxTMubakR48eOHnyJFasWIGrV6/i+vXrCAgIwIsvvmi2GB727rvvIiUlBUuXLi30/p9//hn//fcfvvrqq0fWIxmifPnyePrpp5mkWYgbN27o1eoopcTBgwfRuXPnIteSVahQQVcZKC1D90nbsmULmjRpgqZNm+puc3d3x/379w2+tkajwenTp4tsyeV6tIKMGRwC5FZx88vIyMDYsWPRqVMn1K1bF5988olqLfDmot2OyBY+oOW7fivBdkfzWb16NerVq4cnnnhC7VB01q9fj6SkpCIrAtpfosW1PAYGBsLOzs7gX4KHDh1Cz549sXr1anTv3r3QT+oK4+3tDUdHxyIraYGBgRgxYgS6dOmC1atXF/gQYvDgwbCzs8PmzZsNilVr+/btsLOzQ+/evY063ha4ubmhevXqZkvSpJQYM2YM/vvvP90QjzZt2pjl2loODg4YPXo0oqKicPnyZdXf8D3++OPo3bs3Fi1apKtiaMXGxmLGjBno0qVLiWuJ9NG3b1+cPXsWly5dKvW5qHT0TdIuX76M69evF7vvXoUKFRT7wNCQ6Y53797Fvn37HpkCXLFiRaOStIsXL6J58+b49ddfC71///79aNCgQZnfH03L0HbH2rVrw9vbG6tXr9btZ3f58mV0794dK1aswNSpU9GjRw98+umnj0wRtTVM0sjs2O5oPtpR1pYyJUlKiaVLl8LPzw9PPfVUoY9p0qQJgOKTtIMHD6Jly5YGTdo7dOgQevXqBS8vL6xduxaXL1/We+iInZ0dHnvssUIraZGRkXj++edRv359bN68+ZFPdqtUqQIXFxej23x27NiB9u3bl6o6YQvMOeFx3rx5+O233zBnzhy0aNECQO5aGTW4uLhYzEbt7733Hu7cuYPPPvsMZ86c0a0vmjFjBhITE7FkyRJFNqHVrmliNU19N27cgLe3d4mPCwwMBIBikzR3d3ckJSUpEpch7Y6//vorsrOz8frrrxe4vWLFikYljdquiML2KNRoNAgMDGSrYz6Gtjs6Ojrio48+wsGDB/Hnn39izpw58PHxQXh4OH7//XcsXLgQf/75Jz799FMEBwc/UnGzJUzSyOxYSTMPKSUOHDgAABYzYSo0NBTHjx/HhAkTim2JqVmzZpFJWmZmJg4fPlzsHjoP+++//9CrVy9Ur14d+/fvx5AhQzBmzBiEhobqfY46deo8UklLSkrCc889B2dnZ+zcuVPxN9O3bt1CWFhYmW511DJXkrZp0yZ89NFHGDp0KGbOnKm7ZsOGDU1+bUvXtWtXdO3aFXPnzoWfnx/c3NxQo0YNrF69Gu+88w58fX0VuU6DBg3QpEkTJmkqy8zMxN27d/WqpB08eBAeHh7w8fEp8jGmqKSVlKRJKbFixQq0b98efn5+Be4ztt0xPDwc9vb2hf5duR7tUYZW0gBg9OjRqFOnDl555RV8/PHHeP7553H27Fm89tprusdoq2gBAQGKxmtJtMOabOE9M5M0K8E1aYX7559/MGbMGNy7d0+R8509e1bXzmeOgQf6+Pnnn+Hq6ophw4YV+7imTZsWuVdaWFgY0tPTi/3ENr+jR4+id+/eqF69Ovbt26fbOLtGjRpITk7Wu8JVt27dR9qvVq5ciatXr2Ljxo26dWtK0lZCy+r+aPk1aNAA169ff6TVTklhYWEYPnw4nnrqKaxYsQJCCFy8eBEAzD7y3hIJIeDv74+QkBCsW7cOc+bMQa9evTBs2DDMmjVL0Wv16dMH+/fvV2zQBBlOu3WIPklaYGAgOnfuXOzvdiUrado1aSXtFXjo0CFERETgjTfeeOQ+YytpJ06cQNOmTQtNELXr0ZikPWBoJQ3IbWNdvnw5Ro0ahZCQEPz++++oVatWgce0aNEC3t7eut+TtkhbSbOF98yG/Qsg1bDdsXADBgwAAEybNk2Rioy2igagxHHGpaV9AS5uIqOUEtu3b8ezzz5bYptikyZNsHbt2kLvO3jwIACgU6dOesX24Ycfws3NrUCCBjxIXG/fvq3XG/AmTZpg1apVuv2icnJysGjRInTq1Mmgqp4hVq9ejbp16+pa7sqyBg0aQEqJy5cvm6T1MCYmBgMGDEC1atUKtK1GRUWhRo0aNjECWQkuLi5o164d2rVrZ9Lr9O3bF7/99hvOnz9vUWtqyxJ990i7efMmLl68iPHjxxf7OCUradrKgvZNbFFWrFiB8uXLF6jAaFWsWNGoKYHh4eFFfki4f/9+NGzY8JGEoiwzdHCIVq9evdCrV68i7xdCoHfv3ti8ebPR17B0OTk5EEIo0kauNkXe9QshegshzgkhLgoh3lfinNYgOTm52Ml1SlKz3TEpKQmHDh3CoUOHEB0drUoMhdFoNChXrhymTp1abLuIIQ4cOKAb22rqdkftJrfF/QKOiIjAtWvX8Nxzz5V4vjp16uD+/fuFTgI7ePAgmjRpopsAVZzz589j165dmDhxYoEEDXiQuOq70fTDA022bNmCK1eu4O2339breEMdP34cgYGBmDRpkk28QJfW448/DgDYuHGj4udOSUnBgAEDkJycjG3bthX4txUVFVXkSHEynaeffhq3bt1igqYifZM07QdnJX1Y5e7ujuTk5BL329OH9nebdkR5YRITE7FhwwYMGjSo0M2APT09ce/evRITvfzu3buHmJiYQtej5eTk4MCBA6yiPSQnJ8dk7/mee+45JCQkICQkxCTnV5spf3bmVuokTQhhD2AxgOcA+AAYLIRQ5h2zBdNoNOjbty/q1auHRo0aYfLkydixYwfu37+P1NRUvTYxNfR6alTS4uLi8MQTT+Cpp57CU089hR9//NHsMRQlOjoaaWlpiq3p0K5H044I1icxKg1tklZcf7+2JUGfWLQL1R9eEKzRaBAcHKx35WrZsmVwcHAotNXF0CRNW72JjIwEAHz77beoX7++rgJaWlLKAq2uixYtgpubG0aPHq3I+a1d8+bN8eqrr2L+/PmKTv3TaDR4/fXXdYvSH163cvHiRa5HU4G9vT07LlSmff0taXBIYGAg3NzcSkyoK1SoAACKjOHXJmlFjcEHcrdFSU1NLfT1H8gd9a7RaAxaYqAdGlJYd8PJkyeRkJDAJO0hpqxyPfPMM7C3t7fZlkcmaQW1A3BRSnlJSpkJ4HcAz5dwjNVbsWIFAgMDMXbsWDRp0gQrV65E3759UalSJbi5ucHNzQ0nTpxQ7HpqtDtmZmbipZdeQkxMDH799Vf4+/srMipaKdrqTP49XErjypUrul+wtWrVMvmn0dpfviUlab6+vnqNJS4qSTt9+jQSEhL0StJSU1Px888/46WXXiq03TN/u6M+6tWrBycnJ4SFhSEkJAT//fcfpk6dqtgL6KRJk3TP/+3bt7F+/XqMHDkSlSpVUuT8tuDbb7+Fg4MDJk+erBvNXFozZ87E5s2b8e233z4yoCUlJQU3b95kJY3KpBs3bsDe3h5Vq1Yt9nEHDx7EU089VeKGu9o2dyXWpemTpK1YsQJ+fn5FtuYa+jsAgO69UGFJ2o4dOwBwf7SHGTM4RF+VKlVC+/btbTZJs6UZDkqk6TUBXMv3fQyAJx9+kBBiLICxgOVMzTPWzZs3MWPGDDz99NNYtmwZhBBIT09HYGAgwsPDdW+E9Fk4rC812h1nzZqFAwcO4LfffsPQoUPNem19KJ2kXb9+Xff1gAEDTN4uZ29vj/LlyxeZpCUlJeHgwYOYOnWqXufTJmnadhstbVuNPkND1q9fj4SEBLz55puF3l+1alUIIfSupDk4OODll1/G6tWrcejQIbi7u2PkyJF6HauP+vXrIzY2FnFxcVi2bBkyMzMxefJkxc5vC2rWrInZs2dj+vTp+PvvvzFw4MBSnW/JkiX4/PPPMW7cOEyZMuWR+7UVO1bSqCzSjt8v7k1ifHw8Tp06hVdeeaXE82k/zEtMTHyk/dxQJSVp4eHhCA0NxaJFi4r8/adta9Z3v0zteb28vB4ZxpWSkoJFixbhmWee4Xq0h5i6GtS7d2/MmjULd+/ehaenp8muowZW0owgpfxRStlGStmmpE+YLN3UqVORnp6O5cuX617IXFxc8Oyzz+Ldd9/FjBkzMGPGDEUHTxhaScvKysKkSZPw8ssvY+7cuQZfT0qJ3377DQMHDjRZgpaVlVUgMTKUdi2XUnth3b17V/f1wxt4mkpxG4Pu3bsXWVlZerddFlVJO3r0KLy8vFCnTp1ij5dSYvHixfDz8yuy6ubg4ICqVavqnaQBwEcffYT09HTExMRg6dKlujcdStC2U4aHh2PJkiXo27cvGjdurNj5bcXkyZPh5+eHqVOnGj35T0qJefPmYeLEiejfvz++//77Qt/Iacfvs5JGZZE+G1kHBwdDSqlXd4M5K2krV66Es7NzsZOEjamkhYeHF7oebenSpYiNjcUnn3yi97nKCnt7e0WXzDyse/fuAAoOS7MVTNIKug4gfy9WrbzbbNI///yDjRs3YtasWWZ9M2hoJW3btm1YvHgx9u3bh1mzZhn8An/27FncuHHDpHtNHT9+HLVq1cLff/9tsmsYIi4uTve1ufrjixtnvHPnTpQvX17viYweHh5wdnZ+JElLTk5G5cqVS6wMHjlyBMePH8ebb75Z7GO9vLwM+gXdrFkzXLhwAVevXsWQIUP0Pk6f1jxtkvbmm2/izp07elcdyxpHR0csWbIE0dHRRn1oo9Fo8M4772DmzJkYNmwY/vzzzyLbtLTj95mkUVmkz0bWu3btgpOTk17TPrVJmhITHotL0uLi4rBmzRq88MILxU5KNrSSlpmZiYiIiEdaHVNSUvDll1/imWeeQceOHfX9K5QZzs7OJt06pU2bNnB1ddVtf2BLmKQVFAqgkRCinhDCCcAgAFsVOK/FSU5OxsSJE+Hr64t3333XrNc2tMf2p59+Qo0aNbBq1SpIKQ1eH7d7924AuQtMTeXw4cMAgNatW5vsGobQ/tLp169fifvIKKWoSpqUEjt37kSPHj30jkUIAW9v70eStPT0dDg7O5d4/OLFi1GhQoUS92OrXLmywfvS1atXr8QNVPOTUiI9Pb3EYx577DG4uLjg3LlzmDlzJnr27GlQXGVJ586dMWLECHz99ddFbnpemOzsbIwePRrffvstJk+ejF9++aXYdTRRUVGoXLkyPDw8lAibyKrcvHmz2CQtMTERq1evxssvv4xy5cqVeD7thEVTDw7Rfpj7wQcfFHsODw8P2Nvb6/1B3dmzZ5GZmflIkrZs2TJW0Yrh4uKi29fOFJycnNCpUyfs27fPZNdQi5rT0JVW6iRNSpkNYBKAAACRAP6QUp4p7Xkt0axZsxATE4OffvrJbG/itQxpd4yOjoa/vz9GjRql+6Tu2LFjBl1v9+7daNiwoUk2G9Y6dOgQatWqZTG96NoFzIMHDzbbNYtK0iIjIxEdHW3whMnCkrSMjIwSk527d+9iw4YNGD58eIntiOZYkJuZmQkpZYlvYuzt7bFq1SoEBATgs88+49j9Enz55Zdwc3PDxIkT9apUpqen45VXXsHq1asxe/ZsLFq0qMTnPyoqiuvRqMxKT0+Hm5tbkfevXLkSSUlJmDZtml7n037ApsQb9qKStBMnTmD58uWYOHGibtuOotjZ2aFatWp6V9IKm+zIKlrJTF1JA4Bu3brhzJkzBq0vtAa2tK+wIn8LKeUOKWVjKWUDKaXhvTRWIDQ0FN999x0mTJiADh06mP36hnwy8PPPP0NKiVGjRsHb2xve3t44evSo3tfKysrC/v37TV6VOHTokCo/y6IEBQUBgElbPB9WVJJmyOj9/Ly9vR8ZHKJPRerLL79EZmYmJkyYYND1TCU1NRUA9NoMefDgwXj22WdNHZJNqFatGubOnYu9e/eiTZs2+PHHH4tshU5MTESfPn2wZcsWfP/99/j444/1SoIvXrzIVkeiQmRnZ+O7775D586d0aZNG72OMXWSJqXEpEmTULlyZcyePVuv81SrVk3vSlp4eDicnZ3RpEkT3W3Lli3DnTt3WEUrhqkracCDZR22ti6N7Y5ljJQSb775JqpXr4558+apEoO+7Y45OTlYuXIlnnnmGdSrVw9AbjuhIUlaSEgIkpOTTdrqePPmTVy9etWikjQtc45vLy5J8/HxMXgS6qRJkx5Zc1RSkhYYGIivv/4ao0ePVmzPudJKS0sDAL3agcgw48ePx9KlS5GVlYVx48bB29sbY8eORVhYmK66Fhsbi+7duyMwMBC//fYbJk2apNe5s7OzER0djfr165vyr0BklbZs2YIrV67oXUUDHiRpSlRVtB1AmZmZutvWrl2L4OBgzJ8/X+8WZS8vL72rL6dOnYKPj49uzy9W0fRjjkpa69atUb58eZtreWSSVsacOHECYWFhmDlzpm4DYnPTt3y7a9cuXLt2DWPGjNHd1rp1a5w9e1bvqW67d++GnZ0dunXrZnS8JdGuR2vfvr3JrmGI8+fPq3LdKlWq4N69ewU+2UxOTsbBgweN2ky7W7dueOGFFwrcVlySdv/+fbz++uuoX78+Fi5caPD1TMWQShoZxs7ODuPHj0d4eDgOHz6M1157DWvXrkXbtm3RqlUrLFq0CF26dMGZM2ewZcsWg6a7ajdTzs7ONuHfgMg6LViwAPXr18eAAQP0PkbJSpp2IIh2knFSUhJmzJiBNm3aYNSoUXqfx5BKmp2dXYE3zKyi6cfFxcXkSZqjoyM6d+5sc8NDmKSVMb/88gucnJwwaNAg1WLQt93xp59+QtWqVQuMkG/dujU0Go3ew0MCAgLQrl07ky78P3ToEJycnNCqVSuTXcMQ27ZtAwC0bdvWrNf19fVFVlYWzp07p7tt69atyMzMVLTt8urVq4W+cZ48eTKuX7+O3377TbdA3RKwkmZ6Qgg8+eSTWLlyJW7cuIElS5YAAN566y3cuHEDAQEB6Nevn8HnrF69+iPrIonKupCQEPz333+YOnWqQW8glUzSvL29IYTQbX0zZ84c3Lx5Ez/88INBa3i0lTR91rVWr15dl9CxiqY/Z2dnk7c7Arktj5GRkQZtq2PpbGkza9v4W5hQVlYW1q1bhwEDBhQ7ltbU9PlHd+vWLfzzzz8YMWJEgcEm2umJ+gwPuXfvHkJDQ9GrV6/SBVyCw4cPo1WrVnpNHTQHbZJm7kmT2sXU2sXVGo0G8+bNg4+Pj2LbALzzzjs4fvw45syZU+D2P/74A7/++is++ugji6loarGSZl4VK1bEhAkTcOzYMd0ffTY/L0xhw2uIyroFCxbA3d0dI0eONOg4pdekVatWDdevX8fZs2excOFCjBw5Ek8++aRB5/H29kb58uV1H6YVR7tly+3btzFixAhW0fSkraTpkwiXhrZjypbWpdlSJc1B7QAsnb+/P2JjYzF8+HBV49Cn3XHNmjW6cdn5eXt7o1KlSnqN3f7333+h0WhMmqRlZWUhLCwM48aNU+R8pX0Ru3//vu4FytPTU4mQ9NakSRM4OTkhPDwcQ4cOxebNm3HmzBmsW7dOsU+CXn/9dezZswdz5sxBZGQk/Pz80KBBA0yePBnt2rXDzJkzFbmOklhJU4cQAk888USpzuHt7a3b0JqIgCtXrmDTpk2YNm1aidNzH+bo6AghhO6Dq9KqWbMmDhw4gMjISLi6umL+/PkGn2P69OmYPn26Xo+tXr06MjMz0bhxY6SlpWHu3LmsoulBm5xnZWWZdJr4E088gQoVKmDfvn147bXXTHYdc3r++efN3hVlKkzSSrBmzRpUrVoVvXv3VjUOfdod9+3bBz8/PzRt2rTA7UIING7cWK91VwEBAahUqZJJ/4GHh4cjLS2t1ENDtOusShp5XJJdu3ZBo9EAyF0jZk6Ojo7w9fVFaGgoNBoNPv30UzRu3BivvvqqotdZvHgxpJQIDg7Gpk2bIKWEq6srfvvtt2L3vFILkzTr5e3trZuUSlQWaTQaREZGYufOndi5cycCAwNhZ2eHyZMnG3wuIQR8fX0VG+4wbtw4fPDBB7hw4QIWLlwILy8vRc5bFO2k15YtW2L58uWPvD+hwuV/f2PKJM3BwQFdu3aFv7+/zbQJqrk0SWnW/2yY0L1797B161YMGTJE9TeyJVXSpJQIDQ3V7Yv2MH2SNCklAgIC0LNnT90kJlPQDg0pbZKmncKYkJBQqvNoWx0B81fSAKB///7Yv38/BgwYgJMnT+Kjjz5SvFTv5uaGX375BRcvXkRycjLCwsJw9OhRNGrUSNHrKIXtjtbL29sbcXFxBSbIEZUV9vb2WLBgAXx8fDB9+nTcvHkTU6ZMwaFDhwye1qs1fPhwHDp0SJEBV2PHjsX169fx33//YcqUKaU+X0n69euH48ePY//+/UzQDKBkm2tJhg4diqtXr2L37t0mvxYZhpW0Yvzxxx/IzMzEiBEj1A6lxEra1atXERcXV2QFrFGjRvjtt9+QlpZWZHUiIiIC169fN/l6tEOHDqFmzZqoXbt2qc6jHWwSHx+PmjVrGnWOnJwc7Ny5E/Xq1cPly5fNXkkDgI8//hhXrlzBmjVr0KBBA5Nvpu3q6mr2tXeGYiXNelWvXh0AcPv27VL/P05kbX755RecPn0aXl5e6NWrl9GJWX5Dhw5FcnIy3N3dFYgwt0pjru1vhBBo2bKlWa5lS/JX0kzthRdegKenJ5YvX27y939kGCZpxfjll1/g5+dnES8wJZWhQ0NDAaDIDTIbN24MAIiKioKfn1+hjwkICAAAk28MfOzYMUXaKZWopIWGhiI2NhY9e/bE5cuXVamk2dvbY9WqVWjcuDG6du1q0iqmtWAlzXp5e3sDyN0LkUkalTUDBw7EwIEDFT1njRo19N5ommyDOStpzs7OGDVqFL755hvcuHEDNWrUMPk1ST9sdyzCuXPncPjwYYwYMQJCCLXDgRBCt26qMGFhYXByckLz5s0LvV+bpBXXLhEQEICmTZsq8slfcW7fvo1atWqV+jzaJC0+Pt7oc2zfvh12dna65FaNShqQm6jNnDkTnTp1UuX6lqZfv374999/UbVqVbVDIQPlT9KIiMhw5qykAcCYMWOQk5ODlStXmuV6pB8maUX49ddfYWdnZ9BGrqbk5ORUYMPjh4WGhuLxxx8vcqS9du1RUUlaRkYGAgMDTV7qzs7ORnx8vCIVK227Y2kqadu2bUPHjh11CbAalTR6VI0aNdCjRw+TLpgm02CSRkRUOi1btsSCBQtMPthFq2HDhujZsyd++ukn5OTkmOWaVDImaYXQaDT49ddf8eyzz+recKjNycmpyIX4Go0GR48eLbaFsEKFCvD29i4ySbt58ybS09OLrMQp5d69ewCgSIWktJW0mJgYnDhxAv369UNcXBwcHBwMHo9cFmVnZ9vEBCgyDS8vLwghmKQRERmpUaNGeOutt8zaTTJu3Dhcu3YN/v7+ZrsmFY/vtApx4MABREdHW8TAEK3ikrQLFy4gMTGxyPVoWsVNeIyLiwNg+na/2NhYAMpUrEq7Jm3Hjh0AgL59++Lu3buoUqWKRbS2Wrq7d++y4khFcnBwQIcOHfiBBxGRFXn++efh6+uLO3fuqB0K5eGEgkKsXbsW7u7ueP7559UORcfR0bHIJC0sLAwAShzG0ahRI/z999+F3qetcJk6Sbt79y4AZZI0BwcHlC9f3ugkbdu2bahbty58fHwQFxen2no0a3Pnzh08/fTTaodBFiw4OFjtEIiIyACOjo44deoUP6y2IKykFeLIkSPo3LmzRY3/Lq6SFhoainLlyqFZs2bFnqNx48aIjY0tNKkxtJKWlJSkm8BnCCWTNCB3XZox7Y5paWn4999/0a9fPwghkJKSgvLlyysSky3Lzs5GXFwcqlWrpnYoREREpCAmaJaFSdpDsrOzce7cOfj6+qodSgHFJWlhYWFo1apViaPbtRMeL1y48Mh9hiRp6enpaNq0KSpUqABfX18MGzYMISEhJR4HPEjSlOqzrlSpklGVtP379yMtLQ19+/YFAGRmZhY5dIUe0P47YZJGREREZDpM0h4SFRWFzMxMi0zSCpvumJ2djWPHjpW4Hg0A2rdvjzVr1qBu3bqP3Kd98125cuUSz/P777/jxo0bGD16NBo0aICdO3eif//+evUxa9ekKdVaWKlSJaMqaRs2bICrq6uubS8jI4NJmh60zzGTNCIiIiLTYZL2kDNnzgAAfHx8VI6koKIqaZGRkUhLS9Nrc2gvLy+8/vrrhVax4uLi4O7uDkdHx2LPIaXE999/Dx8fHyxfvhxbt27FgQMHcP/+fYwbNw5SymKPv3v3Ltzd3RUbre7h4WFwJe3IkSNYs2YNxowZo9uLhEmafrRJGvcvIyIiIjIdJmkP0SZpJa3vMreikrTQ0FAA0KuSVhx9B2ccOnQIx44dw+TJk3W9y35+fpg7dy62bNmCX3/9tdjjlZ4MWKlSJd3QE31kZ2dj/Pjx8Pb2xqeffqq7PSMjg3ty6YGVNCIiIiLTY5L2kIiICNSrVw9ubm5qh1JAUUlaWFgY3N3ddZtVG+vevXt6tTp+//33qFixIoYNG1bg9mnTpqFz586YPHkyoqOjizz+7t27ilZhPD09da2a+li8eDGOHz+OhQsXwt3dXXc716TpJzMzE5UrV2aSRkRERGRCTNIecubMGYtrdQSKT9Jat25d6s2F9amk3bhxA5s2bcKoUaMemYRob2+P1atXIy0tDT/88EOR54iNjVW0kla1alWkpqbqNWny+vXrmDVrFnr16oWXX365wH1sd9TPiBEjuF0BERERkYkxScvHUic7AoUnaRqNBmfOnEGLFi1KfX593ngvX74cOTk5mDhxYqH3169fHx06dMDevXuLPIfS7Y7aqpx2IElxpk2bhszMTCxevPiRMbNM0oiIiIjIUjBJy8dSJzsChSdpMTExSE1NRdOmTUt9/vj4eHh4eBR5f0ZGBpYvX44+ffqgQYMGRT6uR48eOHbsWJHrxJRO0rTn0o72L0pAQAA2btyImTNnFho/16QRERERkaVgkpaPpU52BApP0s6dOwcAaNKkSanPX758eSQnJxd5/8cff4zbt29j2rRpxZ6nR48ekFJi//79j9ynbUtUck2aPpW0tLQ0TJw4EY0bN8aMGTMKfQzXpBERERGRpWCSlo+lTnYEAEdHR5MmadWrV8etW7cKvc/f3x9ffvklxo0bhx49ehR7nrZt28LNzQ179ux55D5ttcvc7Y7z589HVFQUli5dWmQixnZHIiIiIrIUTNLyOXPmjEVOdgQKr6SdPXsW7u7uqF69eqnPX1SSduPGDQwfPhzNmzfHggUL9IqzS5cuha5LS0pKAoACUxVLq6R2x3PnzuGLL77A0KFD0b1790Ifk5OTg5ycHCZpRERERGQRmKTlExERYZGtjkDR7Y5NmjR5ZAiGMby8vHD79u0Ct+Xk5OD1119HSkoKNmzYgHLlyul1rh49euDs2bO4fv16gdu18SuZDFWqVAkODg6FVtKklHjzzTdRrlw5fPPNN0WeIyMjAwC4Jo2IiIiILAKTtDyWPNkRyE0gtMmE1rlz5xQZGgLkVtJiY2ORk5MDAEhMTMTAgQOxd+9e/PDDDwa1gGpbIh+upmmTNCWTISEEPD09C03SNmzYgL1792L+/Pnw8vIq8hymSB6JiIiIiIzFJC3PxYsXLXayIwB4eHggOTlZl1CkpKTg2rVriqxHA3KTNI1Gg7t37+LKlSvo2LEjdu7cicWLF2PkyJEGnevxxx9HlSpVzJKkAbnr0h5O0pKSkjB9+nS0bt0aY8eOLfZ4bfLLJI2IiIiILIGD2gFYCkue7AgAderUgZQS165dQ4MGDXD+/HkAygwNAaBb19aoUSNkZGTA1dUV/v7+6Nmzp8HnsrOzQ7du3bBnzx5IKXXtmNokzdHRUZGYtTw9PR9Zk/bpp5/ixo0b+Ouvv2Bvb1/s8UzSiIiIiMiSlKqSJoT4SghxVghxUgixWQhRSaG4zC4iIgKAZU52BIC6desCAK5cuQIgd2gIAMXaHZ955hm8//77GDlyJKZMmYIjR44YlaBp9erVC9euXUNwcLDuNnNV0iIiIrBw4UKMHj0aTz75ZInHa5O4tLQ0ReMiIiIiIjJGadsddwPwk1I+DuA8gA9KH5I6LHmyI/AgSbt69SqA3PVoQgg0bNhQkfNXqFAB8+fPx6JFi/DVV1+hUaNGpTrfkCFD4OXlhU8++UR3mzmSNCklJk+ejPLly2P+/Pl6HV+jRg14eXnh8OHDisZFRERERGSMUiVpUspdUsrsvG8PA6hV+pDUcfPmTdSuXVvtMIpUs2ZN2NnZ6Spp586dQ7169eDi4qJuYEVwdXXFBx98gL179+o2tjZVkubp6Yn4+HhkZ2dj48aN2Lt3L+bOnav3ptlCCHTq1KlA1Y+IiIiISC1KDg4ZBWBnUXcKIcYKIcKEEGHFbTysJjs7y52j4ujoiFq1aukqaWfPnlVsPZqpjB07FjVq1MAnn3wCKaVJK2kAEB0djbfffhtPPPEExo0bZ9A5OnXqhCtXriAmJkbR2IiIiIiIDFViViKE+FcIcbqQP8/ne8xMANkA1hZ1Hinlj1LKNlLKNvpWOKigOnXq4PLly9BoNDh//rzFJ2nlypXDhx9+iMDAQOzZs8dkg0O0/56mTp2K69evY/HixSUOC3lYp06dAIDVNCIiIiJSXYlJmpSyp5TSr5A/fwOAEOJ/APoBGCqllCaOt0zr0KEDgoKC8H//939ITU1F+/bt1Q6pRG+88QZq1qyJzz77THebEptv5+fp6QkA2LZtG/73v/+hQ4cOBp+jRYsWcHV1ZZJGRERERKor7XTH3gBmABggpUxVJiQqysyZM1GzZk3MmTMHtWvXxosvvqh2SCVydnbGO++8gwMHDpgsAcpfmf3888+NOoejoyPat2+PoKAgpcIiIiIiIjJKaRdh/QCgAoDdQogTQohlCsRERXB3d8eyZbk/4nfffVfxtkFTGTNmDKpUqYJVq1ahXLly8PLyUvT8gYGBAHK3IyjNuTt16oTw8HAkJSUpFRoRERERkcFKO92xoZSytpSyZd6f8UoFRoXr27cvrl69ikmTJqkdit7c3NwwdepUSCnx9NNPKzqRMiUlBXPmzAEADBo0qFTn6tixIzQaDUfxExEREZGqLHecIRXpscceU3xdl6lNmjQJtWvXLnUi9bB58+bh9u3bAIDExMRSnat9+/aws7NjyyMRERERqcpB7QCobPDw8EB0dLSi57xw4QK+/vprDBs2DPv370d8fHypzufu7o4WLVowSSMiIiIiVbGSRlZJSompU6fC2dkZX375JTw8PHDv3r1Sn7dTp04ICQlBVlaWAlESERERERmOlTSySv/88w927tyJb775Bt7e3qhcuXKpK2kAMHz4cLRr1w4ajUaBKImIiIiIDMckjaxOWloa3nrrLfj4+GDy5MkActspo6KiSn3uNm3aoE2bNqU+DxERERGRsZikkdX56quvcPnyZezZs0e3DUHlypURGhqqcmRERERERKXHNWlkVS5fvoz58+fj1VdfRffu3XW3e3h4KNLuSERERESkNiZpZFXefvtt2NnZ4euvvy5we+XKlZGamoqMjAyVIiMiIiIiUgaTNLIa/v7+2LJlC2bNmoXatWsXuM/DwwMAWE0jIiIiIqvHJI2sQkZGBqZMmYJGjRph2rRpj9xfuXJlAFBkDD8RERERkZo4OISswoIFC3DhwgXs3LkTzs7Oj9zPShoRERER2QpW0sjiRUdHY86cOXj++efRu3fvQh/DShoRERER2QomaXnKly+P+/fvqx0GFWLKlCkAgEWLFhX5GFbSiIiIiMhWMEnLU79+fURFRUFKqXYolM/WrVvx999/45NPPkGdOnWKfBwraURERERkK5ik5WnYsCESExNx9+5dtUOhPCkpKZg8eTJ8fX0LHRaSX8WKFQGA1VAiIiIisnocHJKnQYMGAICoqChUrVpV5WgIAObMmYPo6GgEBgbC0dGx2MdqK6BCCHOERkRERERkMqyk5WnYsCEA4OLFiypHQgBw+vRpfPPNNxg1ahQ6d+5c4uNzcnIAAPb29qYOjYiIiIjIpJik5alXrx6EEIiKilI7lDJPo9FgwoQJcHd3xxdffKH3MQCTNCIiIiKyfmx3zOPs7IzatWuzkmYBtm/fjqCgIKxYsQKenp56HcNKGhERERHZClbS8mnQoAEraRZg69atcHd3x/Dhw/U+Rpuk2dnxnzQRERERWTe+o82nQYMGrKSpTEoJf39/PPPMMyUOC8mPlTQiIiIishVM0vJp2LAhYmNjkZiYqHYoZdaZM2cQExOD5557zqDjmKQRERERka1gkpZP/jH8pI6dO3cCAHr16mXQcUzSiIiIiMhWMEnLRzuGn0maevz9/dG8eXPUqlXLoOM43ZGIiIiIbAWTtHy0lTSuS1NHUlISDh48aHCrI8BKGhERERHZDiZp+VSoUAFVq1ZlJU0le/fuRVZWFnr37m3wsUzSiIiIiMhWMEl7SMuWLfHPP/8gPj5e7VDKnCNHjsDe3h4dO3Y0+FgmaURERERkK5ikPeTzzz/H3bt38d5776kdSpmTkJCASpUqwcnJyeBjuU8aEREREdkKvqN9SKtWrTBt2jT89NNPOHDggNrhlCn3799HxYoVjTqWlTQiIiIishVM0goxe/Zs1KtXD2PHjkV6erra4ZQZpUnSON2RiIiIiGyFIkmaEGK6EEIKITyVOJ/aXF1dsXz5cpw/fx6fffaZ2uGUGaykEREREREpkKQJIWoDeBZAdOnDsRzPPPMMhg8fji+++AKnTp1SO5wygUkaEREREZEylbQFAGYAkAqcy6J88803qFSpEsaMGaNLAsh0mKQREREREQEOpTlYCPE8gOtSynAhREmPHQtgLAA89thjpbms2Xh6euKHH35AVFQUpLS5HNTiMEkjIiIiItIjSRNC/AugeiF3zQTwIXJbHUskpfwRwI8A0KZNG6vJeF577TW1QygTpJRITEw0Okm7fv06gNzEmoiIiIjImpWYpEkpexZ2uxCiOYB6ALRVtFoAjgkh2kkpbykaJdm85ORkaDQao5O08PBw2NnZwc/PT+HIiIiIiIjMy+h2RynlKQDVtN8LIa4AaCOlvKtAXFTG3L9/HwCMTtJOnDiBxo0bw9XVVcmwiIiIiIjMjvukkUVITU0FALi4uBh1fHh4OFq0aKFkSEREREREqlAsSZNS1mUVjYxVu3ZtCCFw6dIlg4+9f/8+rly5wiSNiIiIiGwCK2lkEcqVK4f69evjzJkzBh978uRJAGCSRkREREQ2gUkaWQxfX19EREQYfNyJEycAAC1btlQ2ICIiIiIiFTBJI4vh4+OD8+fPIysry6DjwsPD4enpCW9vbxNFRkRERERkPkzSyGL4+voiKysLFy9eNOg47dCQkjZUJyIiIiKyBkzSyGL4+voCgEHr0rKzs3Hq1CmuRyMiIiIim8EkjSxGkyZNIIQwaF3a+fPnkZGRwfVoRERERGQzmKSRxXB1dTV4wmNYWBgATnYkIiIiItvBJI0sio+Pj96VNCklvv/+ezRo0EDXKklEREREZO2YpJFF8fX1xblz5/Sa8Lhr1y6EhYXh/fffh729vRmiIyIiIiIyPSZpZFF8fHz0nvD42WefoVatWhg+fLgZIiMiIiIiMg8maWRRWrVqBQDYtGlTsY8LDAxEUFAQ3nvvPTg5OZkjNCIiIiIis2CSRhbF19cXr7zyCubNm4crV64U+bjPPvsMXl5eGD16tPmCIyIiIiIyAyZpZHG+/fZb2NvbY+rUqYXef+TIEezevRvTp09HuXLlzBwdEREREZFpMUkji1OrVi188skn2Lp1K7Zt2/bI/XPnzoWHhwfGjx+vQnRERERERKbFJI0s0tSpU9GsWTNMmTIFaWlpAIBLly7h5ZdfxtatW/HWW2+hQoUKKkdJRERERKQ8B7UDICqMk5MTlixZgm7dumHWrFlwdHTEt99+CwcHB3z66ad477331A6RiIiIiMgkmKSRxXr66acxZMgQfPPNNwCAESNGYO7cuahZs6bKkRERERERmQ6TNLJoCxYsQJUqVfD666+jbdu2aodDRERERGRyTNLIolWrVg3fffed2mEQEREREZkNB4cQERERERFZECZpREREREREFoRJGhERERERkQVhkkZERERERGRBmKQRERERERFZECZpREREREREFoRJGhERERERkQVhkkZERERERGRBhJTS/BcVIhbAVbNf2Lw8AdxVOwhSDJ9P28Ln03bwubQtfD5tC59P28Hn0jTqSCmrFnaHKklaWSCECJNStlE7DlIGn0/bwufTdvC5tC18Pm0Ln0/bwefS/NjuSEREREREZEGYpBEREREREVkQJmmm86PaAZCi+HzaFj6ftoPPpW3h82lb+HzaDj6XZsY1aURERERERBaElTQiIiIiIiILwiSNiIiIiIjIgjBJMwEhRG8hxDkhxEUhxPtqx0OGEUKsEkLcEUKczndbZSHEbiHEhbz/eqgZI+lHCFFbCLFPCBEhhDgjhJiadzufTyskhHARQhwRQoTnPZ+z826vJ4QIyXvN3SCEcFI7VtKPEMJeCHFcCLEt73s+l1ZKCHFFCHFKCHFCCBGWdxtfa62UEKKSEGKTEOKsECJSCNGBz6d5MUlTmBDCHsBiAM8B8AEwWAjho25UZKDVAHo/dNv7APZIKRsB2JP3PVm+bADTpZQ+ANoDmJj3/yOfT+uUAaC7lLIFgJYAegsh2gP4AsACKWVDAPEARqsXIhloKoDIfN/zubRu3aSULfPtp8XXWuu1CIC/lLIpgBbI/f+Uz6cZMUlTXjsAF6WUl6SUmQB+B/C8yjGRAaSUgQDuPXTz8wB+yfv6FwADzRkTGUdKeVNKeSzv6yTk/pKpCT6fVknmSs771jHvjwTQHcCmvNv5fFoJIUQtAH0BrMj7XoDPpa3ha60VEkJUBNAFwEoAkFJmSikTwOfTrJikKa8mgGv5vo/Ju42sm5eU8mbe17cAeKkZDBlOCFEXwBMAQsDn02rltcedAHAHwG4AUQASpJTZeQ/ha671WAhgBgBN3vdVwOfSmkkAu4QQR4UQY/Nu42utdaoHIBbAz3ntyCuEEG7g82lWTNKIDCRz963g3hVWRAhRHsCfAN6SUibmv4/Pp3WRUuZIKVsCqIXczoWm6kZExhBC9ANwR0p5VO1YSDGdpJStkLvcY6IQokv+O/laa1UcALQCsFRK+QSAFDzU2sjn0/SYpCnvOoDa+b6vlXcbWbfbQghvAMj77x2V4yE9CSEckZugrZVS/pV3M59PK5fXerMPQAcAlYQQDnl38TXXOnQEMEAIcQW5ywK6I3cNDJ9LKyWlvJ733zsANiP3QxS+1lqnGAAxUsqQvO83ITdp4/NpRkzSlBcKoFHehConAIMAbFU5Jiq9rQBG5H09AsDfKsZCespb47ISQKSU8tt8d/H5tEJCiKpCiEp5X5cD8Axy1xnuA/By3sP4fFoBKeUHUspaUsq6yP09uVdKORR8Lq2SEMJNCFFB+zWAZwGcBl9rrZKU8haAa0KIJnk39QAQAT6fZiVyq5WkJCFEH+T22tsDWCWlnKtuRGQIIcR6AE8D8ARwG8AnALYA+APAYwCuAnhVSvnwcBGyMEKITgAOAjiFB+tePkTuujQ+n1ZGCPE4cher2yP3Q8Y/pJSfCiHqI7caUxnAcQDDpJQZ6kVKhhBCPA3gHSllPz6X1inveduc960DgHVSyrlCiCrga61VEkK0RO5QHycAlwCMRN7rLvh8mgWTNCIiIiIiIgvCdkciIiIiIiILwiSNiIiIiIjIgjBJIyIiIiIisiBM0oiIiIiIiCwIkzQiIiIiIiILwiSNiIiIiIjIgjBJIyIiIiIisiD/D42QejR5m9TKAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1080x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect our dataset.  It returns a tuple of stroke offsets and matching ascii strings\n",
    "# You can see that there isn't an exact match for every subject for strokes to ascii \n",
    "# because of the token limit in Transformers\n",
    "SUB = 12\n",
    "\n",
    "for s, l in train.batched_onehot_set.take(2).cache():\n",
    "    plot_stroke(s[0][SUB, :, :], s[1][SUB, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Much of the following code is from the excellent Tensorflow tutorial on Transformers: https://www.tensorflow.org/text/tutorials/transformer\n",
    "\n",
    "# STEP 1 - Positional Embeddings from the original paper. Although, you can also just add a randomized vector and I may try that next\n",
    "# TODO: Switch to a random vector and see if performance suffers vs this complex embedding.\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / jnp.power(10000, (2 * (i//2)) / jnp.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(jnp.arange(position)[:, jnp.newaxis],\n",
    "                          jnp.arange(d_model)[jnp.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads = angle_rads.at[:, 0::2].set(jnp.sin(angle_rads[:, 0::2]))\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads = angle_rads.at[:, 1::2].set(jnp.cos(angle_rads[:, 1::2]))\n",
    "\n",
    "  pos_encoding = angle_rads[jnp.newaxis, ...]\n",
    "\n",
    "  return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up some pieces in haiku. See: https://github.com/deepmind/dm-haiku/tree/main/examples/transformer\n",
    "\n",
    "def layer_norm(x: jnp.ndarray) -> jnp.ndarray:\n",
    "  \"\"\"Applies a unique LayerNorm to x with default settings.\"\"\"\n",
    "  ln = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)\n",
    "  return ln(x)\n",
    "\n",
    "def point_wise_feed_forward(x: jnp.ndarray, d_model: int, dff: int) -> jnp.ndarray:\n",
    "  mlp = hk.Sequential([\n",
    "      hk.Linear(dff, name='Lin1'), jax.nn.relu, # (batch_size, seq_len, dff)\n",
    "      hk.Linear(d_model, name='Lin2'),          # (batch_size, seq_len, d_model)\n",
    "  ])\n",
    "  return mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out the point_wise_feed_forward network\n",
    "network = hk.transform(point_wise_feed_forward)\n",
    "params = network.init(rng=jax.random.PRNGKey(42), x=jnp.zeros((32, 100)), d_model=128, dff=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Encoder - Self attention over the input characters - The only mask needed is padded characters\n",
    "class Encoder_Layer(hk.Module):\n",
    "    # The Encoder Layer is one stack of the Encoder, putting the multihead together \n",
    "    # with the point wise network and some normalization layers\n",
    "    def __init__(self, key_size, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__(name='EncoderLayer')\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # In haiku the key_size is specified manually instead of d_model/num_heads. Internally, it \n",
    "        # will project Q, K, and V to dimensions (*leading_dims, num_heads, head_size) before \n",
    "        # computing attention logits. After that you can futher modify it to project to d_model.\n",
    "        # TODO: Define an initializer here?\n",
    "        self.mha = hk.MultiHeadAttention(num_heads=num_heads, key_size=key_size, model_size=d_model, w_init_scale=1)\n",
    "\n",
    "    # I don't think haiku has any method for dealing with removing dropout automatically, so we will need\n",
    "    # to always pass in a training flag to remove it if necessary during inference\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "        mask,\n",
    "        training=True\n",
    "    ) -> jnp.ndarray:\n",
    "        # Need to format the mask properly across q_vals * k_vals\n",
    "        mask1 = mask[:, None, :, None] \n",
    "        mask2 = mask[:, None, None, :] \n",
    "        mask = mask1 & mask2  # [B, H=1, T, T]\n",
    "\n",
    "        attn_out = self.mha(x, x, x, mask)\n",
    "        if training:\n",
    "            attn_out = hk.dropout(hk.next_rng_key(), self.dropout_rate, attn_out)\n",
    "\n",
    "        # residual 1\n",
    "        attn_out = x + attn_out\n",
    "        attn_out1 = layer_norm(attn_out)\n",
    "\n",
    "        ffn_out = point_wise_feed_forward(attn_out1, self.d_model, self.dff)\n",
    "        if training:\n",
    "            ffn_out = hk.dropout(hk.next_rng_key(), self.dropout_rate, ffn_out)\n",
    "        \n",
    "        # residual 2\n",
    "        ffn_out = attn_out1 + ffn_out\n",
    "        attn_out2 = layer_norm(ffn_out)\n",
    "\n",
    "        return attn_out2\n",
    "\n",
    "# The Encoder module handles the pre-processing of the character data - embedding + positional encoding\n",
    "# and looping over the requested number of encoder attention layers\n",
    "class Encoder(hk.Module):\n",
    "    def __init__(self, num_layers, key_size, d_model, num_heads, dff, maximum_positional_encoding, \n",
    "        dropout_rate=0.1):\n",
    "        super().__init__(name='Encoder')\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.enc_layers = [Encoder_Layer(key_size, d_model, num_heads, dff, dropout_rate) \n",
    "                       for _ in range(num_layers)]\n",
    "\n",
    "        # Postional encodings - enocodings are static in this case and not learned parameters\n",
    "        # TODO: Compare this to random\n",
    "        self.positional_embeddings = positional_encoding(maximum_positional_encoding, d_model)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "        mask,\n",
    "        training=True\n",
    "    ) -> jnp.ndarray:\n",
    "        # The mask for the encoder needs to be broadcastable to the last 2 dimensions (1, 1, T, T)\n",
    "        # because the multihead attention is parallel - See https://www.tensorflow.org/text/tutorials/transformer\n",
    "        \n",
    "        seq_len = jnp.shape(x)[1]\n",
    "        \n",
    "        # We are using one-hot encoded characters and not embedded words, so we will just use a\n",
    "        # prenet to connect that to our model of depth d_model instead\n",
    "        x = hk.Linear(self.d_model, name='prenet')(x)\n",
    "\n",
    "        x = x + self.positional_embeddings[:, :seq_len, :]\n",
    "\n",
    "        if training:\n",
    "            x = hk.dropout(hk.next_rng_key(), self.dropout_rate, x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask, training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Encoder - x input will be [B, T, d_model] embedded characters with positional encoding added\n",
    "x=s[1].numpy()\n",
    "\n",
    "mask = jnp.not_equal(jnp.sum(x, -1), 0)\n",
    "\n",
    "def encoder(x: jnp.ndarray, mask: jnp.ndarray) -> jnp.ndarray:\n",
    "    enc = Encoder(4, 32, 128, 4, 128, 200)\n",
    "\n",
    "    return enc(x, mask)\n",
    "\n",
    "network = hk.transform(encoder)\n",
    "key = jax.random.PRNGKey(42) \n",
    "params = network.init(rng=key, x=jnp.ones((32, train.MAX_CHAR_SEQ_LEN, 101)), mask=mask)\n",
    "\n",
    "out = network.apply(params, key, x=x, mask=mask)\n",
    "\n",
    "#params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Decoder - Very similar to the Encoder with a self-attention mechanism, but there is a second cross-attention mechanism with\n",
    "# the output of the Encoder as K, V and the outputs of the self-attention mechanism as Q. That is, as the Decoder attemps to draw\n",
    "# hand written text based on the Encoder characters, it asks what parts of the encoding are important. Hopefully it learns this \n",
    "# relationship and we should see that reflected in the attention weights. It uses additional causal-masking to prevent future tokens\n",
    "# from being attended to as it attempts to predict the next token.\n",
    "\n",
    "class Decoder_Layer(hk.Module):\n",
    "    # The Decoder Layer is one stack of the Decoder, putting the 2 multihead attention blocks together \n",
    "    # with the point wise network and some normalization layers\n",
    "    def __init__(self, key_size, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__(name='DecoderLayer')\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.mha_self = hk.MultiHeadAttention(num_heads=num_heads, key_size=key_size, model_size=d_model, w_init_scale=1)\n",
    "        self.mha_cross = hk.MultiHeadAttention(num_heads=num_heads, key_size=key_size, model_size=d_model, w_init_scale=1)\n",
    "\n",
    "    # Mask here will only deal with the padding mask. We will compute the causal mask as needed in the calling function\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "        enc_output: jnp.ndarray,\n",
    "        mask,       # Mask of stroke padding\n",
    "        enc_mask,   # Mask of character padding\n",
    "        training=True\n",
    "    ) -> jnp.ndarray:\n",
    "        seq_len = jnp.shape(x)[1]\n",
    "\n",
    "        # Need to format the mask properly across q_vals * k_vals\n",
    "        mask1 = mask[:, None, :, None] \n",
    "        mask2 = mask[:, None, None, :] \n",
    "        mask2 = mask1 & mask2  # [B, H=1, T, T]\n",
    "\n",
    "        # Compute the causal mask and combine with the padding mask for the strokes\n",
    "        causal_mask = np.tril(np.ones((1, 1, seq_len, seq_len)))  # [B=1, H=1, T, T]\n",
    "        self_mask = mask2 * causal_mask  # [B, H=1, T, T]\n",
    "\n",
    "        # Self-attention\n",
    "        attn_out = self.mha_self(x, x, x, self_mask)\n",
    "        if training:\n",
    "            attn_out = hk.dropout(hk.next_rng_key(), self.dropout_rate, attn_out)\n",
    "\n",
    "        # residual 1\n",
    "        attn_out = x + attn_out\n",
    "        attn_out1 = layer_norm(attn_out)\n",
    "\n",
    "        # Cross-attention\n",
    "        # Combine the 2 padding masks. We don't need to attend to encodings that are padded or \n",
    "        # query decodings that are padded\n",
    "\n",
    "        # Need to format the mask properly across q_vals * k_vals\n",
    "        # TODO: This code is repeated a lot. This needs to be refactored.\n",
    "        mask1 = mask[:, None, :, None] \n",
    "        mask2 = enc_mask[:, None, None, :] \n",
    "        cross_mask = mask1 & mask2  # [B, H=1, T, T]\n",
    "        attn_out2 = self.mha_cross(attn_out1, enc_output, enc_output, cross_mask)\n",
    "        if training:\n",
    "            attn_out2 = hk.dropout(hk.next_rng_key(), self.dropout_rate, attn_out2)\n",
    "\n",
    "        # residual 2\n",
    "        attn_out2 = attn_out1 + attn_out2\n",
    "        attn_out2 = layer_norm(attn_out2)\n",
    "\n",
    "        attn_out3 = point_wise_feed_forward(attn_out2, self.d_model, self.dff)\n",
    "        if training:\n",
    "            attn_out3 = hk.dropout(hk.next_rng_key(), self.dropout_rate, attn_out3)\n",
    "        \n",
    "        # residual 3\n",
    "        attn_out3 = attn_out2 + attn_out3\n",
    "        out_all = layer_norm(attn_out3)\n",
    "\n",
    "        # TODO: It looks like the haiku transformer does not allow the return of the attent weights,\n",
    "        # only the final projection. I am going to fork my own repo and add that (maybe pull request as well)\n",
    "        return out_all\n",
    "\n",
    "def decoder_prenet(x: jnp.ndarray, d_model: int) -> jnp.ndarray:\n",
    "  mlp = hk.Sequential([\n",
    "      hk.Linear(d_model, name='D_Prenet1'), jax.nn.relu,    # (batch_size, seq_len, d_model)\n",
    "      hk.Linear(d_model, name='D_Prenet2'), jax.nn.relu,   # (batch_size, seq_len, d_model)\n",
    "      hk.Linear(d_model, name='D_Prenet3')                 # (batch_size, seq_len, d_model)\n",
    "  ])\n",
    "  return mlp(x)\n",
    "\n",
    "# The Decoder module handles the pre-processing of the stroke data - embedding + positional encoding\n",
    "# and looping over the requested number of decoder attention layers\n",
    "class Decoder(hk.Module):\n",
    "    def __init__(self, num_layers, key_size, d_model, num_heads, dff, maximum_positional_encoding, \n",
    "        dropout_rate=0.1):\n",
    "        super().__init__(name='Decoder')\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.dec_layers = [Decoder_Layer(key_size, d_model, num_heads, dff, dropout_rate) \n",
    "                       for _ in range(num_layers)]\n",
    "\n",
    "        # Postional encodings - enocodings are static in this case and not learned parameters\n",
    "        # TODO: Compare this to random\n",
    "        self.positional_embeddings = positional_encoding(maximum_positional_encoding, d_model)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "        enc_output: jnp.ndarray,\n",
    "        enc_mask,\n",
    "        training=True\n",
    "    ) -> jnp.ndarray:\n",
    "        # The mask for the encoder needs to be broadcastable to the last 2 dimensions (1, 1, T, T)\n",
    "        # because the multihead attention is parallel - See https://www.tensorflow.org/text/tutorials/transformer\n",
    "        # We just check the pen up for a negative value to indicate a masked stroke (otherwise is should be \n",
    "        # 0 or 1)\n",
    "        # TODO: padding_value should be passed in or made global\n",
    "        mask = jnp.not_equal(x[:,:,2], train.padding_value)\n",
    "\n",
    "        seq_len = jnp.shape(x)[1]\n",
    "        \n",
    "        # Adding a small MLP here to give the network an opportunity to construct filters and non-linear relationships\n",
    "        # among the raw stroke data\n",
    "        x = decoder_prenet(x, self.d_model)\n",
    "\n",
    "        x = x + self.positional_embeddings[:, :seq_len, :]\n",
    "\n",
    "        if training:\n",
    "            x = hk.dropout(hk.next_rng_key(), self.dropout_rate, x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, enc_output, mask, enc_mask, training)\n",
    "\n",
    "        # TODO: Modify the mha from haiku to output attention_weights as well\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 200, 128)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Decoder - x input will be [B, T, d_model] embedded characters with positional encoding added\n",
    "x=s[1].numpy()\n",
    "\n",
    "mask = jnp.not_equal(jnp.sum(x, -1), 0)\n",
    "\n",
    "x=s[0].numpy()\n",
    "\n",
    "def decoder(x: jnp.ndarray, enc_output: jnp.ndarray, enc_mask: jnp.ndarray) -> jnp.ndarray:\n",
    "    dec = Decoder(4, 32, 128, 4, 128, 200)\n",
    "\n",
    "    return dec(x, enc_output, mask)\n",
    "\n",
    "network = hk.transform(decoder)\n",
    "key = jax.random.PRNGKey(42) \n",
    "params = network.init(rng=key, x=jnp.ones((32, train.MAX_STROKE_LEN, 3)), enc_output=out, enc_mask=mask)\n",
    "\n",
    "out2 = network.apply(params, key, x=x, enc_output=out, enc_mask=mask)\n",
    "\n",
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Writing Transformer\n",
    "\n",
    "# Output space - number of parameters in the mixture model\n",
    "NUM_MIX_COM = 20\n",
    "# weights + means (x + y) + std. devs. (x + y) + correlations + end_of_stroke\n",
    "# Unlike the Mixture Density Network notebook we are going to add cross correlation\n",
    "# terms to our loss and sampling functions for added complexity of the density \n",
    "# estimations\n",
    "NUM_PARAMS = NUM_MIX_COM + NUM_MIX_COM*2 + NUM_MIX_COM*2 + NUM_MIX_COM + 1\n",
    "\n",
    "class Writing_Transformer(hk.Module):\n",
    "    def __init__(self, num_layers, key_size, d_model, num_heads, dff, pe_encoding, pe_target, \n",
    "        dropout_rate=0.1):\n",
    "        super().__init__(name='Writing_Transformer')\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.enc = Encoder(num_layers, key_size, d_model, num_heads, dff, pe_encoding, dropout_rate)\n",
    "        self.dec = Decoder(num_layers, key_size, d_model, num_heads, dff, pe_target, dropout_rate)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        inp: jnp.ndarray,\n",
    "        tar: jnp.ndarray,\n",
    "        training=True\n",
    "    ) -> jnp.ndarray:\n",
    "        enc_mask = jnp.not_equal(jnp.sum(inp, -1), 0)\n",
    "\n",
    "        # The Encoder\n",
    "        enc_output = self.enc(inp, enc_mask, training)\n",
    "\n",
    "        # The Decoder\n",
    "        dec_output = self.dec(tar, enc_output, enc_mask, training)\n",
    "\n",
    "        # The final layer to give us our logits\n",
    "        final_output = hk.Linear(NUM_PARAMS, name='final_layer')(dec_output)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 200, 121)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test The full network\n",
    "inp=s[1].numpy()\n",
    "tar=s[0].numpy()\n",
    "\n",
    "def writing_transformer(inp: jnp.ndarray, tar: jnp.ndarray) -> jnp.ndarray:\n",
    "    tra = Writing_Transformer(4, 32, 128, 4, 128, 200, 1000, 0.2)\n",
    "\n",
    "    return tra(inp, tar)\n",
    "\n",
    "network = hk.transform(writing_transformer)\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "rng, init_rng = jax.random.split(key)\n",
    "params = network.init(rng=key, inp=inp, tar=tar)\n",
    "\n",
    "out3 = network.apply(params, key, inp=inp, tar=tar)\n",
    "\n",
    "out3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for the loss function\n",
    "# TODO: I think there are a lot of edge cases here that will result in NaNs when training.\n",
    "\n",
    "EPS = 0.000001\n",
    "\n",
    "@hk.transform\n",
    "def my_loss_fun_MDN(batch: tuple, training: bool) -> jnp.ndarray:\n",
    "    # Predict the next strokes\n",
    "\n",
    "    # We split the input and then use 1 sample ahead as the target (y_true)\n",
    "    inp = batch[0][:, :-1]\n",
    "    y_true = batch[0][:, 1:]\n",
    "\n",
    "    logits = writing_transformer(batch[1], inp, training)\n",
    "\n",
    "    pis, mu, sig, rho, eos = jnp.array_split(logits, [NUM_MIX_COM, NUM_MIX_COM*3, NUM_MIX_COM*5, NUM_MIX_COM*6], axis=-1)\n",
    "    \n",
    "    #print(eos.shape)\n",
    "\n",
    "    # weights - must be a probability distribution so softmax over all components\n",
    "    pis = jax.nn.softmax(pis)\n",
    "    \n",
    "    # means - no transformation needed\n",
    "    mu_x1, mu_x2 = jnp.array_split(mu, 2, axis=-1)\n",
    "    \n",
    "    # standard deviations - must be strictly positive so exponent\n",
    "    sig = jnp.exp(sig)\n",
    "    \n",
    "    sig = jnp.clip(sig, EPS, np.inf)\n",
    "    \n",
    "    sig_x1, sig_x2 = jnp.array_split(sig, 2, axis=-1)\n",
    "    \n",
    "    x1, x2, eos_true = jnp.array_split(y_true, 3, axis=-1)\n",
    "    \n",
    "    eos_true = jnp.squeeze(eos_true)\n",
    "        \n",
    "    # correlations - squish to -1 to 1 with tanh activation\n",
    "    rho = jnp.tanh(rho)\n",
    "    \n",
    "    rho = jnp.clip(rho, -1.+EPS, 1.-EPS)\n",
    "    \n",
    "    # Define Z as in Graves, 2013\n",
    "    Z = jnp.square( ( x1-mu_x1 ) / sig_x1 ) + jnp.square( ( x2-mu_x2 ) / sig_x2 ) - ( 2 * rho * (x1-mu_x1) * (x2-mu_x2) ) / ( sig_x1*sig_x2 )\n",
    "    \n",
    "    one_minus_rho_square = 1. - jnp.square(rho)\n",
    "    \n",
    "    # Now form Gaussian mixtures\n",
    "    term1 = jnp.divide(1., ( 2. * np.pi * sig_x1 * sig_x2 * jnp.sqrt( one_minus_rho_square ) ))\n",
    "    term2 = jnp.exp( jnp.divide ( (-1. * Z) , (2.*( one_minus_rho_square )) ))\n",
    "    \n",
    "    mix_loss = jnp.sum(pis * term1 * term2, axis=-1)       \n",
    "    \n",
    "    mix_loss = jnp.clip(mix_loss, EPS, np.inf)\n",
    "\n",
    "    # end of stroke loss\n",
    "    eos = jnp.squeeze(jax.nn.sigmoid(eos))\n",
    "    \n",
    "    eos = jnp.clip(eos, EPS, 1.-EPS)\n",
    "\n",
    "    eos_loss = jnp.where(jnp.equal(eos_true, 1.), eos, 1.-eos)\n",
    "    \n",
    "    # Only the valid parts of the sequence should count towards the loss.  The invalid parts are tagged with -2200\n",
    "    val_seq = jnp.squeeze(jnp.not_equal(eos_true, train.padding_value))\n",
    "\n",
    "    # This is the total loss for each element (batch * num_timepoints)\n",
    "    tot_loss = -(jnp.log(mix_loss) + jnp.log(eos_loss))\n",
    "\n",
    "    # The sequence loss is the sum of only the valid timepoints\n",
    "    \n",
    "    tot_loss = jnp.where(val_seq, tot_loss, 0.)   \n",
    "    \n",
    "    seq_tot = jnp.sum(val_seq, axis=-1, dtype=float)\n",
    "\n",
    "    tot_loss = jnp.sum(tot_loss, axis=-1) / seq_tot \n",
    "\n",
    "    return jnp.mean(tot_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Transformed' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [158], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test out the loss function\u001b[39;00m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mbatched_onehot_set\u001b[38;5;241m.\u001b[39mprefetch(tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mAUTOTUNE)\u001b[38;5;241m.\u001b[39mas_numpy_iterator()\n\u001b[0;32m----> 4\u001b[0m out4 \u001b[38;5;241m=\u001b[39m \u001b[43mmy_loss_fun_MDN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m out4\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Transformed' object is not callable"
     ]
    }
   ],
   "source": [
    "# Test out the loss function\n",
    "data = train.batched_onehot_set.prefetch(tf.data.experimental.AUTOTUNE).as_numpy_iterator()\n",
    "\n",
    "out4 = my_loss_fun_MDN(params, data.next()[0])\n",
    "\n",
    "out4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "num_layers = 1\n",
    "key_size = 32\n",
    "d_model = 128\n",
    "dff = 512\n",
    "num_heads = 1\n",
    "dropout_rate = 0.01\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "GRAD_CLIP_VALUE = 1\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "total_steps = EPOCHS*350 + EPOCHS\n",
    "warmup_cosine_decay_scheduler = optax.warmup_cosine_decay_schedule(init_value=0.00001, peak_value=0.0001,\n",
    "                                                                   warmup_steps=int(total_steps*0.2),\n",
    "                                                                   decay_steps=total_steps, end_value=0.000001)\n",
    "\n",
    "optimiser = optax.chain(\n",
    "      optax.clip_by_global_norm(GRAD_CLIP_VALUE),\n",
    "      optax.adam(LEARNING_RATE, b1=0.9, b2=0.99),\n",
    "  )\n",
    "\n",
    "def writing_transformer(inp: jnp.ndarray, tar: jnp.ndarray, training: bool) -> jnp.ndarray:\n",
    "    tra = Writing_Transformer(num_layers, key_size, d_model, num_heads, dff, pe_encoding=250, pe_target=1000, dropout_rate=dropout_rate)\n",
    "\n",
    "    return tra(inp, tar, training)\n",
    "\n",
    "#network = hk.transform(writing_transformer)\n",
    "\n",
    "@jax.jit\n",
    "def update(params: hk.Params, rng, opt_state: optax.OptState, batch: tuple):\n",
    "  rng, new_rng = jax.random.split(rng)\n",
    "  loss_and_grad_fn = jax.value_and_grad(my_loss_fun_MDN.apply)\n",
    "  #grad = jax.grad(my_loss_fun_MDN)(params, batch)\n",
    "  loss, gradients = loss_and_grad_fn(params, rng, batch, True)\n",
    "  updates, opt_state = optimiser.update(gradients, opt_state)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "\n",
    "  return params, opt_state, loss, new_rng\n",
    "\n",
    "# TODO: make fetching the iterator more elegant and does the conversion from numpy to jax slow things down?\n",
    "#b = train.batched_onehot_set.prefetch(tf.data.experimental.AUTOTUNE).as_numpy_iterator()\n",
    "#key = jax.random.PRNGKey(42) \n",
    "#s = next(b)[0]\n",
    "\n",
    "data_iter = train.batched_onehot_set.prefetch(tf.data.experimental.AUTOTUNE).as_numpy_iterator()\n",
    "\n",
    "rng = jax.random.PRNGKey(SEED)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "params = my_loss_fun_MDN.init(rng=init_rng, batch=data_iter.next()[0], training=True)\n",
    "opt_state = optimiser.init(params)\n",
    "\n",
    "  # Average loss?\n",
    "  #data_iter = train.batched_onehot_set.prefetch(tf.data.experimental.AUTOTUNE).as_numpy_iterator()\n",
    "  #total_loss = 0\n",
    "  #for b, _ in data_iter:\n",
    "  #  total_loss = total_loss + my_loss_fun_MDN(params, b)\n",
    "\n",
    "  #print(\"   loss {:0.4f}\".format(total_loss*BATCH/20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.00001\n",
    "\n",
    "optimiser = optax.chain(\n",
    "      optax.clip_by_global_norm(GRAD_CLIP_VALUE),\n",
    "      optax.adam(LEARNING_RATE, b1=0.9, b2=0.99),\n",
    "  )\n",
    "\n",
    "opt_state = optimiser.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Epoch': 0}\n",
      "Epoch 1 Batch 0 Loss -0.3787\n",
      "Epoch 1 Batch 50 Loss -1.0224\n",
      "Epoch 1 Batch 100 Loss -0.9886\n",
      "Epoch 1 Batch 150 Loss -1.0032\n",
      "Epoch 1 Batch 200 Loss -1.0476\n",
      "Epoch 1 Batch 250 Loss -1.0715\n",
      "Epoch 1 Batch 300 Loss -1.0747\n",
      "Epoch 1 Batch 350 Loss -1.0895\n",
      "Epoch 1 Loss -1.0989\n",
      "{'Epoch': 1}\n",
      "Epoch 2 Batch 0 Loss -0.3291\n",
      "Epoch 2 Batch 50 Loss -1.0182\n",
      "Epoch 2 Batch 100 Loss -0.9882\n",
      "Epoch 2 Batch 150 Loss -1.0026\n",
      "Epoch 2 Batch 200 Loss -1.0475\n",
      "Epoch 2 Batch 250 Loss -1.0704\n",
      "Epoch 2 Batch 300 Loss -1.0748\n",
      "Epoch 2 Batch 350 Loss -1.0895\n",
      "Epoch 2 Loss -1.0989\n",
      "{'Epoch': 2}\n",
      "Epoch 3 Batch 0 Loss -0.3157\n",
      "Epoch 3 Batch 50 Loss -1.0133\n",
      "Epoch 3 Batch 100 Loss -0.9809\n",
      "Epoch 3 Batch 150 Loss -0.9934\n",
      "Epoch 3 Batch 200 Loss -1.0418\n",
      "Epoch 3 Batch 250 Loss -1.0671\n",
      "Epoch 3 Batch 300 Loss -1.0713\n",
      "Epoch 3 Batch 350 Loss -1.0867\n",
      "Epoch 3 Loss -1.0962\n",
      "{'Epoch': 3}\n",
      "Epoch 4 Batch 0 Loss -0.3252\n",
      "Epoch 4 Batch 50 Loss -1.0192\n",
      "Epoch 4 Batch 100 Loss -0.9885\n",
      "Epoch 4 Batch 150 Loss -1.0034\n",
      "Epoch 4 Batch 200 Loss -1.0482\n",
      "Epoch 4 Batch 250 Loss -1.0712\n",
      "Epoch 4 Batch 300 Loss -1.0757\n",
      "Epoch 4 Batch 350 Loss -1.0907\n",
      "Epoch 4 Loss -1.1003\n",
      "{'Epoch': 4}\n",
      "Epoch 5 Batch 0 Loss -0.3262\n",
      "Epoch 5 Batch 50 Loss -1.0101\n",
      "Epoch 5 Batch 100 Loss -0.9751\n",
      "Epoch 5 Batch 150 Loss -0.9942\n",
      "Epoch 5 Batch 200 Loss -1.0409\n",
      "Epoch 5 Batch 250 Loss -1.0673\n",
      "Epoch 5 Batch 300 Loss -1.0726\n",
      "Epoch 5 Batch 350 Loss -1.0880\n",
      "Epoch 5 Loss -1.0978\n",
      "{'Epoch': 5}\n",
      "Epoch 6 Batch 0 Loss -0.3210\n",
      "Epoch 6 Batch 50 Loss -1.0209\n",
      "Epoch 6 Batch 100 Loss -0.9908\n",
      "Epoch 6 Batch 150 Loss -1.0045\n",
      "Epoch 6 Batch 200 Loss -1.0493\n",
      "Epoch 6 Batch 250 Loss -1.0720\n",
      "Epoch 6 Batch 300 Loss -1.0757\n",
      "Epoch 6 Batch 350 Loss -1.0904\n",
      "Epoch 6 Loss -1.1002\n",
      "{'Epoch': 6}\n",
      "Epoch 7 Batch 0 Loss -0.3235\n",
      "Epoch 7 Batch 50 Loss -1.0089\n",
      "Epoch 7 Batch 100 Loss -0.9642\n",
      "Epoch 7 Batch 150 Loss -0.9694\n",
      "Epoch 7 Batch 200 Loss -1.0201\n",
      "Epoch 7 Batch 250 Loss -1.0473\n",
      "Epoch 7 Batch 300 Loss -1.0529\n",
      "Epoch 7 Batch 350 Loss -1.0677\n",
      "Epoch 7 Loss -1.0759\n",
      "{'Epoch': 7}\n",
      "Epoch 8 Batch 0 Loss -0.3041\n",
      "Epoch 8 Batch 50 Loss -1.0031\n",
      "Epoch 8 Batch 100 Loss -0.9711\n",
      "Epoch 8 Batch 150 Loss -0.9746\n",
      "Epoch 8 Batch 200 Loss -1.0202\n",
      "Epoch 8 Batch 250 Loss -1.0474\n",
      "Epoch 8 Batch 300 Loss -1.0550\n",
      "Epoch 8 Batch 350 Loss -1.0703\n",
      "Epoch 8 Loss -1.0808\n",
      "{'Epoch': 8}\n",
      "Epoch 9 Batch 0 Loss -0.2851\n",
      "Epoch 9 Batch 50 Loss -1.0097\n",
      "Epoch 9 Batch 100 Loss -0.9744\n",
      "Epoch 9 Batch 150 Loss -0.9855\n",
      "Epoch 9 Batch 200 Loss -1.0307\n",
      "Epoch 9 Batch 250 Loss -1.0435\n",
      "Epoch 9 Batch 300 Loss -1.0447\n",
      "Epoch 9 Batch 350 Loss -1.0634\n",
      "Epoch 9 Loss -1.0756\n",
      "{'Epoch': 9}\n",
      "Epoch 10 Batch 0 Loss -0.2998\n",
      "Epoch 10 Batch 50 Loss -1.0245\n",
      "Epoch 10 Batch 100 Loss -0.9968\n",
      "Epoch 10 Batch 150 Loss -1.0119\n",
      "Epoch 10 Batch 200 Loss -1.0545\n",
      "Epoch 10 Batch 250 Loss -1.0723\n",
      "Epoch 10 Batch 300 Loss -1.0694\n",
      "Epoch 10 Batch 350 Loss -1.0807\n",
      "Epoch 10 Loss -1.0879\n",
      "{'Epoch': 10}\n",
      "Epoch 11 Batch 0 Loss -0.3110\n",
      "Epoch 11 Batch 50 Loss -0.9549\n",
      "Epoch 11 Batch 100 Loss -0.9420\n",
      "Epoch 11 Batch 150 Loss -0.9654\n",
      "Epoch 11 Batch 200 Loss -1.0189\n",
      "Epoch 11 Batch 250 Loss -1.0479\n",
      "Epoch 11 Batch 300 Loss -1.0551\n",
      "Epoch 11 Batch 350 Loss -1.0698\n",
      "Epoch 11 Loss -1.0788\n",
      "{'Epoch': 11}\n",
      "Epoch 12 Batch 0 Loss -0.3185\n",
      "Epoch 12 Batch 50 Loss -0.9929\n",
      "Epoch 12 Batch 100 Loss -0.9592\n",
      "Epoch 12 Batch 150 Loss -0.9796\n",
      "Epoch 12 Batch 200 Loss -1.0339\n",
      "Epoch 12 Batch 250 Loss -1.0641\n",
      "Epoch 12 Batch 300 Loss -1.0733\n",
      "Epoch 12 Batch 350 Loss -1.0896\n",
      "Epoch 12 Loss -1.1000\n",
      "{'Epoch': 12}\n",
      "Epoch 13 Batch 0 Loss -0.3476\n",
      "Epoch 13 Batch 50 Loss -1.0297\n",
      "Epoch 13 Batch 100 Loss -0.9982\n",
      "Epoch 13 Batch 150 Loss -1.0110\n",
      "Epoch 13 Batch 200 Loss -1.0557\n",
      "Epoch 13 Batch 250 Loss -1.0777\n",
      "Epoch 13 Batch 300 Loss -1.0782\n",
      "Epoch 13 Batch 350 Loss -1.0875\n",
      "Epoch 13 Loss -1.0946\n",
      "{'Epoch': 13}\n",
      "Epoch 14 Batch 0 Loss -0.2929\n",
      "Epoch 14 Batch 50 Loss -0.9994\n",
      "Epoch 14 Batch 100 Loss -0.9714\n",
      "Epoch 14 Batch 150 Loss -0.9912\n",
      "Epoch 14 Batch 200 Loss -1.0479\n",
      "Epoch 14 Batch 250 Loss -1.0804\n",
      "Epoch 14 Batch 300 Loss -1.0914\n",
      "Epoch 14 Batch 350 Loss -1.1078\n",
      "Epoch 14 Loss -1.1186\n",
      "{'Epoch': 14}\n",
      "Epoch 15 Batch 0 Loss -0.3267\n",
      "Epoch 15 Batch 50 Loss -1.0473\n",
      "Epoch 15 Batch 100 Loss -1.0187\n",
      "Epoch 15 Batch 150 Loss -1.0327\n",
      "Epoch 15 Batch 200 Loss -1.0766\n",
      "Epoch 15 Batch 250 Loss -1.0725\n",
      "Epoch 15 Batch 300 Loss -1.0654\n",
      "Epoch 15 Batch 350 Loss -1.0788\n",
      "Epoch 15 Loss -1.0895\n",
      "{'Epoch': 15}\n",
      "Epoch 16 Batch 0 Loss -0.2866\n",
      "Epoch 16 Batch 50 Loss -1.0288\n",
      "Epoch 16 Batch 100 Loss -0.9988\n",
      "Epoch 16 Batch 150 Loss -1.0116\n",
      "Epoch 16 Batch 200 Loss -1.0568\n",
      "Epoch 16 Batch 250 Loss -1.0773\n",
      "Epoch 16 Batch 300 Loss -1.0821\n",
      "Epoch 16 Batch 350 Loss -1.0961\n",
      "Epoch 16 Loss -1.1050\n",
      "{'Epoch': 16}\n",
      "Epoch 17 Batch 0 Loss -0.3122\n",
      "Epoch 17 Batch 50 Loss -1.0242\n",
      "Epoch 17 Batch 100 Loss -0.9935\n",
      "Epoch 17 Batch 150 Loss -1.0052\n",
      "Epoch 17 Batch 200 Loss -1.0503\n",
      "Epoch 17 Batch 250 Loss -1.0707\n",
      "Epoch 17 Batch 300 Loss -1.0764\n",
      "Epoch 17 Batch 350 Loss -1.0909\n",
      "Epoch 17 Loss -1.0990\n",
      "{'Epoch': 17}\n",
      "Epoch 18 Batch 0 Loss -0.2914\n",
      "Epoch 18 Batch 50 Loss -1.0209\n",
      "Epoch 18 Batch 100 Loss -0.9923\n",
      "Epoch 18 Batch 150 Loss -1.0064\n",
      "Epoch 18 Batch 200 Loss -1.0502\n",
      "Epoch 18 Batch 250 Loss -1.0723\n",
      "Epoch 18 Batch 300 Loss -1.0771\n",
      "Epoch 18 Batch 350 Loss -1.0916\n",
      "Epoch 18 Loss -1.1001\n",
      "{'Epoch': 18}\n",
      "Epoch 19 Batch 0 Loss -0.2800\n",
      "Epoch 19 Batch 50 Loss -1.0176\n",
      "Epoch 19 Batch 100 Loss -0.9899\n",
      "Epoch 19 Batch 150 Loss -1.0047\n",
      "Epoch 19 Batch 200 Loss -1.0508\n",
      "Epoch 19 Batch 250 Loss -1.0729\n",
      "Epoch 19 Batch 300 Loss -1.0774\n",
      "Epoch 19 Batch 350 Loss -1.0884\n",
      "Epoch 19 Loss -1.0920\n",
      "{'Epoch': 19}\n",
      "Epoch 20 Batch 0 Loss -0.2465\n",
      "Epoch 20 Batch 50 Loss -0.9822\n",
      "Epoch 20 Batch 100 Loss -0.9604\n",
      "Epoch 20 Batch 150 Loss -0.9847\n",
      "Epoch 20 Batch 200 Loss -1.0397\n",
      "Epoch 20 Batch 250 Loss -1.0706\n",
      "Epoch 20 Batch 300 Loss -1.0796\n",
      "Epoch 20 Batch 350 Loss -1.0960\n",
      "Epoch 20 Loss -1.1064\n",
      "{'Epoch': 20}\n",
      "Epoch 21 Batch 0 Loss -0.3187\n",
      "Epoch 21 Batch 50 Loss -1.0310\n",
      "Epoch 21 Batch 100 Loss -0.9955\n",
      "Epoch 21 Batch 150 Loss -1.0096\n",
      "Epoch 21 Batch 200 Loss -1.0547\n",
      "Epoch 21 Batch 250 Loss -1.0774\n",
      "Epoch 21 Batch 300 Loss -1.0815\n",
      "Epoch 21 Batch 350 Loss -1.0964\n",
      "Epoch 21 Loss -1.1059\n",
      "{'Epoch': 21}\n",
      "Epoch 22 Batch 0 Loss -0.3297\n",
      "Epoch 22 Batch 50 Loss -1.0161\n",
      "Epoch 22 Batch 100 Loss -0.9862\n",
      "Epoch 22 Batch 150 Loss -1.0026\n",
      "Epoch 22 Batch 200 Loss -1.0484\n",
      "Epoch 22 Batch 250 Loss -1.0733\n",
      "Epoch 22 Batch 300 Loss -1.0783\n",
      "Epoch 22 Batch 350 Loss -1.0939\n",
      "Epoch 22 Loss -1.1036\n",
      "{'Epoch': 22}\n",
      "Epoch 23 Batch 0 Loss -0.3338\n",
      "Epoch 23 Batch 50 Loss -1.0177\n",
      "Epoch 23 Batch 100 Loss -0.9880\n",
      "Epoch 23 Batch 150 Loss -1.0039\n",
      "Epoch 23 Batch 200 Loss -1.0479\n",
      "Epoch 23 Batch 250 Loss -1.0723\n",
      "Epoch 23 Batch 300 Loss -1.0776\n",
      "Epoch 23 Batch 350 Loss -1.0929\n",
      "Epoch 23 Loss -1.1017\n",
      "{'Epoch': 23}\n",
      "Epoch 24 Batch 0 Loss -0.3103\n",
      "Epoch 24 Batch 50 Loss -1.0214\n",
      "Epoch 24 Batch 100 Loss -0.9903\n",
      "Epoch 24 Batch 150 Loss -1.0047\n",
      "Epoch 24 Batch 200 Loss -1.0491\n",
      "Epoch 24 Batch 250 Loss -1.0742\n",
      "Epoch 24 Batch 300 Loss -1.0782\n",
      "Epoch 24 Batch 350 Loss -1.0934\n",
      "Epoch 24 Loss -1.1028\n",
      "{'Epoch': 24}\n",
      "Epoch 25 Batch 0 Loss -0.3278\n",
      "Epoch 25 Batch 50 Loss -1.0225\n",
      "Epoch 25 Batch 100 Loss -0.9935\n",
      "Epoch 25 Batch 150 Loss -1.0082\n",
      "Epoch 25 Batch 200 Loss -1.0534\n",
      "Epoch 25 Batch 250 Loss -1.0769\n",
      "Epoch 25 Batch 300 Loss -1.0809\n",
      "Epoch 25 Batch 350 Loss -1.0959\n",
      "Epoch 25 Loss -1.1053\n",
      "{'Epoch': 25}\n",
      "Epoch 26 Batch 0 Loss -0.3264\n",
      "Epoch 26 Batch 50 Loss -1.0222\n",
      "Epoch 26 Batch 100 Loss -0.9913\n",
      "Epoch 26 Batch 150 Loss -1.0063\n",
      "Epoch 26 Batch 200 Loss -1.0513\n",
      "Epoch 26 Batch 250 Loss -1.0755\n",
      "Epoch 26 Batch 300 Loss -1.0798\n",
      "Epoch 26 Batch 350 Loss -1.0948\n",
      "Epoch 26 Loss -1.1047\n",
      "{'Epoch': 26}\n",
      "Epoch 27 Batch 0 Loss -0.3265\n",
      "Epoch 27 Batch 50 Loss -1.0213\n",
      "Epoch 27 Batch 100 Loss -0.9928\n",
      "Epoch 27 Batch 150 Loss -1.0073\n",
      "Epoch 27 Batch 200 Loss -1.0521\n",
      "Epoch 27 Batch 250 Loss -1.0761\n",
      "Epoch 27 Batch 300 Loss -1.0806\n",
      "Epoch 27 Batch 350 Loss -1.0954\n",
      "Epoch 27 Loss -1.1045\n",
      "{'Epoch': 27}\n",
      "Epoch 28 Batch 0 Loss -0.3269\n",
      "Epoch 28 Batch 50 Loss -1.0220\n",
      "Epoch 28 Batch 100 Loss -0.9919\n",
      "Epoch 28 Batch 150 Loss -1.0071\n",
      "Epoch 28 Batch 200 Loss -1.0517\n",
      "Epoch 28 Batch 250 Loss -1.0767\n",
      "Epoch 28 Batch 300 Loss -1.0803\n",
      "Epoch 28 Batch 350 Loss -1.0955\n",
      "Epoch 28 Loss -1.1052\n",
      "{'Epoch': 28}\n",
      "Epoch 29 Batch 0 Loss -0.3381\n",
      "Epoch 29 Batch 50 Loss -1.0228\n",
      "Epoch 29 Batch 100 Loss -0.9939\n",
      "Epoch 29 Batch 150 Loss -1.0086\n",
      "Epoch 29 Batch 200 Loss -1.0530\n",
      "Epoch 29 Batch 250 Loss -1.0768\n",
      "Epoch 29 Batch 300 Loss -1.0811\n",
      "Epoch 29 Batch 350 Loss -1.0959\n",
      "Epoch 29 Loss -1.1055\n",
      "{'Epoch': 29}\n",
      "Epoch 30 Batch 0 Loss -0.3359\n",
      "Epoch 30 Batch 50 Loss -1.0187\n",
      "Epoch 30 Batch 100 Loss -0.9894\n",
      "Epoch 30 Batch 150 Loss -1.0054\n",
      "Epoch 30 Batch 200 Loss -1.0513\n",
      "Epoch 30 Batch 250 Loss -1.0764\n",
      "Epoch 30 Batch 300 Loss -1.0807\n",
      "Epoch 30 Batch 350 Loss -1.0957\n",
      "Epoch 30 Loss -1.1054\n",
      "{'Epoch': 30}\n",
      "Epoch 31 Batch 0 Loss -0.3342\n",
      "Epoch 31 Batch 50 Loss -1.0216\n",
      "Epoch 31 Batch 100 Loss -0.9924\n",
      "Epoch 31 Batch 150 Loss -1.0075\n",
      "Epoch 31 Batch 200 Loss -1.0527\n",
      "Epoch 31 Batch 250 Loss -1.0740\n",
      "Epoch 31 Batch 300 Loss -1.0788\n",
      "Epoch 31 Batch 350 Loss -1.0943\n",
      "Epoch 31 Loss -1.1043\n",
      "{'Epoch': 31}\n",
      "Epoch 32 Batch 0 Loss -0.3268\n",
      "Epoch 32 Batch 50 Loss -1.0197\n",
      "Epoch 32 Batch 100 Loss -0.9906\n",
      "Epoch 32 Batch 150 Loss -1.0068\n",
      "Epoch 32 Batch 200 Loss -1.0526\n",
      "Epoch 32 Batch 250 Loss -1.0774\n",
      "Epoch 32 Batch 300 Loss -1.0819\n",
      "Epoch 32 Batch 350 Loss -1.0971\n",
      "Epoch 32 Loss -1.1066\n",
      "{'Epoch': 32}\n",
      "Epoch 33 Batch 0 Loss -0.3278\n",
      "Epoch 33 Batch 50 Loss -1.0253\n",
      "Epoch 33 Batch 100 Loss -0.9950\n",
      "Epoch 33 Batch 150 Loss -1.0094\n",
      "Epoch 33 Batch 200 Loss -1.0540\n",
      "Epoch 33 Batch 250 Loss -1.0765\n",
      "Epoch 33 Batch 300 Loss -1.0813\n",
      "Epoch 33 Batch 350 Loss -1.0964\n",
      "Epoch 33 Loss -1.1063\n",
      "{'Epoch': 33}\n",
      "Epoch 34 Batch 0 Loss -0.3361\n",
      "Epoch 34 Batch 50 Loss -1.0204\n",
      "Epoch 34 Batch 100 Loss -0.9930\n",
      "Epoch 34 Batch 150 Loss -1.0084\n",
      "Epoch 34 Batch 200 Loss -1.0535\n",
      "Epoch 34 Batch 250 Loss -1.0777\n",
      "Epoch 34 Batch 300 Loss -1.0819\n",
      "Epoch 34 Batch 350 Loss -1.0970\n",
      "Epoch 34 Loss -1.1066\n",
      "{'Epoch': 34}\n",
      "Epoch 35 Batch 0 Loss -0.3409\n",
      "Epoch 35 Batch 50 Loss -1.0246\n",
      "Epoch 35 Batch 100 Loss -0.9960\n",
      "Epoch 35 Batch 150 Loss -1.0101\n",
      "Epoch 35 Batch 200 Loss -1.0484\n",
      "Epoch 35 Batch 250 Loss -1.0580\n",
      "Epoch 35 Batch 300 Loss -1.0672\n",
      "Epoch 35 Batch 350 Loss -1.0846\n",
      "Epoch 35 Loss -1.0961\n",
      "{'Epoch': 35}\n",
      "Epoch 36 Batch 0 Loss -0.3358\n",
      "Epoch 36 Batch 50 Loss -1.0338\n",
      "Epoch 36 Batch 100 Loss -1.0038\n",
      "Epoch 36 Batch 150 Loss -1.0176\n",
      "Epoch 36 Batch 200 Loss -1.0635\n",
      "Epoch 36 Batch 250 Loss -1.0860\n",
      "Epoch 36 Batch 300 Loss -1.0879\n",
      "Epoch 36 Batch 350 Loss -1.1021\n",
      "Epoch 36 Loss -1.1116\n",
      "{'Epoch': 36}\n",
      "Epoch 37 Batch 0 Loss -0.3272\n",
      "Epoch 37 Batch 50 Loss -1.0245\n",
      "Epoch 37 Batch 100 Loss -0.9962\n",
      "Epoch 37 Batch 150 Loss -1.0114\n",
      "Epoch 37 Batch 200 Loss -1.0520\n",
      "Epoch 37 Batch 250 Loss -1.0695\n",
      "Epoch 37 Batch 300 Loss -1.0734\n",
      "Epoch 37 Batch 350 Loss -1.0873\n",
      "Epoch 37 Loss -1.0926\n",
      "{'Epoch': 37}\n",
      "Epoch 38 Batch 0 Loss -0.2524\n",
      "Epoch 38 Batch 50 Loss -1.0095\n",
      "Epoch 38 Batch 100 Loss -0.9867\n",
      "Epoch 38 Batch 150 Loss -1.0065\n",
      "Epoch 38 Batch 200 Loss -1.0571\n",
      "Epoch 38 Batch 250 Loss -1.0832\n",
      "Epoch 38 Batch 300 Loss -1.0891\n",
      "Epoch 38 Batch 350 Loss -1.1036\n",
      "Epoch 38 Loss -1.1125\n",
      "{'Epoch': 38}\n",
      "Epoch 39 Batch 0 Loss -0.3096\n",
      "Epoch 39 Batch 50 Loss -1.0243\n",
      "Epoch 39 Batch 100 Loss -0.9952\n",
      "Epoch 39 Batch 150 Loss -1.0097\n",
      "Epoch 39 Batch 200 Loss -1.0550\n",
      "Epoch 39 Batch 250 Loss -1.0564\n",
      "Epoch 39 Batch 300 Loss -1.0623\n",
      "Epoch 39 Batch 350 Loss -1.0746\n",
      "Epoch 39 Loss -1.0849\n",
      "{'Epoch': 39}\n",
      "Epoch 40 Batch 0 Loss -0.2930\n",
      "Epoch 40 Batch 50 Loss -1.0119\n",
      "Epoch 40 Batch 100 Loss -0.9837\n",
      "Epoch 40 Batch 150 Loss -0.9975\n",
      "Epoch 40 Batch 200 Loss -1.0420\n",
      "Epoch 40 Batch 250 Loss -1.0630\n",
      "Epoch 40 Batch 300 Loss -1.0688\n",
      "Epoch 40 Batch 350 Loss -1.0814\n",
      "Epoch 40 Loss -1.0909\n",
      "{'Epoch': 40}\n",
      "Epoch 41 Batch 0 Loss -0.2953\n",
      "Epoch 41 Batch 50 Loss -1.0082\n",
      "Epoch 41 Batch 100 Loss -0.9772\n",
      "Epoch 41 Batch 150 Loss -0.9815\n",
      "Epoch 41 Batch 200 Loss -1.0265\n",
      "Epoch 41 Batch 250 Loss -1.0454\n",
      "Epoch 41 Batch 300 Loss -1.0516\n",
      "Epoch 41 Batch 350 Loss -1.0704\n",
      "Epoch 41 Loss -1.0831\n",
      "{'Epoch': 41}\n",
      "Epoch 42 Batch 0 Loss -0.3207\n",
      "Epoch 42 Batch 50 Loss -1.0355\n",
      "Epoch 42 Batch 100 Loss -1.0053\n",
      "Epoch 42 Batch 150 Loss -1.0209\n",
      "Epoch 42 Batch 200 Loss -1.0676\n",
      "Epoch 42 Batch 250 Loss -1.0923\n",
      "Epoch 42 Batch 300 Loss -1.0966\n",
      "Epoch 42 Batch 350 Loss -1.1101\n",
      "Epoch 42 Loss -1.1188\n",
      "{'Epoch': 42}\n",
      "Epoch 43 Batch 0 Loss -0.3281\n",
      "Epoch 43 Batch 50 Loss -1.0296\n",
      "Epoch 43 Batch 100 Loss -0.9991\n",
      "Epoch 43 Batch 150 Loss -1.0136\n",
      "Epoch 43 Batch 200 Loss -1.0590\n",
      "Epoch 43 Batch 250 Loss -1.0823\n",
      "Epoch 43 Batch 300 Loss -1.0866\n",
      "Epoch 43 Batch 350 Loss -1.1011\n",
      "Epoch 43 Loss -1.1106\n",
      "{'Epoch': 43}\n",
      "Epoch 44 Batch 0 Loss -0.3324\n",
      "Epoch 44 Batch 50 Loss -1.0170\n",
      "Epoch 44 Batch 100 Loss -0.9857\n",
      "Epoch 44 Batch 150 Loss -0.9974\n",
      "Epoch 44 Batch 200 Loss -1.0248\n",
      "Epoch 44 Batch 250 Loss -1.0477\n",
      "Epoch 44 Batch 300 Loss -1.0612\n",
      "Epoch 44 Batch 350 Loss -1.0805\n",
      "Epoch 44 Loss -1.0923\n",
      "{'Epoch': 44}\n",
      "Epoch 45 Batch 0 Loss -0.3145\n",
      "Epoch 45 Batch 50 Loss -1.0376\n",
      "Epoch 45 Batch 100 Loss -1.0064\n",
      "Epoch 45 Batch 150 Loss -1.0203\n",
      "Epoch 45 Batch 200 Loss -1.0658\n",
      "Epoch 45 Batch 250 Loss -1.0893\n",
      "Epoch 45 Batch 300 Loss -1.0916\n",
      "Epoch 45 Batch 350 Loss -1.1055\n",
      "Epoch 45 Loss -1.1141\n",
      "{'Epoch': 45}\n",
      "Epoch 46 Batch 0 Loss -0.3248\n",
      "Epoch 46 Batch 50 Loss -1.0281\n",
      "Epoch 46 Batch 100 Loss -0.9982\n",
      "Epoch 46 Batch 150 Loss -1.0125\n",
      "Epoch 46 Batch 200 Loss -1.0578\n",
      "Epoch 46 Batch 250 Loss -1.0814\n",
      "Epoch 46 Batch 300 Loss -1.0858\n",
      "Epoch 46 Batch 350 Loss -1.1004\n",
      "Epoch 46 Loss -1.1098\n",
      "{'Epoch': 46}\n",
      "Epoch 47 Batch 0 Loss -0.3370\n",
      "Epoch 47 Batch 50 Loss -1.0249\n",
      "Epoch 47 Batch 100 Loss -0.9869\n",
      "Epoch 47 Batch 150 Loss -0.9976\n",
      "Epoch 47 Batch 200 Loss -1.0436\n",
      "Epoch 47 Batch 250 Loss -1.0732\n",
      "Epoch 47 Batch 300 Loss -1.0817\n",
      "Epoch 47 Batch 350 Loss -1.0980\n",
      "Epoch 47 Loss -1.1086\n",
      "{'Epoch': 47}\n",
      "Epoch 48 Batch 0 Loss -0.3254\n",
      "Epoch 48 Batch 50 Loss -1.0334\n",
      "Epoch 48 Batch 100 Loss -1.0052\n",
      "Epoch 48 Batch 150 Loss -1.0193\n",
      "Epoch 48 Batch 200 Loss -1.0644\n",
      "Epoch 48 Batch 250 Loss -1.0862\n",
      "Epoch 48 Batch 300 Loss -1.0885\n",
      "Epoch 48 Batch 350 Loss -1.1025\n",
      "Epoch 48 Loss -1.1121\n",
      "{'Epoch': 48}\n",
      "Epoch 49 Batch 0 Loss -0.3338\n",
      "Epoch 49 Batch 50 Loss -1.0277\n",
      "Epoch 49 Batch 100 Loss -0.9999\n",
      "Epoch 49 Batch 150 Loss -1.0150\n",
      "Epoch 49 Batch 200 Loss -1.0591\n",
      "Epoch 49 Batch 250 Loss -1.0815\n",
      "Epoch 49 Batch 300 Loss -1.0843\n",
      "Epoch 49 Batch 350 Loss -1.0993\n",
      "Epoch 49 Loss -1.1094\n",
      "{'Epoch': 49}\n",
      "Epoch 50 Batch 0 Loss -0.3339\n",
      "Epoch 50 Batch 50 Loss -1.0290\n",
      "Epoch 50 Batch 100 Loss -1.0006\n",
      "Epoch 50 Batch 150 Loss -1.0152\n",
      "Epoch 50 Batch 200 Loss -1.0604\n",
      "Epoch 50 Batch 250 Loss -1.0829\n",
      "Epoch 50 Batch 300 Loss -1.0831\n",
      "Epoch 50 Batch 350 Loss -1.0924\n",
      "Epoch 50 Loss -1.0991\n",
      "{'Epoch': 50}\n",
      "Epoch 51 Batch 0 Loss -0.2245\n",
      "Epoch 51 Batch 50 Loss -0.9803\n",
      "Epoch 51 Batch 100 Loss -0.9584\n",
      "Epoch 51 Batch 150 Loss -0.9788\n",
      "Epoch 51 Batch 200 Loss -1.0307\n",
      "Epoch 51 Batch 250 Loss -1.0589\n",
      "Epoch 51 Batch 300 Loss -1.0668\n",
      "Epoch 51 Batch 350 Loss -1.0821\n",
      "Epoch 51 Loss -1.0923\n",
      "{'Epoch': 51}\n",
      "Epoch 52 Batch 0 Loss -0.2982\n",
      "Epoch 52 Batch 50 Loss -1.0142\n",
      "Epoch 52 Batch 100 Loss -0.9826\n",
      "Epoch 52 Batch 150 Loss -0.9752\n",
      "Epoch 52 Batch 200 Loss -1.0204\n",
      "Epoch 52 Batch 250 Loss -1.0464\n",
      "Epoch 52 Batch 300 Loss -1.0573\n",
      "Epoch 52 Batch 350 Loss -1.0737\n",
      "Epoch 52 Loss -1.0836\n",
      "{'Epoch': 52}\n",
      "Epoch 53 Batch 0 Loss -0.3135\n",
      "Epoch 53 Batch 50 Loss -0.9993\n",
      "Epoch 53 Batch 100 Loss -0.9715\n",
      "Epoch 53 Batch 150 Loss -0.9898\n",
      "Epoch 53 Batch 200 Loss -1.0385\n",
      "Epoch 53 Batch 250 Loss -1.0647\n",
      "Epoch 53 Batch 300 Loss -1.0678\n",
      "Epoch 53 Batch 350 Loss -1.0819\n",
      "Epoch 53 Loss -1.0907\n",
      "{'Epoch': 53}\n",
      "Epoch 54 Batch 0 Loss -0.3005\n",
      "Epoch 54 Batch 50 Loss -1.0120\n",
      "Epoch 54 Batch 100 Loss -0.9862\n",
      "Epoch 54 Batch 150 Loss -1.0058\n",
      "Epoch 54 Batch 200 Loss -1.0582\n",
      "Epoch 54 Batch 250 Loss -1.0873\n",
      "Epoch 54 Batch 300 Loss -1.0965\n",
      "Epoch 54 Batch 350 Loss -1.1131\n",
      "Epoch 54 Loss -1.1240\n",
      "{'Epoch': 54}\n",
      "Epoch 55 Batch 0 Loss -0.3297\n",
      "Epoch 55 Batch 50 Loss -1.0465\n",
      "Epoch 55 Batch 100 Loss -1.0229\n",
      "Epoch 55 Batch 150 Loss -1.0387\n",
      "Epoch 55 Batch 200 Loss -1.0845\n",
      "Epoch 55 Batch 250 Loss -1.0913\n",
      "Epoch 55 Batch 300 Loss -1.0868\n",
      "Epoch 55 Batch 350 Loss -1.0942\n",
      "Epoch 55 Loss -1.1019\n",
      "{'Epoch': 55}\n",
      "Epoch 56 Batch 0 Loss -0.3215\n",
      "Epoch 56 Batch 50 Loss -1.0194\n",
      "Epoch 56 Batch 100 Loss -0.9862\n",
      "Epoch 56 Batch 150 Loss -1.0004\n",
      "Epoch 56 Batch 200 Loss -1.0452\n",
      "Epoch 56 Batch 250 Loss -1.0692\n",
      "Epoch 56 Batch 300 Loss -1.0702\n",
      "Epoch 56 Batch 350 Loss -1.0846\n",
      "Epoch 56 Loss -1.0911\n",
      "{'Epoch': 56}\n",
      "Epoch 57 Batch 0 Loss -0.2992\n",
      "Epoch 57 Batch 50 Loss -1.0278\n",
      "Epoch 57 Batch 100 Loss -1.0106\n",
      "Epoch 57 Batch 150 Loss -1.0308\n",
      "Epoch 57 Batch 200 Loss -1.0806\n",
      "Epoch 57 Batch 250 Loss -1.1085\n",
      "Epoch 57 Batch 300 Loss -1.1150\n",
      "Epoch 57 Batch 350 Loss -1.1292\n",
      "Epoch 57 Loss -1.1388\n",
      "{'Epoch': 57}\n",
      "Epoch 58 Batch 0 Loss -0.3350\n",
      "Epoch 58 Batch 50 Loss -1.0287\n",
      "Epoch 58 Batch 100 Loss -0.9716\n",
      "Epoch 58 Batch 150 Loss -0.9814\n",
      "Epoch 58 Batch 200 Loss -1.0279\n",
      "Epoch 58 Batch 250 Loss -1.0528\n",
      "Epoch 58 Batch 300 Loss -1.0659\n",
      "Epoch 58 Batch 350 Loss -1.0850\n",
      "Epoch 58 Loss -1.0969\n",
      "{'Epoch': 58}\n",
      "Epoch 59 Batch 0 Loss -0.3342\n",
      "Epoch 59 Batch 50 Loss -1.0418\n",
      "Epoch 59 Batch 100 Loss -1.0116\n",
      "Epoch 59 Batch 150 Loss -1.0239\n",
      "Epoch 59 Batch 200 Loss -1.0694\n",
      "Epoch 59 Batch 250 Loss -1.0924\n",
      "Epoch 59 Batch 300 Loss -1.0903\n",
      "Epoch 59 Batch 350 Loss -1.0962\n",
      "Epoch 59 Loss -1.0980\n",
      "{'Epoch': 59}\n",
      "Epoch 60 Batch 0 Loss -0.2710\n",
      "Epoch 60 Batch 50 Loss -1.0068\n",
      "Epoch 60 Batch 100 Loss -0.9790\n",
      "Epoch 60 Batch 150 Loss -1.0005\n",
      "Epoch 60 Batch 200 Loss -1.0534\n",
      "Epoch 60 Batch 250 Loss -1.0820\n",
      "Epoch 60 Batch 300 Loss -1.0900\n",
      "Epoch 60 Batch 350 Loss -1.1053\n",
      "Epoch 60 Loss -1.1088\n",
      "{'Epoch': 60}\n",
      "Epoch 61 Batch 0 Loss -0.2680\n",
      "Epoch 61 Batch 50 Loss -0.9753\n",
      "Epoch 61 Batch 100 Loss -0.9261\n",
      "Epoch 61 Batch 150 Loss -0.9530\n",
      "Epoch 61 Batch 200 Loss -1.0106\n",
      "Epoch 61 Batch 250 Loss -1.0405\n",
      "Epoch 61 Batch 300 Loss -1.0406\n",
      "Epoch 61 Batch 350 Loss -1.0622\n",
      "Epoch 61 Loss -1.0767\n",
      "{'Epoch': 61}\n",
      "Epoch 62 Batch 0 Loss -0.3302\n",
      "Epoch 62 Batch 50 Loss -1.0531\n",
      "Epoch 62 Batch 100 Loss -1.0257\n",
      "Epoch 62 Batch 150 Loss -1.0417\n",
      "Epoch 62 Batch 200 Loss -1.0895\n",
      "Epoch 62 Batch 250 Loss -1.1040\n",
      "Epoch 62 Batch 300 Loss -1.1041\n",
      "Epoch 62 Batch 350 Loss -1.1168\n",
      "Epoch 62 Loss -1.1261\n",
      "{'Epoch': 62}\n",
      "Epoch 63 Batch 0 Loss -0.3254\n",
      "Epoch 63 Batch 50 Loss -1.0408\n",
      "Epoch 63 Batch 100 Loss -1.0078\n",
      "Epoch 63 Batch 150 Loss -1.0208\n",
      "Epoch 63 Batch 200 Loss -1.0652\n",
      "Epoch 63 Batch 250 Loss -1.0887\n",
      "Epoch 63 Batch 300 Loss -1.0913\n",
      "Epoch 63 Batch 350 Loss -1.1056\n",
      "Epoch 63 Loss -1.1148\n",
      "{'Epoch': 63}\n",
      "Epoch 64 Batch 0 Loss -0.3262\n",
      "Epoch 64 Batch 50 Loss -1.0327\n",
      "Epoch 64 Batch 100 Loss -1.0031\n",
      "Epoch 64 Batch 150 Loss -1.0168\n",
      "Epoch 64 Batch 200 Loss -1.0620\n",
      "Epoch 64 Batch 250 Loss -1.0857\n",
      "Epoch 64 Batch 300 Loss -1.0901\n",
      "Epoch 64 Batch 350 Loss -1.1049\n",
      "Epoch 64 Loss -1.1143\n",
      "{'Epoch': 64}\n",
      "Epoch 65 Batch 0 Loss -0.3294\n",
      "Epoch 65 Batch 50 Loss -1.0311\n",
      "Epoch 65 Batch 100 Loss -1.0024\n",
      "Epoch 65 Batch 150 Loss -1.0167\n",
      "Epoch 65 Batch 200 Loss -1.0621\n",
      "Epoch 65 Batch 250 Loss -1.0865\n",
      "Epoch 65 Batch 300 Loss -1.0903\n",
      "Epoch 65 Batch 350 Loss -1.1051\n",
      "Epoch 65 Loss -1.1147\n",
      "{'Epoch': 65}\n",
      "Epoch 66 Batch 0 Loss -0.3418\n",
      "Epoch 66 Batch 50 Loss -1.0282\n",
      "Epoch 66 Batch 100 Loss -0.9982\n",
      "Epoch 66 Batch 150 Loss -1.0141\n",
      "Epoch 66 Batch 200 Loss -1.0601\n",
      "Epoch 66 Batch 250 Loss -1.0844\n",
      "Epoch 66 Batch 300 Loss -1.0892\n",
      "Epoch 66 Batch 350 Loss -1.1043\n",
      "Epoch 66 Loss -1.1136\n",
      "{'Epoch': 66}\n",
      "Epoch 67 Batch 0 Loss -0.3365\n",
      "Epoch 67 Batch 50 Loss -1.0338\n",
      "Epoch 67 Batch 100 Loss -1.0034\n",
      "Epoch 67 Batch 150 Loss -1.0178\n",
      "Epoch 67 Batch 200 Loss -1.0621\n",
      "Epoch 67 Batch 250 Loss -1.0859\n",
      "Epoch 67 Batch 300 Loss -1.0902\n",
      "Epoch 67 Batch 350 Loss -1.1049\n",
      "Epoch 67 Loss -1.1139\n",
      "{'Epoch': 67}\n",
      "Epoch 68 Batch 0 Loss -0.3313\n",
      "Epoch 68 Batch 50 Loss -1.0340\n",
      "Epoch 68 Batch 100 Loss -1.0026\n",
      "Epoch 68 Batch 150 Loss -1.0169\n",
      "Epoch 68 Batch 200 Loss -1.0624\n",
      "Epoch 68 Batch 250 Loss -1.0855\n",
      "Epoch 68 Batch 300 Loss -1.0894\n",
      "Epoch 68 Batch 350 Loss -1.1045\n",
      "Epoch 68 Loss -1.1141\n",
      "{'Epoch': 68}\n",
      "Epoch 69 Batch 0 Loss -0.3519\n",
      "Epoch 69 Batch 50 Loss -1.0314\n",
      "Epoch 69 Batch 100 Loss -0.9999\n",
      "Epoch 69 Batch 150 Loss -1.0155\n",
      "Epoch 69 Batch 200 Loss -1.0609\n",
      "Epoch 69 Batch 250 Loss -1.0855\n",
      "Epoch 69 Batch 300 Loss -1.0901\n",
      "Epoch 69 Batch 350 Loss -1.1052\n",
      "Epoch 69 Loss -1.1146\n",
      "{'Epoch': 69}\n",
      "Epoch 70 Batch 0 Loss -0.3321\n",
      "Epoch 70 Batch 50 Loss -1.0322\n",
      "Epoch 70 Batch 100 Loss -1.0039\n",
      "Epoch 70 Batch 150 Loss -1.0184\n",
      "Epoch 70 Batch 200 Loss -1.0628\n",
      "Epoch 70 Batch 250 Loss -1.0865\n",
      "Epoch 70 Batch 300 Loss -1.0906\n",
      "Epoch 70 Batch 350 Loss -1.1055\n",
      "Epoch 70 Loss -1.1150\n",
      "{'Epoch': 70}\n",
      "Epoch 71 Batch 0 Loss -0.3393\n",
      "Epoch 71 Batch 50 Loss -1.0304\n",
      "Epoch 71 Batch 100 Loss -0.9985\n",
      "Epoch 71 Batch 150 Loss -1.0147\n",
      "Epoch 71 Batch 200 Loss -1.0609\n",
      "Epoch 71 Batch 250 Loss -1.0857\n",
      "Epoch 71 Batch 300 Loss -1.0904\n",
      "Epoch 71 Batch 350 Loss -1.1055\n",
      "Epoch 71 Loss -1.1152\n",
      "{'Epoch': 71}\n",
      "Epoch 72 Batch 0 Loss -0.3394\n",
      "Epoch 72 Batch 50 Loss -1.0356\n",
      "Epoch 72 Batch 100 Loss -1.0048\n",
      "Epoch 72 Batch 150 Loss -1.0193\n",
      "Epoch 72 Batch 200 Loss -1.0637\n",
      "Epoch 72 Batch 250 Loss -1.0873\n",
      "Epoch 72 Batch 300 Loss -1.0916\n",
      "Epoch 72 Batch 350 Loss -1.1062\n",
      "Epoch 72 Loss -1.1158\n",
      "{'Epoch': 72}\n",
      "Epoch 73 Batch 0 Loss -0.3345\n",
      "Epoch 73 Batch 50 Loss -1.0303\n",
      "Epoch 73 Batch 100 Loss -0.9878\n",
      "Epoch 73 Batch 150 Loss -0.9912\n",
      "Epoch 73 Batch 200 Loss -1.0360\n",
      "Epoch 73 Batch 250 Loss -1.0620\n",
      "Epoch 73 Batch 300 Loss -1.0737\n",
      "Epoch 73 Batch 350 Loss -1.0918\n",
      "Epoch 73 Loss -1.1022\n",
      "{'Epoch': 73}\n",
      "Epoch 74 Batch 0 Loss -0.3023\n",
      "Epoch 74 Batch 50 Loss -1.0434\n",
      "Epoch 74 Batch 100 Loss -1.0133\n",
      "Epoch 74 Batch 150 Loss -1.0257\n",
      "Epoch 74 Batch 200 Loss -1.0695\n",
      "Epoch 74 Batch 250 Loss -1.0823\n",
      "Epoch 74 Batch 300 Loss -1.0817\n",
      "Epoch 74 Batch 350 Loss -1.0918\n",
      "Epoch 74 Loss -1.0999\n",
      "{'Epoch': 74}\n",
      "Epoch 75 Batch 0 Loss -0.2025\n",
      "Epoch 75 Batch 50 Loss -1.0068\n",
      "Epoch 75 Batch 100 Loss -0.9913\n",
      "Epoch 75 Batch 150 Loss -1.0130\n",
      "Epoch 75 Batch 200 Loss -1.0640\n",
      "Epoch 75 Batch 250 Loss -1.0920\n",
      "Epoch 75 Batch 300 Loss -1.0990\n",
      "Epoch 75 Batch 350 Loss -1.1138\n",
      "Epoch 75 Loss -1.1235\n",
      "{'Epoch': 75}\n",
      "Epoch 76 Batch 0 Loss -0.3243\n",
      "Epoch 76 Batch 50 Loss -1.0379\n",
      "Epoch 76 Batch 100 Loss -1.0085\n",
      "Epoch 76 Batch 150 Loss -1.0226\n",
      "Epoch 76 Batch 200 Loss -1.0672\n",
      "Epoch 76 Batch 250 Loss -1.0901\n",
      "Epoch 76 Batch 300 Loss -1.0884\n",
      "Epoch 76 Batch 350 Loss -1.0972\n",
      "Epoch 76 Loss -1.1063\n",
      "{'Epoch': 76}\n",
      "Epoch 77 Batch 0 Loss -0.3118\n",
      "Epoch 77 Batch 50 Loss -1.0303\n",
      "Epoch 77 Batch 100 Loss -0.9963\n",
      "Epoch 77 Batch 150 Loss -1.0208\n",
      "Epoch 77 Batch 200 Loss -1.0741\n",
      "Epoch 77 Batch 250 Loss -1.1045\n",
      "Epoch 77 Batch 300 Loss -1.1129\n",
      "Epoch 77 Batch 350 Loss -1.1283\n",
      "Epoch 77 Loss -1.1381\n",
      "{'Epoch': 77}\n",
      "Epoch 78 Batch 0 Loss -0.3271\n",
      "Epoch 78 Batch 50 Loss -1.0605\n",
      "Epoch 78 Batch 100 Loss -1.0319\n",
      "Epoch 78 Batch 150 Loss -1.0458\n",
      "Epoch 78 Batch 200 Loss -1.0904\n",
      "Epoch 78 Batch 250 Loss -1.0990\n",
      "Epoch 78 Batch 300 Loss -1.0884\n",
      "Epoch 78 Batch 350 Loss -1.0982\n",
      "Epoch 78 Loss -1.1064\n",
      "{'Epoch': 78}\n",
      "Epoch 79 Batch 0 Loss -0.3124\n",
      "Epoch 79 Batch 50 Loss -0.9846\n",
      "Epoch 79 Batch 100 Loss -0.9606\n",
      "Epoch 79 Batch 150 Loss -0.9909\n",
      "Epoch 79 Batch 200 Loss -1.0473\n",
      "Epoch 79 Batch 250 Loss -1.0791\n",
      "Epoch 79 Batch 300 Loss -1.0891\n",
      "Epoch 79 Batch 350 Loss -1.1053\n",
      "Epoch 79 Loss -1.1153\n",
      "{'Epoch': 79}\n",
      "Epoch 80 Batch 0 Loss -0.2959\n",
      "Epoch 80 Batch 50 Loss -1.0426\n",
      "Epoch 80 Batch 100 Loss -1.0107\n",
      "Epoch 80 Batch 150 Loss -1.0236\n",
      "Epoch 80 Batch 200 Loss -1.0677\n",
      "Epoch 80 Batch 250 Loss -1.0805\n",
      "Epoch 80 Batch 300 Loss -1.0772\n",
      "Epoch 80 Batch 350 Loss -1.0838\n",
      "Epoch 80 Loss -1.0930\n",
      "{'Epoch': 80}\n",
      "Epoch 81 Batch 0 Loss -0.3516\n",
      "Epoch 81 Batch 50 Loss -1.0486\n",
      "Epoch 81 Batch 100 Loss -1.0229\n",
      "Epoch 81 Batch 150 Loss -1.0402\n",
      "Epoch 81 Batch 200 Loss -1.0890\n",
      "Epoch 81 Batch 250 Loss -1.1010\n",
      "Epoch 81 Batch 300 Loss -1.0978\n",
      "Epoch 81 Batch 350 Loss -1.1073\n",
      "Epoch 81 Loss -1.1170\n",
      "{'Epoch': 81}\n",
      "Epoch 82 Batch 0 Loss -0.3068\n",
      "Epoch 82 Batch 50 Loss -1.0456\n",
      "Epoch 82 Batch 100 Loss -1.0140\n",
      "Epoch 82 Batch 150 Loss -1.0263\n",
      "Epoch 82 Batch 200 Loss -1.0715\n",
      "Epoch 82 Batch 250 Loss -1.0949\n",
      "Epoch 82 Batch 300 Loss -1.0992\n",
      "Epoch 82 Batch 350 Loss -1.1129\n",
      "Epoch 82 Loss -1.1216\n",
      "{'Epoch': 82}\n",
      "Epoch 83 Batch 0 Loss -0.2989\n",
      "Epoch 83 Batch 50 Loss -1.0345\n",
      "Epoch 83 Batch 100 Loss -1.0058\n",
      "Epoch 83 Batch 150 Loss -1.0193\n",
      "Epoch 83 Batch 200 Loss -1.0644\n",
      "Epoch 83 Batch 250 Loss -1.0805\n",
      "Epoch 83 Batch 300 Loss -1.0672\n",
      "Epoch 83 Batch 350 Loss -1.0783\n",
      "Epoch 83 Loss -1.0883\n",
      "{'Epoch': 83}\n",
      "Epoch 84 Batch 0 Loss -0.2881\n",
      "Epoch 84 Batch 50 Loss -0.9841\n",
      "Epoch 84 Batch 100 Loss -0.9612\n",
      "Epoch 84 Batch 150 Loss -0.9807\n",
      "Epoch 84 Batch 200 Loss -1.0295\n",
      "Epoch 84 Batch 250 Loss -1.0601\n",
      "Epoch 84 Batch 300 Loss -1.0637\n",
      "Epoch 84 Batch 350 Loss -1.0785\n",
      "Epoch 84 Loss -1.0880\n",
      "{'Epoch': 84}\n",
      "Epoch 85 Batch 0 Loss -0.2820\n",
      "Epoch 85 Batch 50 Loss -1.0208\n",
      "Epoch 85 Batch 100 Loss -0.9865\n",
      "Epoch 85 Batch 150 Loss -1.0018\n",
      "Epoch 85 Batch 200 Loss -1.0483\n",
      "Epoch 85 Batch 250 Loss -1.0732\n",
      "Epoch 85 Batch 300 Loss -1.0753\n",
      "Epoch 85 Batch 350 Loss -1.0891\n",
      "Epoch 85 Loss -1.0973\n",
      "{'Epoch': 85}\n",
      "Epoch 86 Batch 0 Loss -0.2790\n",
      "Epoch 86 Batch 50 Loss -0.9843\n",
      "Epoch 86 Batch 100 Loss -0.9551\n",
      "Epoch 86 Batch 150 Loss -0.9786\n",
      "Epoch 86 Batch 200 Loss -1.0331\n",
      "Epoch 86 Batch 250 Loss -1.0636\n",
      "Epoch 86 Batch 300 Loss -1.0707\n",
      "Epoch 86 Batch 350 Loss -1.0857\n",
      "Epoch 86 Loss -1.0949\n",
      "{'Epoch': 86}\n",
      "Epoch 87 Batch 0 Loss -0.3060\n",
      "Epoch 87 Batch 50 Loss -1.0237\n",
      "Epoch 87 Batch 100 Loss -0.9866\n",
      "Epoch 87 Batch 150 Loss -1.0009\n",
      "Epoch 87 Batch 200 Loss -1.0464\n",
      "Epoch 87 Batch 250 Loss -1.0711\n",
      "Epoch 87 Batch 300 Loss -1.0726\n",
      "Epoch 87 Batch 350 Loss -1.0862\n",
      "Epoch 87 Loss -1.0951\n",
      "{'Epoch': 87}\n",
      "Epoch 88 Batch 0 Loss -0.3143\n",
      "Epoch 88 Batch 50 Loss -1.0211\n",
      "Epoch 88 Batch 100 Loss -0.9895\n",
      "Epoch 88 Batch 150 Loss -1.0032\n",
      "Epoch 88 Batch 200 Loss -1.0473\n",
      "Epoch 88 Batch 250 Loss -1.0709\n",
      "Epoch 88 Batch 300 Loss -1.0761\n",
      "Epoch 88 Batch 350 Loss -1.0900\n",
      "Epoch 88 Loss -1.0982\n",
      "{'Epoch': 88}\n",
      "Epoch 89 Batch 0 Loss -0.2880\n",
      "Epoch 89 Batch 50 Loss -1.0183\n",
      "Epoch 89 Batch 100 Loss -0.9848\n",
      "Epoch 89 Batch 150 Loss -0.9980\n",
      "Epoch 89 Batch 200 Loss -1.0505\n",
      "Epoch 89 Batch 250 Loss -1.0801\n",
      "Epoch 89 Batch 300 Loss -1.0887\n",
      "Epoch 89 Batch 350 Loss -1.1049\n",
      "Epoch 89 Loss -1.1139\n",
      "{'Epoch': 89}\n",
      "Epoch 90 Batch 0 Loss -0.3246\n",
      "Epoch 90 Batch 50 Loss -1.0383\n",
      "Epoch 90 Batch 100 Loss -1.0085\n",
      "Epoch 90 Batch 150 Loss -1.0239\n",
      "Epoch 90 Batch 200 Loss -1.0690\n",
      "Epoch 90 Batch 250 Loss -1.0934\n",
      "Epoch 90 Batch 300 Loss -1.0966\n",
      "Epoch 90 Batch 350 Loss -1.1110\n",
      "Epoch 90 Loss -1.1199\n",
      "{'Epoch': 90}\n",
      "Epoch 91 Batch 0 Loss -0.3325\n",
      "Epoch 91 Batch 50 Loss -1.0373\n",
      "Epoch 91 Batch 100 Loss -1.0069\n",
      "Epoch 91 Batch 150 Loss -1.0218\n",
      "Epoch 91 Batch 200 Loss -1.0663\n",
      "Epoch 91 Batch 250 Loss -1.0907\n",
      "Epoch 91 Batch 300 Loss -1.0953\n",
      "Epoch 91 Batch 350 Loss -1.1100\n",
      "Epoch 91 Loss -1.1197\n",
      "{'Epoch': 91}\n",
      "Epoch 92 Batch 0 Loss -0.3428\n",
      "Epoch 92 Batch 50 Loss -1.0306\n",
      "Epoch 92 Batch 100 Loss -0.9955\n",
      "Epoch 92 Batch 150 Loss -1.0139\n",
      "Epoch 92 Batch 200 Loss -1.0622\n",
      "Epoch 92 Batch 250 Loss -1.0885\n",
      "Epoch 92 Batch 300 Loss -1.0931\n",
      "Epoch 92 Batch 350 Loss -1.1086\n",
      "Epoch 92 Loss -1.1184\n",
      "{'Epoch': 92}\n",
      "Epoch 93 Batch 0 Loss -0.3448\n",
      "Epoch 93 Batch 50 Loss -1.0380\n",
      "Epoch 93 Batch 100 Loss -1.0084\n",
      "Epoch 93 Batch 150 Loss -1.0231\n",
      "Epoch 93 Batch 200 Loss -1.0681\n",
      "Epoch 93 Batch 250 Loss -1.0920\n",
      "Epoch 93 Batch 300 Loss -1.0963\n",
      "Epoch 93 Batch 350 Loss -1.1112\n",
      "Epoch 93 Loss -1.1208\n",
      "{'Epoch': 93}\n",
      "Epoch 94 Batch 0 Loss -0.3383\n",
      "Epoch 94 Batch 50 Loss -1.0289\n",
      "Epoch 94 Batch 100 Loss -0.9989\n",
      "Epoch 94 Batch 150 Loss -1.0174\n",
      "Epoch 94 Batch 200 Loss -1.0651\n",
      "Epoch 94 Batch 250 Loss -1.0903\n",
      "Epoch 94 Batch 300 Loss -1.0946\n",
      "Epoch 94 Batch 350 Loss -1.1098\n",
      "Epoch 94 Loss -1.1193\n",
      "{'Epoch': 94}\n",
      "Epoch 95 Batch 0 Loss -0.3448\n",
      "Epoch 95 Batch 50 Loss -1.0406\n",
      "Epoch 95 Batch 100 Loss -1.0098\n",
      "Epoch 95 Batch 150 Loss -1.0238\n",
      "Epoch 95 Batch 200 Loss -1.0688\n",
      "Epoch 95 Batch 250 Loss -1.0928\n",
      "Epoch 95 Batch 300 Loss -1.0970\n",
      "Epoch 95 Batch 350 Loss -1.1118\n",
      "Epoch 95 Loss -1.1213\n",
      "{'Epoch': 95}\n",
      "Epoch 96 Batch 0 Loss -0.3420\n",
      "Epoch 96 Batch 50 Loss -1.0323\n",
      "Epoch 96 Batch 100 Loss -0.9959\n",
      "Epoch 96 Batch 150 Loss -1.0029\n",
      "Epoch 96 Batch 200 Loss -1.0541\n",
      "Epoch 96 Batch 250 Loss -1.0841\n",
      "Epoch 96 Batch 300 Loss -1.0929\n",
      "Epoch 96 Batch 350 Loss -1.1091\n",
      "Epoch 96 Loss -1.1194\n",
      "{'Epoch': 96}\n",
      "Epoch 97 Batch 0 Loss -0.3445\n",
      "Epoch 97 Batch 50 Loss -1.0412\n",
      "Epoch 97 Batch 100 Loss -1.0114\n",
      "Epoch 97 Batch 150 Loss -1.0255\n",
      "Epoch 97 Batch 200 Loss -1.0703\n",
      "Epoch 97 Batch 250 Loss -1.0931\n",
      "Epoch 97 Batch 300 Loss -1.0971\n",
      "Epoch 97 Batch 350 Loss -1.1109\n",
      "Epoch 97 Loss -1.1205\n",
      "{'Epoch': 97}\n",
      "Epoch 98 Batch 0 Loss -0.3492\n",
      "Epoch 98 Batch 50 Loss -1.0406\n",
      "Epoch 98 Batch 100 Loss -1.0102\n",
      "Epoch 98 Batch 150 Loss -1.0240\n",
      "Epoch 98 Batch 200 Loss -1.0691\n",
      "Epoch 98 Batch 250 Loss -1.0933\n",
      "Epoch 98 Batch 300 Loss -1.0973\n",
      "Epoch 98 Batch 350 Loss -1.1119\n",
      "Epoch 98 Loss -1.1212\n",
      "{'Epoch': 98}\n",
      "Epoch 99 Batch 0 Loss -0.3506\n",
      "Epoch 99 Batch 50 Loss -1.0401\n",
      "Epoch 99 Batch 100 Loss -1.0092\n",
      "Epoch 99 Batch 150 Loss -1.0239\n",
      "Epoch 99 Batch 200 Loss -1.0693\n",
      "Epoch 99 Batch 250 Loss -1.0932\n",
      "Epoch 99 Batch 300 Loss -1.0970\n",
      "Epoch 99 Batch 350 Loss -1.1119\n",
      "Epoch 99 Loss -1.1217\n",
      "{'Epoch': 99}\n",
      "Epoch 100 Batch 0 Loss -0.3532\n",
      "Epoch 100 Batch 50 Loss -1.0314\n",
      "Epoch 100 Batch 100 Loss -0.9909\n",
      "Epoch 100 Batch 150 Loss -0.9952\n",
      "Epoch 100 Batch 200 Loss -1.0421\n",
      "Epoch 100 Batch 250 Loss -1.0589\n",
      "Epoch 100 Batch 300 Loss -1.0643\n",
      "Epoch 100 Batch 350 Loss -1.0773\n",
      "Epoch 100 Loss -1.0864\n",
      "{'Epoch': 100}\n",
      "Epoch 101 Batch 0 Loss -0.2541\n",
      "Epoch 101 Batch 50 Loss -0.9649\n",
      "Epoch 101 Batch 100 Loss -0.9607\n",
      "Epoch 101 Batch 150 Loss -0.9794\n",
      "Epoch 101 Batch 200 Loss -1.0385\n",
      "Epoch 101 Batch 250 Loss -1.0757\n",
      "Epoch 101 Batch 300 Loss -1.0891\n",
      "Epoch 101 Batch 350 Loss -1.1085\n",
      "Epoch 101 Loss -1.1208\n",
      "{'Epoch': 101}\n",
      "Epoch 102 Batch 0 Loss -0.3370\n",
      "Epoch 102 Batch 50 Loss -1.0671\n",
      "Epoch 102 Batch 100 Loss -1.0399\n",
      "Epoch 102 Batch 150 Loss -1.0543\n",
      "Epoch 102 Batch 200 Loss -1.0995\n",
      "Epoch 102 Batch 250 Loss -1.1035\n",
      "Epoch 102 Batch 300 Loss -1.0956\n",
      "Epoch 102 Batch 350 Loss -1.1087\n",
      "Epoch 102 Loss -1.1193\n",
      "{'Epoch': 102}\n",
      "Epoch 103 Batch 0 Loss -0.3339\n",
      "Epoch 103 Batch 50 Loss -1.0516\n",
      "Epoch 103 Batch 100 Loss -1.0213\n",
      "Epoch 103 Batch 150 Loss -1.0345\n",
      "Epoch 103 Batch 200 Loss -1.0807\n",
      "Epoch 103 Batch 250 Loss -1.1042\n",
      "Epoch 103 Batch 300 Loss -1.1077\n",
      "Epoch 103 Batch 350 Loss -1.1201\n",
      "Epoch 103 Loss -1.1253\n",
      "{'Epoch': 103}\n",
      "Epoch 104 Batch 0 Loss -0.3084\n",
      "Epoch 104 Batch 50 Loss -0.9861\n",
      "Epoch 104 Batch 100 Loss -0.9495\n",
      "Epoch 104 Batch 150 Loss -0.9693\n",
      "Epoch 104 Batch 200 Loss -1.0190\n",
      "Epoch 104 Batch 250 Loss -1.0493\n",
      "Epoch 104 Batch 300 Loss -1.0618\n",
      "Epoch 104 Batch 350 Loss -1.0850\n",
      "Epoch 104 Loss -1.0992\n",
      "{'Epoch': 104}\n",
      "Epoch 105 Batch 0 Loss -0.3586\n",
      "Epoch 105 Batch 50 Loss -1.0671\n",
      "Epoch 105 Batch 100 Loss -1.0372\n",
      "Epoch 105 Batch 150 Loss -1.0521\n",
      "Epoch 105 Batch 200 Loss -1.0982\n",
      "Epoch 105 Batch 250 Loss -1.1229\n",
      "Epoch 105 Batch 300 Loss -1.1064\n",
      "Epoch 105 Batch 350 Loss -1.1133\n",
      "Epoch 105 Loss -1.1240\n",
      "{'Epoch': 105}\n",
      "Epoch 106 Batch 0 Loss -0.3466\n",
      "Epoch 106 Batch 50 Loss -1.0626\n",
      "Epoch 106 Batch 100 Loss -1.0343\n",
      "Epoch 106 Batch 150 Loss -1.0496\n",
      "Epoch 106 Batch 200 Loss -1.0961\n",
      "Epoch 106 Batch 250 Loss -1.1212\n",
      "Epoch 106 Batch 300 Loss -1.1245\n",
      "Epoch 106 Batch 350 Loss -1.1376\n",
      "Epoch 106 Loss -1.1468\n",
      "{'Epoch': 106}\n",
      "Epoch 107 Batch 0 Loss -0.3376\n",
      "Epoch 107 Batch 50 Loss -1.0426\n",
      "Epoch 107 Batch 100 Loss -0.9798\n",
      "Epoch 107 Batch 150 Loss -1.0006\n",
      "Epoch 107 Batch 200 Loss -1.0547\n",
      "Epoch 107 Batch 250 Loss -1.0861\n",
      "Epoch 107 Batch 300 Loss -1.0949\n",
      "Epoch 107 Batch 350 Loss -1.1114\n",
      "Epoch 107 Loss -1.1217\n",
      "{'Epoch': 107}\n",
      "Epoch 108 Batch 0 Loss -0.3435\n",
      "Epoch 108 Batch 50 Loss -1.0424\n",
      "Epoch 108 Batch 100 Loss -1.0128\n",
      "Epoch 108 Batch 150 Loss -1.0269\n",
      "Epoch 108 Batch 200 Loss -1.0723\n",
      "Epoch 108 Batch 250 Loss -1.0958\n",
      "Epoch 108 Batch 300 Loss -1.1001\n",
      "Epoch 108 Batch 350 Loss -1.1147\n",
      "Epoch 108 Loss -1.1235\n",
      "{'Epoch': 108}\n",
      "Epoch 109 Batch 0 Loss -0.3335\n",
      "Epoch 109 Batch 50 Loss -1.0424\n",
      "Epoch 109 Batch 100 Loss -1.0125\n",
      "Epoch 109 Batch 150 Loss -1.0265\n",
      "Epoch 109 Batch 200 Loss -1.0704\n",
      "Epoch 109 Batch 250 Loss -1.0941\n",
      "Epoch 109 Batch 300 Loss -1.0985\n",
      "Epoch 109 Batch 350 Loss -1.1132\n",
      "Epoch 109 Loss -1.1229\n",
      "{'Epoch': 109}\n",
      "Epoch 110 Batch 0 Loss -0.3454\n",
      "Epoch 110 Batch 50 Loss -1.0320\n",
      "Epoch 110 Batch 100 Loss -0.9950\n",
      "Epoch 110 Batch 150 Loss -1.0142\n",
      "Epoch 110 Batch 200 Loss -1.0636\n",
      "Epoch 110 Batch 250 Loss -1.0903\n",
      "Epoch 110 Batch 300 Loss -1.0962\n",
      "Epoch 110 Batch 350 Loss -1.1102\n",
      "Epoch 110 Loss -1.1199\n",
      "{'Epoch': 110}\n",
      "Epoch 111 Batch 0 Loss -0.3390\n",
      "Epoch 111 Batch 50 Loss -1.0440\n",
      "Epoch 111 Batch 100 Loss -1.0144\n",
      "Epoch 111 Batch 150 Loss -1.0282\n",
      "Epoch 111 Batch 200 Loss -1.0723\n",
      "Epoch 111 Batch 250 Loss -1.0960\n",
      "Epoch 111 Batch 300 Loss -1.1002\n",
      "Epoch 111 Batch 350 Loss -1.1143\n",
      "Epoch 111 Loss -1.1233\n",
      "{'Epoch': 111}\n",
      "Epoch 112 Batch 0 Loss -0.3357\n",
      "Epoch 112 Batch 50 Loss -1.0423\n",
      "Epoch 112 Batch 100 Loss -1.0117\n",
      "Epoch 112 Batch 150 Loss -1.0263\n",
      "Epoch 112 Batch 200 Loss -1.0708\n",
      "Epoch 112 Batch 250 Loss -1.0953\n",
      "Epoch 112 Batch 300 Loss -1.0992\n",
      "Epoch 112 Batch 350 Loss -1.1143\n",
      "Epoch 112 Loss -1.1238\n",
      "{'Epoch': 112}\n",
      "Epoch 113 Batch 0 Loss -0.3470\n",
      "Epoch 113 Batch 50 Loss -1.0424\n",
      "Epoch 113 Batch 100 Loss -1.0130\n",
      "Epoch 113 Batch 150 Loss -1.0267\n",
      "Epoch 113 Batch 200 Loss -1.0711\n",
      "Epoch 113 Batch 250 Loss -1.0935\n",
      "Epoch 113 Batch 300 Loss -1.0981\n",
      "Epoch 113 Batch 350 Loss -1.1133\n",
      "Epoch 113 Loss -1.1229\n",
      "{'Epoch': 113}\n",
      "Epoch 114 Batch 0 Loss -0.3498\n",
      "Epoch 114 Batch 50 Loss -1.0415\n",
      "Epoch 114 Batch 100 Loss -1.0074\n",
      "Epoch 114 Batch 150 Loss -1.0216\n",
      "Epoch 114 Batch 200 Loss -1.0678\n",
      "Epoch 114 Batch 250 Loss -1.0929\n",
      "Epoch 114 Batch 300 Loss -1.0978\n",
      "Epoch 114 Batch 350 Loss -1.1131\n",
      "Epoch 114 Loss -1.1228\n",
      "{'Epoch': 114}\n",
      "Epoch 115 Batch 0 Loss -0.3503\n",
      "Epoch 115 Batch 50 Loss -1.0412\n",
      "Epoch 115 Batch 100 Loss -1.0054\n",
      "Epoch 115 Batch 150 Loss -1.0218\n",
      "Epoch 115 Batch 200 Loss -1.0677\n",
      "Epoch 115 Batch 250 Loss -1.0926\n",
      "Epoch 115 Batch 300 Loss -1.0968\n",
      "Epoch 115 Batch 350 Loss -1.1039\n",
      "Epoch 115 Loss -1.1126\n",
      "{'Epoch': 115}\n",
      "Epoch 116 Batch 0 Loss -0.3449\n",
      "Epoch 116 Batch 50 Loss -1.0571\n",
      "Epoch 116 Batch 100 Loss -1.0323\n",
      "Epoch 116 Batch 150 Loss -1.0499\n",
      "Epoch 116 Batch 200 Loss -1.0976\n",
      "Epoch 116 Batch 250 Loss -1.1059\n",
      "Epoch 116 Batch 300 Loss -1.0892\n",
      "Epoch 116 Batch 350 Loss -1.0979\n",
      "Epoch 116 Loss -1.1070\n",
      "{'Epoch': 116}\n",
      "Epoch 117 Batch 0 Loss -0.3267\n",
      "Epoch 117 Batch 50 Loss -1.0272\n",
      "Epoch 117 Batch 100 Loss -0.9988\n",
      "Epoch 117 Batch 150 Loss -1.0116\n",
      "Epoch 117 Batch 200 Loss -1.0569\n",
      "Epoch 117 Batch 250 Loss -1.0808\n",
      "Epoch 117 Batch 300 Loss -1.0864\n",
      "Epoch 117 Batch 350 Loss -1.0990\n",
      "Epoch 117 Loss -1.1080\n",
      "{'Epoch': 117}\n",
      "Epoch 118 Batch 0 Loss -0.3085\n",
      "Epoch 118 Batch 50 Loss -1.0220\n",
      "Epoch 118 Batch 100 Loss -0.9926\n",
      "Epoch 118 Batch 150 Loss -0.9990\n",
      "Epoch 118 Batch 200 Loss -1.0457\n",
      "Epoch 118 Batch 250 Loss -1.0749\n",
      "Epoch 118 Batch 300 Loss -1.0854\n",
      "Epoch 118 Batch 350 Loss -1.1032\n",
      "Epoch 118 Loss -1.1142\n",
      "{'Epoch': 118}\n",
      "Epoch 119 Batch 0 Loss -0.3393\n",
      "Epoch 119 Batch 50 Loss -1.0496\n",
      "Epoch 119 Batch 100 Loss -1.0186\n",
      "Epoch 119 Batch 150 Loss -1.0323\n",
      "Epoch 119 Batch 200 Loss -1.0783\n",
      "Epoch 119 Batch 250 Loss -1.1018\n",
      "Epoch 119 Batch 300 Loss -1.1059\n",
      "Epoch 119 Batch 350 Loss -1.1198\n",
      "Epoch 119 Loss -1.1290\n",
      "{'Epoch': 119}\n",
      "Epoch 120 Batch 0 Loss -0.3459\n",
      "Epoch 120 Batch 50 Loss -1.0439\n",
      "Epoch 120 Batch 100 Loss -1.0128\n",
      "Epoch 120 Batch 150 Loss -1.0273\n",
      "Epoch 120 Batch 200 Loss -1.0728\n",
      "Epoch 120 Batch 250 Loss -1.0971\n",
      "Epoch 120 Batch 300 Loss -1.1015\n",
      "Epoch 120 Batch 350 Loss -1.1163\n",
      "Epoch 120 Loss -1.1254\n",
      "{'Epoch': 120}\n",
      "Epoch 121 Batch 0 Loss -0.3389\n",
      "Epoch 121 Batch 50 Loss -1.0438\n",
      "Epoch 121 Batch 100 Loss -1.0127\n",
      "Epoch 121 Batch 150 Loss -1.0272\n",
      "Epoch 121 Batch 200 Loss -1.0723\n",
      "Epoch 121 Batch 250 Loss -1.0960\n",
      "Epoch 121 Batch 300 Loss -1.1007\n",
      "Epoch 121 Batch 350 Loss -1.1153\n",
      "Epoch 121 Loss -1.1248\n",
      "{'Epoch': 121}\n",
      "Epoch 122 Batch 0 Loss -0.3572\n",
      "Epoch 122 Batch 50 Loss -1.0440\n",
      "Epoch 122 Batch 100 Loss -1.0136\n",
      "Epoch 122 Batch 150 Loss -1.0278\n",
      "Epoch 122 Batch 200 Loss -1.0734\n",
      "Epoch 122 Batch 250 Loss -1.0973\n",
      "Epoch 122 Batch 300 Loss -1.1008\n",
      "Epoch 122 Batch 350 Loss -1.1113\n",
      "Epoch 122 Loss -1.1209\n",
      "{'Epoch': 122}\n",
      "Epoch 123 Batch 0 Loss -0.3326\n",
      "Epoch 123 Batch 50 Loss -1.0488\n",
      "Epoch 123 Batch 100 Loss -1.0182\n",
      "Epoch 123 Batch 150 Loss -1.0323\n",
      "Epoch 123 Batch 200 Loss -1.0780\n",
      "Epoch 123 Batch 250 Loss -1.1016\n",
      "Epoch 123 Batch 300 Loss -1.1055\n",
      "Epoch 123 Batch 350 Loss -1.1197\n",
      "Epoch 123 Loss -1.1285\n",
      "{'Epoch': 123}\n",
      "Epoch 124 Batch 0 Loss -0.3357\n",
      "Epoch 124 Batch 50 Loss -1.0414\n",
      "Epoch 124 Batch 100 Loss -0.9832\n",
      "Epoch 124 Batch 150 Loss -0.9862\n",
      "Epoch 124 Batch 200 Loss -1.0369\n",
      "Epoch 124 Batch 250 Loss -1.0665\n",
      "Epoch 124 Batch 300 Loss -1.0741\n",
      "Epoch 124 Batch 350 Loss -1.0894\n",
      "Epoch 124 Loss -1.0987\n",
      "{'Epoch': 124}\n",
      "Epoch 125 Batch 0 Loss -0.3292\n",
      "Epoch 125 Batch 50 Loss -1.0327\n",
      "Epoch 125 Batch 100 Loss -0.9992\n",
      "Epoch 125 Batch 150 Loss -1.0112\n",
      "Epoch 125 Batch 200 Loss -1.0554\n",
      "Epoch 125 Batch 250 Loss -1.0786\n",
      "Epoch 125 Batch 300 Loss -1.0793\n",
      "Epoch 125 Batch 350 Loss -1.0930\n",
      "Epoch 125 Loss -1.1025\n",
      "{'Epoch': 125}\n",
      "Epoch 126 Batch 0 Loss -0.3248\n",
      "Epoch 126 Batch 50 Loss -1.0289\n",
      "Epoch 126 Batch 100 Loss -0.9983\n",
      "Epoch 126 Batch 150 Loss -1.0116\n",
      "Epoch 126 Batch 200 Loss -1.0562\n",
      "Epoch 126 Batch 250 Loss -1.0790\n",
      "Epoch 126 Batch 300 Loss -1.0844\n",
      "Epoch 126 Batch 350 Loss -1.1019\n",
      "Epoch 126 Loss -1.1126\n",
      "{'Epoch': 126}\n",
      "Epoch 127 Batch 0 Loss -0.3490\n",
      "Epoch 127 Batch 50 Loss -1.0474\n",
      "Epoch 127 Batch 100 Loss -1.0180\n",
      "Epoch 127 Batch 150 Loss -1.0323\n",
      "Epoch 127 Batch 200 Loss -1.0779\n",
      "Epoch 127 Batch 250 Loss -1.1022\n",
      "Epoch 127 Batch 300 Loss -1.1062\n",
      "Epoch 127 Batch 350 Loss -1.1205\n",
      "Epoch 127 Loss -1.1298\n",
      "{'Epoch': 127}\n",
      "Epoch 128 Batch 0 Loss -0.3510\n",
      "Epoch 128 Batch 50 Loss -1.0458\n",
      "Epoch 128 Batch 100 Loss -1.0161\n",
      "Epoch 128 Batch 150 Loss -1.0304\n",
      "Epoch 128 Batch 200 Loss -1.0742\n",
      "Epoch 128 Batch 250 Loss -1.0974\n",
      "Epoch 128 Batch 300 Loss -1.1024\n",
      "Epoch 128 Batch 350 Loss -1.1171\n",
      "Epoch 128 Loss -1.1263\n",
      "{'Epoch': 128}\n",
      "Epoch 129 Batch 0 Loss -0.3442\n",
      "Epoch 129 Batch 50 Loss -1.0436\n",
      "Epoch 129 Batch 100 Loss -1.0101\n",
      "Epoch 129 Batch 150 Loss -1.0263\n",
      "Epoch 129 Batch 200 Loss -1.0722\n",
      "Epoch 129 Batch 250 Loss -1.0974\n",
      "Epoch 129 Batch 300 Loss -1.1023\n",
      "Epoch 129 Batch 350 Loss -1.1171\n",
      "Epoch 129 Loss -1.1263\n",
      "{'Epoch': 129}\n",
      "Epoch 130 Batch 0 Loss -0.3475\n",
      "Epoch 130 Batch 50 Loss -1.0463\n",
      "Epoch 130 Batch 100 Loss -1.0163\n",
      "Epoch 130 Batch 150 Loss -1.0301\n",
      "Epoch 130 Batch 200 Loss -1.0741\n",
      "Epoch 130 Batch 250 Loss -1.0984\n",
      "Epoch 130 Batch 300 Loss -1.1027\n",
      "Epoch 130 Batch 350 Loss -1.1174\n",
      "Epoch 130 Loss -1.1263\n",
      "{'Epoch': 130}\n",
      "Epoch 131 Batch 0 Loss -0.3393\n",
      "Epoch 131 Batch 50 Loss -1.0469\n",
      "Epoch 131 Batch 100 Loss -1.0151\n",
      "Epoch 131 Batch 150 Loss -1.0297\n",
      "Epoch 131 Batch 200 Loss -1.0756\n",
      "Epoch 131 Batch 250 Loss -1.0999\n",
      "Epoch 131 Batch 300 Loss -1.1040\n",
      "Epoch 131 Batch 350 Loss -1.1190\n",
      "Epoch 131 Loss -1.1285\n",
      "{'Epoch': 131}\n",
      "Epoch 132 Batch 0 Loss -0.3555\n",
      "Epoch 132 Batch 50 Loss -1.0437\n",
      "Epoch 132 Batch 100 Loss -1.0092\n",
      "Epoch 132 Batch 150 Loss -1.0256\n",
      "Epoch 132 Batch 200 Loss -1.0726\n",
      "Epoch 132 Batch 250 Loss -1.0975\n",
      "Epoch 132 Batch 300 Loss -1.1014\n",
      "Epoch 132 Batch 350 Loss -1.1116\n",
      "Epoch 132 Loss -1.1181\n",
      "{'Epoch': 132}\n",
      "Epoch 133 Batch 0 Loss -0.3292\n",
      "Epoch 133 Batch 50 Loss -1.0423\n",
      "Epoch 133 Batch 100 Loss -1.0165\n",
      "Epoch 133 Batch 150 Loss -1.0332\n",
      "Epoch 133 Batch 200 Loss -1.0807\n",
      "Epoch 133 Batch 250 Loss -1.1056\n",
      "Epoch 133 Batch 300 Loss -1.1114\n",
      "Epoch 133 Batch 350 Loss -1.1254\n",
      "Epoch 133 Loss -1.1344\n",
      "{'Epoch': 133}\n",
      "Epoch 134 Batch 0 Loss -0.3503\n",
      "Epoch 134 Batch 50 Loss -1.0484\n",
      "Epoch 134 Batch 100 Loss -1.0158\n",
      "Epoch 134 Batch 150 Loss -1.0295\n",
      "Epoch 134 Batch 200 Loss -1.0752\n",
      "Epoch 134 Batch 250 Loss -1.0994\n",
      "Epoch 134 Batch 300 Loss -1.1037\n",
      "Epoch 134 Batch 350 Loss -1.1188\n",
      "Epoch 134 Loss -1.1281\n",
      "{'Epoch': 134}\n",
      "Epoch 135 Batch 0 Loss -0.3421\n",
      "Epoch 135 Batch 50 Loss -1.0463\n",
      "Epoch 135 Batch 100 Loss -1.0161\n",
      "Epoch 135 Batch 150 Loss -1.0298\n",
      "Epoch 135 Batch 200 Loss -1.0746\n",
      "Epoch 135 Batch 250 Loss -1.0984\n",
      "Epoch 135 Batch 300 Loss -1.1029\n",
      "Epoch 135 Batch 350 Loss -1.1179\n",
      "Epoch 135 Loss -1.1274\n",
      "{'Epoch': 135}\n",
      "Epoch 136 Batch 0 Loss -0.3493\n",
      "Epoch 136 Batch 50 Loss -1.0460\n",
      "Epoch 136 Batch 100 Loss -1.0153\n",
      "Epoch 136 Batch 150 Loss -1.0295\n",
      "Epoch 136 Batch 200 Loss -1.0747\n",
      "Epoch 136 Batch 250 Loss -1.0995\n",
      "Epoch 136 Batch 300 Loss -1.1041\n",
      "Epoch 136 Batch 350 Loss -1.1190\n",
      "Epoch 136 Loss -1.1283\n",
      "{'Epoch': 136}\n",
      "Epoch 137 Batch 0 Loss -0.3410\n",
      "Epoch 137 Batch 50 Loss -1.0455\n",
      "Epoch 137 Batch 100 Loss -0.9937\n",
      "Epoch 137 Batch 150 Loss -0.9928\n",
      "Epoch 137 Batch 200 Loss -1.0364\n",
      "Epoch 137 Batch 250 Loss -1.0575\n",
      "Epoch 137 Batch 300 Loss -1.0642\n",
      "Epoch 137 Batch 350 Loss -1.0837\n",
      "Epoch 137 Loss -1.0943\n",
      "{'Epoch': 137}\n",
      "Epoch 138 Batch 0 Loss -0.3331\n",
      "Epoch 138 Batch 50 Loss -0.9907\n",
      "Epoch 138 Batch 100 Loss -0.9849\n",
      "Epoch 138 Batch 150 Loss -0.9890\n",
      "Epoch 138 Batch 200 Loss -1.0376\n",
      "Epoch 138 Batch 250 Loss -1.0628\n",
      "Epoch 138 Batch 300 Loss -1.0721\n",
      "Epoch 138 Batch 350 Loss -1.0881\n",
      "Epoch 138 Loss -1.0990\n",
      "{'Epoch': 138}\n",
      "Epoch 139 Batch 0 Loss -0.2981\n",
      "Epoch 139 Batch 50 Loss -1.0382\n",
      "Epoch 139 Batch 100 Loss -1.0023\n",
      "Epoch 139 Batch 150 Loss -1.0126\n",
      "Epoch 139 Batch 200 Loss -1.0575\n",
      "Epoch 139 Batch 250 Loss -1.0757\n",
      "Epoch 139 Batch 300 Loss -1.0741\n",
      "Epoch 139 Batch 350 Loss -1.0857\n",
      "Epoch 139 Loss -1.0958\n",
      "{'Epoch': 139}\n",
      "Epoch 140 Batch 0 Loss -0.3719\n",
      "Epoch 140 Batch 50 Loss -1.0267\n",
      "Epoch 140 Batch 100 Loss -0.9963\n",
      "Epoch 140 Batch 150 Loss -1.0050\n",
      "Epoch 140 Batch 200 Loss -1.0536\n",
      "Epoch 140 Batch 250 Loss -1.0822\n",
      "Epoch 140 Batch 300 Loss -1.0892\n",
      "Epoch 140 Batch 350 Loss -1.1033\n",
      "Epoch 140 Loss -1.1122\n",
      "{'Epoch': 140}\n",
      "Epoch 141 Batch 0 Loss -0.3337\n",
      "Epoch 141 Batch 50 Loss -1.0399\n",
      "Epoch 141 Batch 100 Loss -1.0060\n",
      "Epoch 141 Batch 150 Loss -1.0177\n",
      "Epoch 141 Batch 200 Loss -1.0611\n",
      "Epoch 141 Batch 250 Loss -1.0846\n",
      "Epoch 141 Batch 300 Loss -1.0861\n",
      "Epoch 141 Batch 350 Loss -1.0997\n",
      "Epoch 141 Loss -1.1087\n",
      "{'Epoch': 141}\n",
      "Epoch 142 Batch 0 Loss -0.3246\n",
      "Epoch 142 Batch 50 Loss -1.0320\n",
      "Epoch 142 Batch 100 Loss -1.0014\n",
      "Epoch 142 Batch 150 Loss -1.0140\n",
      "Epoch 142 Batch 200 Loss -1.0588\n",
      "Epoch 142 Batch 250 Loss -1.0814\n",
      "Epoch 142 Batch 300 Loss -1.0868\n",
      "Epoch 142 Batch 350 Loss -1.1011\n",
      "Epoch 142 Loss -1.1103\n",
      "{'Epoch': 142}\n",
      "Epoch 143 Batch 0 Loss -0.3034\n",
      "Epoch 143 Batch 50 Loss -1.0285\n",
      "Epoch 143 Batch 100 Loss -0.9995\n",
      "Epoch 143 Batch 150 Loss -1.0030\n",
      "Epoch 143 Batch 200 Loss -1.0475\n",
      "Epoch 143 Batch 250 Loss -1.0678\n",
      "Epoch 143 Batch 300 Loss -1.0734\n",
      "Epoch 143 Batch 350 Loss -1.0896\n",
      "Epoch 143 Loss -1.1011\n",
      "{'Epoch': 143}\n",
      "Epoch 144 Batch 0 Loss -0.3153\n",
      "Epoch 144 Batch 50 Loss -1.0399\n",
      "Epoch 144 Batch 100 Loss -1.0093\n",
      "Epoch 144 Batch 150 Loss -1.0211\n",
      "Epoch 144 Batch 200 Loss -1.0664\n",
      "Epoch 144 Batch 250 Loss -1.0908\n",
      "Epoch 144 Batch 300 Loss -1.0864\n",
      "Epoch 144 Batch 350 Loss -1.0969\n",
      "Epoch 144 Loss -1.1048\n",
      "{'Epoch': 144}\n",
      "Epoch 145 Batch 0 Loss -0.2863\n",
      "Epoch 145 Batch 50 Loss -1.0079\n",
      "Epoch 145 Batch 100 Loss -0.9851\n",
      "Epoch 145 Batch 150 Loss -0.9985\n",
      "Epoch 145 Batch 200 Loss -1.0474\n",
      "Epoch 145 Batch 250 Loss -1.0801\n",
      "Epoch 145 Batch 300 Loss -1.0923\n",
      "Epoch 145 Batch 350 Loss -1.1101\n",
      "Epoch 145 Loss -1.1215\n",
      "{'Epoch': 145}\n",
      "Epoch 146 Batch 0 Loss -0.3493\n",
      "Epoch 146 Batch 50 Loss -1.0545\n",
      "Epoch 146 Batch 100 Loss -1.0252\n",
      "Epoch 146 Batch 150 Loss -1.0389\n",
      "Epoch 146 Batch 200 Loss -1.0847\n",
      "Epoch 146 Batch 250 Loss -1.1073\n",
      "Epoch 146 Batch 300 Loss -1.1113\n",
      "Epoch 146 Batch 350 Loss -1.1252\n",
      "Epoch 146 Loss -1.1346\n",
      "{'Epoch': 146}\n",
      "Epoch 147 Batch 0 Loss -0.3386\n",
      "Epoch 147 Batch 50 Loss -1.0448\n",
      "Epoch 147 Batch 100 Loss -1.0177\n",
      "Epoch 147 Batch 150 Loss -1.0322\n",
      "Epoch 147 Batch 200 Loss -1.0787\n",
      "Epoch 147 Batch 250 Loss -1.1024\n",
      "Epoch 147 Batch 300 Loss -1.0994\n",
      "Epoch 147 Batch 350 Loss -1.1093\n",
      "Epoch 147 Loss -1.1174\n",
      "{'Epoch': 147}\n",
      "Epoch 148 Batch 0 Loss -0.3099\n",
      "Epoch 148 Batch 50 Loss -1.0301\n",
      "Epoch 148 Batch 100 Loss -0.9920\n",
      "Epoch 148 Batch 150 Loss -1.0079\n",
      "Epoch 148 Batch 200 Loss -1.0612\n",
      "Epoch 148 Batch 250 Loss -1.0922\n",
      "Epoch 148 Batch 300 Loss -1.1017\n",
      "Epoch 148 Batch 350 Loss -1.1177\n",
      "Epoch 148 Loss -1.1282\n",
      "{'Epoch': 148}\n",
      "Epoch 149 Batch 0 Loss -0.3285\n",
      "Epoch 149 Batch 50 Loss -1.0448\n",
      "Epoch 149 Batch 100 Loss -1.0192\n",
      "Epoch 149 Batch 150 Loss -1.0343\n",
      "Epoch 149 Batch 200 Loss -1.0802\n",
      "Epoch 149 Batch 250 Loss -1.1034\n",
      "Epoch 149 Batch 300 Loss -1.1079\n",
      "Epoch 149 Batch 350 Loss -1.1226\n",
      "Epoch 149 Loss -1.1323\n",
      "{'Epoch': 149}\n",
      "Epoch 150 Batch 0 Loss -0.3418\n",
      "Epoch 150 Batch 50 Loss -1.0470\n",
      "Epoch 150 Batch 100 Loss -1.0192\n",
      "Epoch 150 Batch 150 Loss -1.0333\n",
      "Epoch 150 Batch 200 Loss -1.0793\n",
      "Epoch 150 Batch 250 Loss -1.1016\n",
      "Epoch 150 Batch 300 Loss -1.1014\n",
      "Epoch 150 Batch 350 Loss -1.1153\n",
      "Epoch 150 Loss -1.1260\n",
      "{'Epoch': 150}\n",
      "Epoch 151 Batch 0 Loss -0.3328\n",
      "Epoch 151 Batch 50 Loss -1.0570\n",
      "Epoch 151 Batch 100 Loss -1.0275\n",
      "Epoch 151 Batch 150 Loss -1.0411\n",
      "Epoch 151 Batch 200 Loss -1.0865\n",
      "Epoch 151 Batch 250 Loss -1.1104\n",
      "Epoch 151 Batch 300 Loss -1.1110\n",
      "Epoch 151 Batch 350 Loss -1.1197\n",
      "Epoch 151 Loss -1.1228\n",
      "{'Epoch': 151}\n",
      "Epoch 152 Batch 0 Loss -0.2783\n",
      "Epoch 152 Batch 50 Loss -1.0102\n",
      "Epoch 152 Batch 100 Loss -0.9883\n",
      "Epoch 152 Batch 150 Loss -1.0155\n",
      "Epoch 152 Batch 200 Loss -1.0726\n",
      "Epoch 152 Batch 250 Loss -1.1043\n",
      "Epoch 152 Batch 300 Loss -1.1165\n",
      "Epoch 152 Batch 350 Loss -1.1341\n",
      "Epoch 152 Loss -1.1453\n",
      "{'Epoch': 152}\n",
      "Epoch 153 Batch 0 Loss -0.3456\n",
      "Epoch 153 Batch 50 Loss -1.0773\n",
      "Epoch 153 Batch 100 Loss -1.0488\n",
      "Epoch 153 Batch 150 Loss -1.0626\n",
      "Epoch 153 Batch 200 Loss -1.1079\n",
      "Epoch 153 Batch 250 Loss -1.1066\n",
      "Epoch 153 Batch 300 Loss -1.0978\n",
      "Epoch 153 Batch 350 Loss -1.1059\n",
      "Epoch 153 Loss -1.1128\n",
      "{'Epoch': 153}\n",
      "Epoch 154 Batch 0 Loss -0.3276\n",
      "Epoch 154 Batch 50 Loss -1.0153\n",
      "Epoch 154 Batch 100 Loss -0.9956\n",
      "Epoch 154 Batch 150 Loss -1.0185\n",
      "Epoch 154 Batch 200 Loss -1.0717\n",
      "Epoch 154 Batch 250 Loss -1.1013\n",
      "Epoch 154 Batch 300 Loss -1.1085\n",
      "Epoch 154 Batch 350 Loss -1.1234\n",
      "Epoch 154 Loss -1.1334\n",
      "{'Epoch': 154}\n",
      "Epoch 155 Batch 0 Loss -0.3572\n",
      "Epoch 155 Batch 50 Loss -1.0519\n",
      "Epoch 155 Batch 100 Loss -1.0224\n",
      "Epoch 155 Batch 150 Loss -1.0365\n",
      "Epoch 155 Batch 200 Loss -1.0805\n",
      "Epoch 155 Batch 250 Loss -1.1042\n",
      "Epoch 155 Batch 300 Loss -1.1090\n",
      "Epoch 155 Batch 350 Loss -1.1234\n",
      "Epoch 155 Loss -1.1331\n",
      "{'Epoch': 155}\n",
      "Epoch 156 Batch 0 Loss -0.3484\n",
      "Epoch 156 Batch 50 Loss -1.0485\n",
      "Epoch 156 Batch 100 Loss -1.0203\n",
      "Epoch 156 Batch 150 Loss -1.0345\n",
      "Epoch 156 Batch 200 Loss -1.0806\n",
      "Epoch 156 Batch 250 Loss -1.1040\n",
      "Epoch 156 Batch 300 Loss -1.0995\n",
      "Epoch 156 Batch 350 Loss -1.1038\n",
      "Epoch 156 Loss -1.1076\n",
      "{'Epoch': 156}\n",
      "Epoch 157 Batch 0 Loss -0.3031\n",
      "Epoch 157 Batch 50 Loss -1.0181\n",
      "Epoch 157 Batch 100 Loss -1.0057\n",
      "Epoch 157 Batch 150 Loss -1.0267\n",
      "Epoch 157 Batch 200 Loss -1.0773\n",
      "Epoch 157 Batch 250 Loss -1.1057\n",
      "Epoch 157 Batch 300 Loss -1.1136\n",
      "Epoch 157 Batch 350 Loss -1.1282\n",
      "Epoch 157 Loss -1.1376\n",
      "{'Epoch': 157}\n",
      "Epoch 158 Batch 0 Loss -0.3524\n",
      "Epoch 158 Batch 50 Loss -1.0561\n",
      "Epoch 158 Batch 100 Loss -1.0244\n",
      "Epoch 158 Batch 150 Loss -1.0378\n",
      "Epoch 158 Batch 200 Loss -1.0834\n",
      "Epoch 158 Batch 250 Loss -1.1069\n",
      "Epoch 158 Batch 300 Loss -1.1109\n",
      "Epoch 158 Batch 350 Loss -1.1250\n",
      "Epoch 158 Loss -1.1345\n",
      "{'Epoch': 158}\n",
      "Epoch 159 Batch 0 Loss -0.3632\n",
      "Epoch 159 Batch 50 Loss -1.0467\n",
      "Epoch 159 Batch 100 Loss -1.0156\n",
      "Epoch 159 Batch 150 Loss -1.0321\n",
      "Epoch 159 Batch 200 Loss -1.0785\n",
      "Epoch 159 Batch 250 Loss -1.1032\n",
      "Epoch 159 Batch 300 Loss -1.1074\n",
      "Epoch 159 Batch 350 Loss -1.1225\n",
      "Epoch 159 Loss -1.1320\n",
      "{'Epoch': 159}\n",
      "Epoch 160 Batch 0 Loss -0.3574\n",
      "Epoch 160 Batch 50 Loss -1.0524\n",
      "Epoch 160 Batch 100 Loss -1.0202\n",
      "Epoch 160 Batch 150 Loss -1.0348\n",
      "Epoch 160 Batch 200 Loss -1.0800\n",
      "Epoch 160 Batch 250 Loss -1.1041\n",
      "Epoch 160 Batch 300 Loss -1.1084\n",
      "Epoch 160 Batch 350 Loss -1.1231\n",
      "Epoch 160 Loss -1.1326\n",
      "{'Epoch': 160}\n",
      "Epoch 161 Batch 0 Loss -0.3474\n",
      "Epoch 161 Batch 50 Loss -1.0500\n",
      "Epoch 161 Batch 100 Loss -1.0184\n",
      "Epoch 161 Batch 150 Loss -1.0339\n",
      "Epoch 161 Batch 200 Loss -1.0793\n",
      "Epoch 161 Batch 250 Loss -1.1042\n",
      "Epoch 161 Batch 300 Loss -1.1088\n",
      "Epoch 161 Batch 350 Loss -1.1232\n",
      "Epoch 161 Loss -1.1322\n",
      "{'Epoch': 161}\n",
      "Epoch 162 Batch 0 Loss -0.3430\n",
      "Epoch 162 Batch 50 Loss -1.0524\n",
      "Epoch 162 Batch 100 Loss -1.0216\n",
      "Epoch 162 Batch 150 Loss -1.0360\n",
      "Epoch 162 Batch 200 Loss -1.0812\n",
      "Epoch 162 Batch 250 Loss -1.1049\n",
      "Epoch 162 Batch 300 Loss -1.1094\n",
      "Epoch 162 Batch 350 Loss -1.1239\n",
      "Epoch 162 Loss -1.1336\n",
      "{'Epoch': 162}\n",
      "Epoch 163 Batch 0 Loss -0.3561\n",
      "Epoch 163 Batch 50 Loss -1.0503\n",
      "Epoch 163 Batch 100 Loss -1.0190\n",
      "Epoch 163 Batch 150 Loss -1.0341\n",
      "Epoch 163 Batch 200 Loss -1.0798\n",
      "Epoch 163 Batch 250 Loss -1.1043\n",
      "Epoch 163 Batch 300 Loss -1.1075\n",
      "Epoch 163 Batch 350 Loss -1.1221\n",
      "Epoch 163 Loss -1.1309\n",
      "{'Epoch': 163}\n",
      "Epoch 164 Batch 0 Loss -0.3504\n",
      "Epoch 164 Batch 50 Loss -1.0519\n",
      "Epoch 164 Batch 100 Loss -1.0181\n",
      "Epoch 164 Batch 150 Loss -1.0335\n",
      "Epoch 164 Batch 200 Loss -1.0803\n",
      "Epoch 164 Batch 250 Loss -1.1048\n",
      "Epoch 164 Batch 300 Loss -1.1085\n",
      "Epoch 164 Batch 350 Loss -1.1226\n",
      "Epoch 164 Loss -1.1324\n",
      "{'Epoch': 164}\n",
      "Epoch 165 Batch 0 Loss -0.3576\n",
      "Epoch 165 Batch 50 Loss -1.0550\n",
      "Epoch 165 Batch 100 Loss -1.0224\n",
      "Epoch 165 Batch 150 Loss -1.0366\n",
      "Epoch 165 Batch 200 Loss -1.0823\n",
      "Epoch 165 Batch 250 Loss -1.1065\n",
      "Epoch 165 Batch 300 Loss -1.1108\n",
      "Epoch 165 Batch 350 Loss -1.1256\n",
      "Epoch 165 Loss -1.1350\n",
      "{'Epoch': 165}\n",
      "Epoch 166 Batch 0 Loss -0.3628\n",
      "Epoch 166 Batch 50 Loss -1.0527\n",
      "Epoch 166 Batch 100 Loss -1.0225\n",
      "Epoch 166 Batch 150 Loss -1.0363\n",
      "Epoch 166 Batch 200 Loss -1.0807\n",
      "Epoch 166 Batch 250 Loss -1.1047\n",
      "Epoch 166 Batch 300 Loss -1.1082\n",
      "Epoch 166 Batch 350 Loss -1.1217\n",
      "Epoch 166 Loss -1.1315\n",
      "{'Epoch': 166}\n",
      "Epoch 167 Batch 0 Loss -0.3667\n",
      "Epoch 167 Batch 50 Loss -1.0548\n",
      "Epoch 167 Batch 100 Loss -1.0239\n",
      "Epoch 167 Batch 150 Loss -1.0374\n",
      "Epoch 167 Batch 200 Loss -1.0829\n",
      "Epoch 167 Batch 250 Loss -1.1071\n",
      "Epoch 167 Batch 300 Loss -1.1103\n",
      "Epoch 167 Batch 350 Loss -1.1249\n",
      "Epoch 167 Loss -1.1341\n",
      "{'Epoch': 167}\n",
      "Epoch 168 Batch 0 Loss -0.3518\n",
      "Epoch 168 Batch 50 Loss -1.0535\n",
      "Epoch 168 Batch 100 Loss -1.0223\n",
      "Epoch 168 Batch 150 Loss -1.0360\n",
      "Epoch 168 Batch 200 Loss -1.0810\n",
      "Epoch 168 Batch 250 Loss -1.1050\n",
      "Epoch 168 Batch 300 Loss -1.1096\n",
      "Epoch 168 Batch 350 Loss -1.1234\n",
      "Epoch 168 Loss -1.1323\n",
      "{'Epoch': 168}\n",
      "Epoch 169 Batch 0 Loss -0.3436\n",
      "Epoch 169 Batch 50 Loss -1.0521\n",
      "Epoch 169 Batch 100 Loss -1.0214\n",
      "Epoch 169 Batch 150 Loss -1.0364\n",
      "Epoch 169 Batch 200 Loss -1.0822\n",
      "Epoch 169 Batch 250 Loss -1.1065\n",
      "Epoch 169 Batch 300 Loss -1.1109\n",
      "Epoch 169 Batch 350 Loss -1.1253\n",
      "Epoch 169 Loss -1.1344\n",
      "{'Epoch': 169}\n",
      "Epoch 170 Batch 0 Loss -0.3583\n",
      "Epoch 170 Batch 50 Loss -1.0533\n",
      "Epoch 170 Batch 100 Loss -1.0234\n",
      "Epoch 170 Batch 150 Loss -1.0372\n",
      "Epoch 170 Batch 200 Loss -1.0823\n",
      "Epoch 170 Batch 250 Loss -1.1063\n",
      "Epoch 170 Batch 300 Loss -1.1106\n",
      "Epoch 170 Batch 350 Loss -1.1250\n",
      "Epoch 170 Loss -1.1347\n",
      "{'Epoch': 170}\n",
      "Epoch 171 Batch 0 Loss -0.3683\n",
      "Epoch 171 Batch 50 Loss -1.0526\n",
      "Epoch 171 Batch 100 Loss -1.0164\n",
      "Epoch 171 Batch 150 Loss -1.0323\n",
      "Epoch 171 Batch 200 Loss -1.0789\n",
      "Epoch 171 Batch 250 Loss -1.1045\n",
      "Epoch 171 Batch 300 Loss -1.1096\n",
      "Epoch 171 Batch 350 Loss -1.1245\n",
      "Epoch 171 Loss -1.1336\n",
      "{'Epoch': 171}\n",
      "Epoch 172 Batch 0 Loss -0.3602\n",
      "Epoch 172 Batch 50 Loss -1.0552\n",
      "Epoch 172 Batch 100 Loss -1.0221\n",
      "Epoch 172 Batch 150 Loss -1.0361\n",
      "Epoch 172 Batch 200 Loss -1.0820\n",
      "Epoch 172 Batch 250 Loss -1.1060\n",
      "Epoch 172 Batch 300 Loss -1.1097\n",
      "Epoch 172 Batch 350 Loss -1.1117\n",
      "Epoch 172 Loss -1.1203\n",
      "{'Epoch': 172}\n",
      "Epoch 173 Batch 0 Loss -0.3557\n",
      "Epoch 173 Batch 50 Loss -1.0242\n",
      "Epoch 173 Batch 100 Loss -0.9881\n",
      "Epoch 173 Batch 150 Loss -0.9927\n",
      "Epoch 173 Batch 200 Loss -1.0363\n",
      "Epoch 173 Batch 250 Loss -1.0638\n",
      "Epoch 173 Batch 300 Loss -1.0697\n",
      "Epoch 173 Batch 350 Loss -1.0793\n",
      "Epoch 173 Loss -1.0875\n",
      "{'Epoch': 173}\n",
      "Epoch 174 Batch 0 Loss -0.3046\n",
      "Epoch 174 Batch 50 Loss -1.0072\n",
      "Epoch 174 Batch 100 Loss -0.9934\n",
      "Epoch 174 Batch 150 Loss -1.0104\n",
      "Epoch 174 Batch 200 Loss -1.0665\n",
      "Epoch 174 Batch 250 Loss -1.1008\n",
      "Epoch 174 Batch 300 Loss -1.1129\n",
      "Epoch 174 Batch 350 Loss -1.1308\n",
      "Epoch 174 Loss -1.1423\n",
      "{'Epoch': 174}\n",
      "Epoch 175 Batch 0 Loss -0.3587\n",
      "Epoch 175 Batch 50 Loss -1.0476\n",
      "Epoch 175 Batch 100 Loss -1.0056\n",
      "Epoch 175 Batch 150 Loss -1.0268\n",
      "Epoch 175 Batch 200 Loss -1.0770\n",
      "Epoch 175 Batch 250 Loss -1.1033\n",
      "Epoch 175 Batch 300 Loss -1.1065\n",
      "Epoch 175 Batch 350 Loss -1.1217\n",
      "Epoch 175 Loss -1.1318\n",
      "{'Epoch': 175}\n",
      "Epoch 176 Batch 0 Loss -0.3686\n",
      "Epoch 176 Batch 50 Loss -1.0534\n",
      "Epoch 176 Batch 100 Loss -1.0253\n",
      "Epoch 176 Batch 150 Loss -1.0395\n",
      "Epoch 176 Batch 200 Loss -1.0845\n",
      "Epoch 176 Batch 250 Loss -1.1081\n",
      "Epoch 176 Batch 300 Loss -1.1116\n",
      "Epoch 176 Batch 350 Loss -1.1257\n",
      "Epoch 176 Loss -1.1355\n",
      "{'Epoch': 176}\n",
      "Epoch 177 Batch 0 Loss -0.3632\n",
      "Epoch 177 Batch 50 Loss -1.0545\n",
      "Epoch 177 Batch 100 Loss -1.0263\n",
      "Epoch 177 Batch 150 Loss -1.0404\n",
      "Epoch 177 Batch 200 Loss -1.0858\n",
      "Epoch 177 Batch 250 Loss -1.1092\n",
      "Epoch 177 Batch 300 Loss -1.1110\n",
      "Epoch 177 Batch 350 Loss -1.1205\n",
      "Epoch 177 Loss -1.1264\n",
      "{'Epoch': 177}\n",
      "Epoch 178 Batch 0 Loss -0.3186\n",
      "Epoch 178 Batch 50 Loss -1.0357\n",
      "Epoch 178 Batch 100 Loss -1.0084\n",
      "Epoch 178 Batch 150 Loss -1.0224\n",
      "Epoch 178 Batch 200 Loss -1.0662\n",
      "Epoch 178 Batch 250 Loss -1.0888\n",
      "Epoch 178 Batch 300 Loss -1.0929\n",
      "Epoch 178 Batch 350 Loss -1.1076\n",
      "Epoch 178 Loss -1.1171\n",
      "{'Epoch': 178}\n",
      "Epoch 179 Batch 0 Loss -0.3255\n",
      "Epoch 179 Batch 50 Loss -1.0353\n",
      "Epoch 179 Batch 100 Loss -1.0056\n",
      "Epoch 179 Batch 150 Loss -1.0142\n",
      "Epoch 179 Batch 200 Loss -1.0609\n",
      "Epoch 179 Batch 250 Loss -1.0795\n",
      "Epoch 179 Batch 300 Loss -1.0817\n",
      "Epoch 179 Batch 350 Loss -1.1013\n",
      "Epoch 179 Loss -1.1145\n",
      "{'Epoch': 179}\n",
      "Epoch 180 Batch 0 Loss -0.3459\n",
      "Epoch 180 Batch 50 Loss -1.0773\n",
      "Epoch 180 Batch 100 Loss -1.0482\n",
      "Epoch 180 Batch 150 Loss -1.0644\n",
      "Epoch 180 Batch 200 Loss -1.1123\n",
      "Epoch 180 Batch 250 Loss -1.1395\n",
      "Epoch 180 Batch 300 Loss -1.1448\n",
      "Epoch 180 Batch 350 Loss -1.1582\n",
      "Epoch 180 Loss -1.1675\n",
      "{'Epoch': 180}\n",
      "Epoch 181 Batch 0 Loss -0.3511\n",
      "Epoch 181 Batch 50 Loss -1.0714\n",
      "Epoch 181 Batch 100 Loss -1.0441\n",
      "Epoch 181 Batch 150 Loss -1.0601\n",
      "Epoch 181 Batch 200 Loss -1.1063\n",
      "Epoch 181 Batch 250 Loss -1.1254\n",
      "Epoch 181 Batch 300 Loss -1.1328\n",
      "Epoch 181 Batch 350 Loss -1.1468\n",
      "Epoch 181 Loss -1.1566\n",
      "{'Epoch': 181}\n",
      "Epoch 182 Batch 0 Loss -0.3750\n",
      "Epoch 182 Batch 50 Loss -1.0778\n",
      "Epoch 182 Batch 100 Loss -1.0494\n",
      "Epoch 182 Batch 150 Loss -1.0633\n",
      "Epoch 182 Batch 200 Loss -1.1088\n",
      "Epoch 182 Batch 250 Loss -1.1322\n",
      "Epoch 182 Batch 300 Loss -1.1314\n",
      "Epoch 182 Batch 350 Loss -1.1466\n",
      "Epoch 182 Loss -1.1566\n",
      "{'Epoch': 182}\n",
      "Epoch 183 Batch 0 Loss -0.3683\n",
      "Epoch 183 Batch 50 Loss -1.0806\n",
      "Epoch 183 Batch 100 Loss -1.0522\n",
      "Epoch 183 Batch 150 Loss -1.0656\n",
      "Epoch 183 Batch 200 Loss -1.1101\n",
      "Epoch 183 Batch 250 Loss -1.1338\n",
      "Epoch 183 Batch 300 Loss -1.1375\n",
      "Epoch 183 Batch 350 Loss -1.1521\n",
      "Epoch 183 Loss -1.1611\n",
      "{'Epoch': 183}\n",
      "Epoch 184 Batch 0 Loss -0.3541\n",
      "Epoch 184 Batch 50 Loss -1.0566\n",
      "Epoch 184 Batch 100 Loss -1.0237\n",
      "Epoch 184 Batch 150 Loss -1.0448\n",
      "Epoch 184 Batch 200 Loss -1.0954\n",
      "Epoch 184 Batch 250 Loss -1.1126\n",
      "Epoch 184 Batch 300 Loss -1.1166\n",
      "Epoch 184 Batch 350 Loss -1.1279\n",
      "Epoch 184 Loss -1.1374\n",
      "{'Epoch': 184}\n",
      "Epoch 185 Batch 0 Loss -0.3671\n",
      "Epoch 185 Batch 50 Loss -1.0599\n",
      "Epoch 185 Batch 100 Loss -1.0323\n",
      "Epoch 185 Batch 150 Loss -1.0453\n",
      "Epoch 185 Batch 200 Loss -1.0896\n",
      "Epoch 185 Batch 250 Loss -1.1118\n",
      "Epoch 185 Batch 300 Loss -1.1117\n",
      "Epoch 185 Batch 350 Loss -1.1263\n",
      "Epoch 185 Loss -1.1361\n",
      "{'Epoch': 185}\n",
      "Epoch 186 Batch 0 Loss -0.3548\n",
      "Epoch 186 Batch 50 Loss -1.0556\n",
      "Epoch 186 Batch 100 Loss -1.0279\n",
      "Epoch 186 Batch 150 Loss -1.0423\n",
      "Epoch 186 Batch 200 Loss -1.0881\n",
      "Epoch 186 Batch 250 Loss -1.1117\n",
      "Epoch 186 Batch 300 Loss -1.1088\n",
      "Epoch 186 Batch 350 Loss -1.1179\n",
      "Epoch 186 Loss -1.1272\n",
      "{'Epoch': 186}\n",
      "Epoch 187 Batch 0 Loss -0.3774\n",
      "Epoch 187 Batch 50 Loss -1.0407\n",
      "Epoch 187 Batch 100 Loss -0.9960\n",
      "Epoch 187 Batch 150 Loss -1.0086\n",
      "Epoch 187 Batch 200 Loss -1.0643\n",
      "Epoch 187 Batch 250 Loss -1.0963\n",
      "Epoch 187 Batch 300 Loss -1.1064\n",
      "Epoch 187 Batch 350 Loss -1.1232\n",
      "Epoch 187 Loss -1.1336\n",
      "{'Epoch': 187}\n",
      "Epoch 188 Batch 0 Loss -0.3536\n",
      "Epoch 188 Batch 50 Loss -1.0619\n",
      "Epoch 188 Batch 100 Loss -1.0313\n",
      "Epoch 188 Batch 150 Loss -1.0442\n",
      "Epoch 188 Batch 200 Loss -1.0891\n",
      "Epoch 188 Batch 250 Loss -1.1131\n",
      "Epoch 188 Batch 300 Loss -1.1159\n",
      "Epoch 188 Batch 350 Loss -1.1300\n",
      "Epoch 188 Loss -1.1391\n",
      "{'Epoch': 188}\n",
      "Epoch 189 Batch 0 Loss -0.3504\n",
      "Epoch 189 Batch 50 Loss -1.0561\n",
      "Epoch 189 Batch 100 Loss -1.0263\n",
      "Epoch 189 Batch 150 Loss -1.0405\n",
      "Epoch 189 Batch 200 Loss -1.0858\n",
      "Epoch 189 Batch 250 Loss -1.1101\n",
      "Epoch 189 Batch 300 Loss -1.1141\n",
      "Epoch 189 Batch 350 Loss -1.1289\n",
      "Epoch 189 Loss -1.1382\n",
      "{'Epoch': 189}\n",
      "Epoch 190 Batch 0 Loss -0.3603\n",
      "Epoch 190 Batch 50 Loss -1.0562\n",
      "Epoch 190 Batch 100 Loss -1.0234\n",
      "Epoch 190 Batch 150 Loss -1.0384\n",
      "Epoch 190 Batch 200 Loss -1.0837\n",
      "Epoch 190 Batch 250 Loss -1.1093\n",
      "Epoch 190 Batch 300 Loss -1.1140\n",
      "Epoch 190 Batch 350 Loss -1.1287\n",
      "Epoch 190 Loss -1.1380\n",
      "{'Epoch': 190}\n",
      "Epoch 191 Batch 0 Loss -0.3511\n",
      "Epoch 191 Batch 50 Loss -1.0562\n",
      "Epoch 191 Batch 100 Loss -1.0256\n",
      "Epoch 191 Batch 150 Loss -1.0398\n",
      "Epoch 191 Batch 200 Loss -1.0850\n",
      "Epoch 191 Batch 250 Loss -1.1093\n",
      "Epoch 191 Batch 300 Loss -1.1134\n",
      "Epoch 191 Batch 350 Loss -1.1270\n",
      "Epoch 191 Loss -1.1367\n",
      "{'Epoch': 191}\n",
      "Epoch 192 Batch 0 Loss -0.3599\n",
      "Epoch 192 Batch 50 Loss -1.0572\n",
      "Epoch 192 Batch 100 Loss -1.0047\n",
      "Epoch 192 Batch 150 Loss -1.0083\n",
      "Epoch 192 Batch 200 Loss -1.0575\n",
      "Epoch 192 Batch 250 Loss -1.0866\n",
      "Epoch 192 Batch 300 Loss -1.0950\n",
      "Epoch 192 Batch 350 Loss -1.1097\n",
      "Epoch 192 Loss -1.1198\n",
      "{'Epoch': 192}\n",
      "Epoch 193 Batch 0 Loss -0.3123\n",
      "Epoch 193 Batch 50 Loss -1.0480\n",
      "Epoch 193 Batch 100 Loss -1.0133\n",
      "Epoch 193 Batch 150 Loss -1.0230\n",
      "Epoch 193 Batch 200 Loss -1.0679\n",
      "Epoch 193 Batch 250 Loss -1.0894\n",
      "Epoch 193 Batch 300 Loss -1.0932\n",
      "Epoch 193 Batch 350 Loss -1.1071\n",
      "Epoch 193 Loss -1.1157\n",
      "{'Epoch': 193}\n",
      "Epoch 194 Batch 0 Loss -0.3239\n",
      "Epoch 194 Batch 50 Loss -1.0305\n",
      "Epoch 194 Batch 100 Loss -1.0034\n",
      "Epoch 194 Batch 150 Loss -1.0174\n",
      "Epoch 194 Batch 200 Loss -1.0656\n",
      "Epoch 194 Batch 250 Loss -1.0862\n",
      "Epoch 194 Batch 300 Loss -1.0855\n",
      "Epoch 194 Batch 350 Loss -1.1003\n",
      "Epoch 194 Loss -1.1058\n",
      "{'Epoch': 194}\n",
      "Epoch 195 Batch 0 Loss -0.3239\n",
      "Epoch 195 Batch 50 Loss -1.0570\n",
      "Epoch 195 Batch 100 Loss -1.0374\n",
      "Epoch 195 Batch 150 Loss -1.0567\n",
      "Epoch 195 Batch 200 Loss -1.1060\n",
      "Epoch 195 Batch 250 Loss -1.1343\n",
      "Epoch 195 Batch 300 Loss -1.1420\n",
      "Epoch 195 Batch 350 Loss -1.1564\n",
      "Epoch 195 Loss -1.1628\n",
      "{'Epoch': 195}\n",
      "Epoch 196 Batch 0 Loss -0.3219\n",
      "Epoch 196 Batch 50 Loss -1.0465\n",
      "Epoch 196 Batch 100 Loss -1.0256\n",
      "Epoch 196 Batch 150 Loss -1.0404\n",
      "Epoch 196 Batch 200 Loss -1.0870\n",
      "Epoch 196 Batch 250 Loss -1.1105\n",
      "Epoch 196 Batch 300 Loss -1.1158\n",
      "Epoch 196 Batch 350 Loss -1.1301\n",
      "Epoch 196 Loss -1.1384\n",
      "{'Epoch': 196}\n",
      "Epoch 197 Batch 0 Loss -0.3342\n",
      "Epoch 197 Batch 50 Loss -1.0589\n",
      "Epoch 197 Batch 100 Loss -1.0299\n",
      "Epoch 197 Batch 150 Loss -1.0427\n",
      "Epoch 197 Batch 200 Loss -1.0873\n",
      "Epoch 197 Batch 250 Loss -1.1063\n",
      "Epoch 197 Batch 300 Loss -1.1121\n",
      "Epoch 197 Batch 350 Loss -1.1271\n",
      "Epoch 197 Loss -1.1361\n",
      "{'Epoch': 197}\n",
      "Epoch 198 Batch 0 Loss -0.3346\n",
      "Epoch 198 Batch 50 Loss -1.0604\n",
      "Epoch 198 Batch 100 Loss -1.0313\n",
      "Epoch 198 Batch 150 Loss -1.0426\n",
      "Epoch 198 Batch 200 Loss -1.0881\n",
      "Epoch 198 Batch 250 Loss -1.1101\n",
      "Epoch 198 Batch 300 Loss -1.1149\n",
      "Epoch 198 Batch 350 Loss -1.1291\n",
      "Epoch 198 Loss -1.1352\n",
      "{'Epoch': 198}\n",
      "Epoch 199 Batch 0 Loss -0.3234\n",
      "Epoch 199 Batch 50 Loss -1.0114\n",
      "Epoch 199 Batch 100 Loss -0.9927\n",
      "Epoch 199 Batch 150 Loss -1.0101\n",
      "Epoch 199 Batch 200 Loss -1.0595\n",
      "Epoch 199 Batch 250 Loss -1.0900\n",
      "Epoch 199 Batch 300 Loss -1.0996\n",
      "Epoch 199 Batch 350 Loss -1.1166\n",
      "Epoch 199 Loss -1.1244\n",
      "{'Epoch': 199}\n",
      "Epoch 200 Batch 0 Loss -0.3414\n",
      "Epoch 200 Batch 50 Loss -0.9848\n",
      "Epoch 200 Batch 100 Loss -0.9620\n",
      "Epoch 200 Batch 150 Loss -0.9865\n",
      "Epoch 200 Batch 200 Loss -1.0406\n",
      "Epoch 200 Batch 250 Loss -1.0775\n",
      "Epoch 200 Batch 300 Loss -1.0923\n",
      "Epoch 200 Batch 350 Loss -1.1119\n",
      "Epoch 200 Loss -1.1240\n",
      "{'Epoch': 200}\n",
      "Epoch 201 Batch 0 Loss -0.3592\n",
      "Epoch 201 Batch 50 Loss -1.0702\n",
      "Epoch 201 Batch 100 Loss -1.0393\n",
      "Epoch 201 Batch 150 Loss -1.0519\n",
      "Epoch 201 Batch 200 Loss -1.0974\n",
      "Epoch 201 Batch 250 Loss -1.1211\n",
      "Epoch 201 Batch 300 Loss -1.1251\n",
      "Epoch 201 Batch 350 Loss -1.1388\n",
      "Epoch 201 Loss -1.1480\n",
      "{'Epoch': 201}\n",
      "Epoch 202 Batch 0 Loss -0.3648\n",
      "Epoch 202 Batch 50 Loss -1.0572\n",
      "Epoch 202 Batch 100 Loss -1.0289\n",
      "Epoch 202 Batch 150 Loss -1.0431\n",
      "Epoch 202 Batch 200 Loss -1.0883\n",
      "Epoch 202 Batch 250 Loss -1.1119\n",
      "Epoch 202 Batch 300 Loss -1.1137\n",
      "Epoch 202 Batch 350 Loss -1.1281\n",
      "Epoch 202 Loss -1.1383\n",
      "{'Epoch': 202}\n",
      "Epoch 203 Batch 0 Loss -0.3601\n",
      "Epoch 203 Batch 50 Loss -1.0605\n",
      "Epoch 203 Batch 100 Loss -1.0322\n",
      "Epoch 203 Batch 150 Loss -1.0466\n",
      "Epoch 203 Batch 200 Loss -1.0921\n",
      "Epoch 203 Batch 250 Loss -1.1156\n",
      "Epoch 203 Batch 300 Loss -1.1203\n",
      "Epoch 203 Batch 350 Loss -1.1347\n",
      "Epoch 203 Loss -1.1442\n",
      "{'Epoch': 203}\n",
      "Epoch 204 Batch 0 Loss -0.3555\n",
      "Epoch 204 Batch 50 Loss -1.0583\n",
      "Epoch 204 Batch 100 Loss -1.0288\n",
      "Epoch 204 Batch 150 Loss -1.0433\n",
      "Epoch 204 Batch 200 Loss -1.0889\n",
      "Epoch 204 Batch 250 Loss -1.1126\n",
      "Epoch 204 Batch 300 Loss -1.1174\n",
      "Epoch 204 Batch 350 Loss -1.1322\n",
      "Epoch 204 Loss -1.1420\n",
      "{'Epoch': 204}\n",
      "Epoch 205 Batch 0 Loss -0.3492\n",
      "Epoch 205 Batch 50 Loss -1.0557\n",
      "Epoch 205 Batch 100 Loss -1.0284\n",
      "Epoch 205 Batch 150 Loss -1.0431\n",
      "Epoch 205 Batch 200 Loss -1.0890\n",
      "Epoch 205 Batch 250 Loss -1.1127\n",
      "Epoch 205 Batch 300 Loss -1.1180\n",
      "Epoch 205 Batch 350 Loss -1.1328\n",
      "Epoch 205 Loss -1.1425\n",
      "{'Epoch': 205}\n",
      "Epoch 206 Batch 0 Loss -0.3583\n",
      "Epoch 206 Batch 50 Loss -1.0556\n",
      "Epoch 206 Batch 100 Loss -1.0276\n",
      "Epoch 206 Batch 150 Loss -1.0427\n",
      "Epoch 206 Batch 200 Loss -1.0883\n",
      "Epoch 206 Batch 250 Loss -1.1127\n",
      "Epoch 206 Batch 300 Loss -1.1179\n",
      "Epoch 206 Batch 350 Loss -1.1323\n",
      "Epoch 206 Loss -1.1419\n",
      "{'Epoch': 206}\n",
      "Epoch 207 Batch 0 Loss -0.3656\n",
      "Epoch 207 Batch 50 Loss -1.0566\n",
      "Epoch 207 Batch 100 Loss -1.0282\n",
      "Epoch 207 Batch 150 Loss -1.0427\n",
      "Epoch 207 Batch 200 Loss -1.0890\n",
      "Epoch 207 Batch 250 Loss -1.1128\n",
      "Epoch 207 Batch 300 Loss -1.1099\n",
      "Epoch 207 Batch 350 Loss -1.1140\n",
      "Epoch 207 Loss -1.1209\n",
      "{'Epoch': 207}\n",
      "Epoch 208 Batch 0 Loss -0.2660\n",
      "Epoch 208 Batch 50 Loss -1.0126\n",
      "Epoch 208 Batch 100 Loss -0.9883\n",
      "Epoch 208 Batch 150 Loss -1.0016\n",
      "Epoch 208 Batch 200 Loss -1.0482\n",
      "Epoch 208 Batch 250 Loss -1.0688\n",
      "Epoch 208 Batch 300 Loss -1.0780\n",
      "Epoch 208 Batch 350 Loss -1.0950\n",
      "Epoch 208 Loss -1.1041\n",
      "{'Epoch': 208}\n",
      "Epoch 209 Batch 0 Loss -0.3397\n",
      "Epoch 209 Batch 50 Loss -1.0432\n",
      "Epoch 209 Batch 100 Loss -1.0018\n",
      "Epoch 209 Batch 150 Loss -1.0250\n",
      "Epoch 209 Batch 200 Loss -1.0795\n",
      "Epoch 209 Batch 250 Loss -1.1100\n",
      "Epoch 209 Batch 300 Loss -1.1197\n",
      "Epoch 209 Batch 350 Loss -1.1353\n",
      "Epoch 209 Loss -1.1452\n",
      "{'Epoch': 209}\n",
      "Epoch 210 Batch 0 Loss -0.3714\n",
      "Epoch 210 Batch 50 Loss -1.0679\n",
      "Epoch 210 Batch 100 Loss -1.0350\n",
      "Epoch 210 Batch 150 Loss -1.0481\n",
      "Epoch 210 Batch 200 Loss -1.0935\n",
      "Epoch 210 Batch 250 Loss -1.1168\n",
      "Epoch 210 Batch 300 Loss -1.1213\n",
      "Epoch 210 Batch 350 Loss -1.1354\n",
      "Epoch 210 Loss -1.1442\n",
      "{'Epoch': 210}\n",
      "Epoch 211 Batch 0 Loss -0.3573\n",
      "Epoch 211 Batch 50 Loss -1.0606\n",
      "Epoch 211 Batch 100 Loss -1.0293\n",
      "Epoch 211 Batch 150 Loss -1.0436\n",
      "Epoch 211 Batch 200 Loss -1.0892\n",
      "Epoch 211 Batch 250 Loss -1.1135\n",
      "Epoch 211 Batch 300 Loss -1.1181\n",
      "Epoch 211 Batch 350 Loss -1.1327\n",
      "Epoch 211 Loss -1.1414\n",
      "{'Epoch': 211}\n",
      "Epoch 212 Batch 0 Loss -0.3494\n",
      "Epoch 212 Batch 50 Loss -1.0605\n",
      "Epoch 212 Batch 100 Loss -1.0293\n",
      "Epoch 212 Batch 150 Loss -1.0433\n",
      "Epoch 212 Batch 200 Loss -1.0893\n",
      "Epoch 212 Batch 250 Loss -1.1139\n",
      "Epoch 212 Batch 300 Loss -1.1180\n",
      "Epoch 212 Batch 350 Loss -1.1326\n",
      "Epoch 212 Loss -1.1416\n",
      "{'Epoch': 212}\n",
      "Epoch 213 Batch 0 Loss -0.3537\n",
      "Epoch 213 Batch 50 Loss -1.0606\n",
      "Epoch 213 Batch 100 Loss -1.0288\n",
      "Epoch 213 Batch 150 Loss -1.0435\n",
      "Epoch 213 Batch 200 Loss -1.0892\n",
      "Epoch 213 Batch 250 Loss -1.1137\n",
      "Epoch 213 Batch 300 Loss -1.1179\n",
      "Epoch 213 Batch 350 Loss -1.1327\n",
      "Epoch 213 Loss -1.1417\n",
      "{'Epoch': 213}\n",
      "Epoch 214 Batch 0 Loss -0.3571\n",
      "Epoch 214 Batch 50 Loss -1.0623\n",
      "Epoch 214 Batch 100 Loss -1.0302\n",
      "Epoch 214 Batch 150 Loss -1.0447\n",
      "Epoch 214 Batch 200 Loss -1.0903\n",
      "Epoch 214 Batch 250 Loss -1.1147\n",
      "Epoch 214 Batch 300 Loss -1.1189\n",
      "Epoch 214 Batch 350 Loss -1.1334\n",
      "Epoch 214 Loss -1.1428\n",
      "{'Epoch': 214}\n",
      "Epoch 215 Batch 0 Loss -0.3626\n",
      "Epoch 215 Batch 50 Loss -1.0611\n",
      "Epoch 215 Batch 100 Loss -1.0295\n",
      "Epoch 215 Batch 150 Loss -1.0432\n",
      "Epoch 215 Batch 200 Loss -1.0884\n",
      "Epoch 215 Batch 250 Loss -1.1134\n",
      "Epoch 215 Batch 300 Loss -1.1180\n",
      "Epoch 215 Batch 350 Loss -1.1330\n",
      "Epoch 215 Loss -1.1420\n",
      "{'Epoch': 215}\n",
      "Epoch 216 Batch 0 Loss -0.3597\n",
      "Epoch 216 Batch 50 Loss -1.0601\n",
      "Epoch 216 Batch 100 Loss -1.0309\n",
      "Epoch 216 Batch 150 Loss -1.0451\n",
      "Epoch 216 Batch 200 Loss -1.0907\n",
      "Epoch 216 Batch 250 Loss -1.1146\n",
      "Epoch 216 Batch 300 Loss -1.1188\n",
      "Epoch 216 Batch 350 Loss -1.1330\n",
      "Epoch 216 Loss -1.1423\n",
      "{'Epoch': 216}\n",
      "Epoch 217 Batch 0 Loss -0.3633\n",
      "Epoch 217 Batch 50 Loss -1.0613\n",
      "Epoch 217 Batch 100 Loss -1.0309\n",
      "Epoch 217 Batch 150 Loss -1.0448\n",
      "Epoch 217 Batch 200 Loss -1.0906\n",
      "Epoch 217 Batch 250 Loss -1.1147\n",
      "Epoch 217 Batch 300 Loss -1.1189\n",
      "Epoch 217 Batch 350 Loss -1.1336\n",
      "Epoch 217 Loss -1.1428\n",
      "{'Epoch': 217}\n",
      "Epoch 218 Batch 0 Loss -0.3526\n",
      "Epoch 218 Batch 50 Loss -1.0624\n",
      "Epoch 218 Batch 100 Loss -1.0308\n",
      "Epoch 218 Batch 150 Loss -1.0450\n",
      "Epoch 218 Batch 200 Loss -1.0905\n",
      "Epoch 218 Batch 250 Loss -1.1143\n",
      "Epoch 218 Batch 300 Loss -1.1183\n",
      "Epoch 218 Batch 350 Loss -1.1329\n",
      "Epoch 218 Loss -1.1421\n",
      "{'Epoch': 218}\n",
      "Epoch 219 Batch 0 Loss -0.3620\n",
      "Epoch 219 Batch 50 Loss -1.0607\n",
      "Epoch 219 Batch 100 Loss -1.0295\n",
      "Epoch 219 Batch 150 Loss -1.0446\n",
      "Epoch 219 Batch 200 Loss -1.0903\n",
      "Epoch 219 Batch 250 Loss -1.1151\n",
      "Epoch 219 Batch 300 Loss -1.1190\n",
      "Epoch 219 Batch 350 Loss -1.1336\n",
      "Epoch 219 Loss -1.1430\n",
      "{'Epoch': 219}\n",
      "Epoch 220 Batch 0 Loss -0.3640\n",
      "Epoch 220 Batch 50 Loss -1.0620\n",
      "Epoch 220 Batch 100 Loss -1.0295\n",
      "Epoch 220 Batch 150 Loss -1.0441\n",
      "Epoch 220 Batch 200 Loss -1.0902\n",
      "Epoch 220 Batch 250 Loss -1.1145\n",
      "Epoch 220 Batch 300 Loss -1.1192\n",
      "Epoch 220 Batch 350 Loss -1.1335\n",
      "Epoch 220 Loss -1.1426\n",
      "{'Epoch': 220}\n",
      "Epoch 221 Batch 0 Loss -0.3689\n",
      "Epoch 221 Batch 50 Loss -1.0615\n",
      "Epoch 221 Batch 100 Loss -1.0115\n",
      "Epoch 221 Batch 150 Loss -1.0118\n",
      "Epoch 221 Batch 200 Loss -1.0620\n",
      "Epoch 221 Batch 250 Loss -1.0898\n",
      "Epoch 221 Batch 300 Loss -1.0981\n",
      "Epoch 221 Batch 350 Loss -1.1133\n",
      "Epoch 221 Loss -1.1234\n",
      "{'Epoch': 221}\n",
      "Epoch 222 Batch 0 Loss -0.3274\n",
      "Epoch 222 Batch 50 Loss -1.0467\n",
      "Epoch 222 Batch 100 Loss -1.0168\n",
      "Epoch 222 Batch 150 Loss -1.0237\n",
      "Epoch 222 Batch 200 Loss -1.0679\n",
      "Epoch 222 Batch 250 Loss -1.0903\n",
      "Epoch 222 Batch 300 Loss -1.0920\n",
      "Epoch 222 Batch 350 Loss -1.1062\n",
      "Epoch 222 Loss -1.1123\n",
      "{'Epoch': 222}\n",
      "Epoch 223 Batch 0 Loss -0.3696\n",
      "Epoch 223 Batch 50 Loss -1.0437\n",
      "Epoch 223 Batch 100 Loss -0.9891\n",
      "Epoch 223 Batch 150 Loss -1.0130\n",
      "Epoch 223 Batch 200 Loss -1.0681\n",
      "Epoch 223 Batch 250 Loss -1.1012\n",
      "Epoch 223 Batch 300 Loss -1.1121\n",
      "Epoch 223 Batch 350 Loss -1.1291\n",
      "Epoch 223 Loss -1.1398\n",
      "{'Epoch': 223}\n",
      "Epoch 224 Batch 0 Loss -0.3599\n",
      "Epoch 224 Batch 50 Loss -1.0751\n",
      "Epoch 224 Batch 100 Loss -1.0408\n",
      "Epoch 224 Batch 150 Loss -1.0537\n",
      "Epoch 224 Batch 200 Loss -1.0987\n",
      "Epoch 224 Batch 250 Loss -1.1210\n",
      "Epoch 224 Batch 300 Loss -1.1250\n",
      "Epoch 224 Batch 350 Loss -1.1387\n",
      "Epoch 224 Loss -1.1469\n",
      "{'Epoch': 224}\n",
      "Epoch 225 Batch 0 Loss -0.3476\n",
      "Epoch 225 Batch 50 Loss -1.0636\n",
      "Epoch 225 Batch 100 Loss -1.0334\n",
      "Epoch 225 Batch 150 Loss -1.0476\n",
      "Epoch 225 Batch 200 Loss -1.0928\n",
      "Epoch 225 Batch 250 Loss -1.1171\n",
      "Epoch 225 Batch 300 Loss -1.1206\n",
      "Epoch 225 Batch 350 Loss -1.1350\n",
      "Epoch 225 Loss -1.1439\n",
      "{'Epoch': 225}\n",
      "Epoch 226 Batch 0 Loss -0.3564\n",
      "Epoch 226 Batch 50 Loss -1.0633\n",
      "Epoch 226 Batch 100 Loss -1.0327\n",
      "Epoch 226 Batch 150 Loss -1.0469\n",
      "Epoch 226 Batch 200 Loss -1.0923\n",
      "Epoch 226 Batch 250 Loss -1.1165\n",
      "Epoch 226 Batch 300 Loss -1.1205\n",
      "Epoch 226 Batch 350 Loss -1.1348\n",
      "Epoch 226 Loss -1.1434\n",
      "{'Epoch': 226}\n",
      "Epoch 227 Batch 0 Loss -0.3493\n",
      "Epoch 227 Batch 50 Loss -1.0589\n",
      "Epoch 227 Batch 100 Loss -1.0311\n",
      "Epoch 227 Batch 150 Loss -1.0455\n",
      "Epoch 227 Batch 200 Loss -1.0915\n",
      "Epoch 227 Batch 250 Loss -1.1161\n",
      "Epoch 227 Batch 300 Loss -1.1205\n",
      "Epoch 227 Batch 350 Loss -1.1348\n",
      "Epoch 227 Loss -1.1439\n",
      "{'Epoch': 227}\n",
      "Epoch 228 Batch 0 Loss -0.3555\n",
      "Epoch 228 Batch 50 Loss -1.0652\n",
      "Epoch 228 Batch 100 Loss -1.0330\n",
      "Epoch 228 Batch 150 Loss -1.0466\n",
      "Epoch 228 Batch 200 Loss -1.0921\n",
      "Epoch 228 Batch 250 Loss -1.1158\n",
      "Epoch 228 Batch 300 Loss -1.1204\n",
      "Epoch 228 Batch 350 Loss -1.1350\n",
      "Epoch 228 Loss -1.1443\n",
      "{'Epoch': 228}\n",
      "Epoch 229 Batch 0 Loss -0.3590\n",
      "Epoch 229 Batch 50 Loss -1.0641\n",
      "Epoch 229 Batch 100 Loss -1.0327\n",
      "Epoch 229 Batch 150 Loss -1.0464\n",
      "Epoch 229 Batch 200 Loss -1.0918\n",
      "Epoch 229 Batch 250 Loss -1.1161\n",
      "Epoch 229 Batch 300 Loss -1.1203\n",
      "Epoch 229 Batch 350 Loss -1.1351\n",
      "Epoch 229 Loss -1.1445\n",
      "{'Epoch': 229}\n",
      "Epoch 230 Batch 0 Loss -0.3689\n",
      "Epoch 230 Batch 50 Loss -1.0653\n",
      "Epoch 230 Batch 100 Loss -1.0319\n",
      "Epoch 230 Batch 150 Loss -1.0460\n",
      "Epoch 230 Batch 200 Loss -1.0921\n",
      "Epoch 230 Batch 250 Loss -1.1169\n",
      "Epoch 230 Batch 300 Loss -1.1213\n",
      "Epoch 230 Batch 350 Loss -1.1357\n",
      "Epoch 230 Loss -1.1450\n",
      "{'Epoch': 230}\n",
      "Epoch 231 Batch 0 Loss -0.3693\n",
      "Epoch 231 Batch 50 Loss -1.0614\n",
      "Epoch 231 Batch 100 Loss -1.0293\n",
      "Epoch 231 Batch 150 Loss -1.0445\n",
      "Epoch 231 Batch 200 Loss -1.0912\n",
      "Epoch 231 Batch 250 Loss -1.1158\n",
      "Epoch 231 Batch 300 Loss -1.1197\n",
      "Epoch 231 Batch 350 Loss -1.1344\n",
      "Epoch 231 Loss -1.1438\n",
      "{'Epoch': 231}\n",
      "Epoch 232 Batch 0 Loss -0.3673\n",
      "Epoch 232 Batch 50 Loss -1.0642\n",
      "Epoch 232 Batch 100 Loss -1.0335\n",
      "Epoch 232 Batch 150 Loss -1.0474\n",
      "Epoch 232 Batch 200 Loss -1.0928\n",
      "Epoch 232 Batch 250 Loss -1.1167\n",
      "Epoch 232 Batch 300 Loss -1.1210\n",
      "Epoch 232 Batch 350 Loss -1.1350\n",
      "Epoch 232 Loss -1.1445\n",
      "{'Epoch': 232}\n",
      "Epoch 233 Batch 0 Loss -0.3683\n",
      "Epoch 233 Batch 50 Loss -1.0666\n",
      "Epoch 233 Batch 100 Loss -1.0336\n",
      "Epoch 233 Batch 150 Loss -1.0477\n",
      "Epoch 233 Batch 200 Loss -1.0932\n",
      "Epoch 233 Batch 250 Loss -1.1175\n",
      "Epoch 233 Batch 300 Loss -1.1222\n",
      "Epoch 233 Batch 350 Loss -1.1367\n",
      "Epoch 233 Loss -1.1459\n",
      "{'Epoch': 233}\n",
      "Epoch 234 Batch 0 Loss -0.3651\n",
      "Epoch 234 Batch 50 Loss -1.0657\n",
      "Epoch 234 Batch 100 Loss -1.0342\n",
      "Epoch 234 Batch 150 Loss -1.0478\n",
      "Epoch 234 Batch 200 Loss -1.0931\n",
      "Epoch 234 Batch 250 Loss -1.1173\n",
      "Epoch 234 Batch 300 Loss -1.1213\n",
      "Epoch 234 Batch 350 Loss -1.1350\n",
      "Epoch 234 Loss -1.1445\n",
      "{'Epoch': 234}\n",
      "Epoch 235 Batch 0 Loss -0.3745\n",
      "Epoch 235 Batch 50 Loss -1.0665\n",
      "Epoch 235 Batch 100 Loss -1.0335\n",
      "Epoch 235 Batch 150 Loss -1.0476\n",
      "Epoch 235 Batch 200 Loss -1.0936\n",
      "Epoch 235 Batch 250 Loss -1.1182\n",
      "Epoch 235 Batch 300 Loss -1.1229\n",
      "Epoch 235 Batch 350 Loss -1.1374\n",
      "Epoch 235 Loss -1.1465\n",
      "{'Epoch': 235}\n",
      "Epoch 236 Batch 0 Loss -0.3676\n",
      "Epoch 236 Batch 50 Loss -1.0643\n",
      "Epoch 236 Batch 100 Loss -1.0337\n",
      "Epoch 236 Batch 150 Loss -1.0472\n",
      "Epoch 236 Batch 200 Loss -1.0926\n",
      "Epoch 236 Batch 250 Loss -1.1169\n",
      "Epoch 236 Batch 300 Loss -1.1200\n",
      "Epoch 236 Batch 350 Loss -1.1296\n",
      "Epoch 236 Loss -1.1338\n",
      "{'Epoch': 236}\n",
      "Epoch 237 Batch 0 Loss -0.2904\n",
      "Epoch 237 Batch 50 Loss -1.0020\n",
      "Epoch 237 Batch 100 Loss -0.9913\n",
      "Epoch 237 Batch 150 Loss -1.0138\n",
      "Epoch 237 Batch 200 Loss -1.0642\n",
      "Epoch 237 Batch 250 Loss -1.0919\n",
      "Epoch 237 Batch 300 Loss -1.0992\n",
      "Epoch 237 Batch 350 Loss -1.1142\n",
      "Epoch 237 Loss -1.1239\n",
      "{'Epoch': 237}\n",
      "Epoch 238 Batch 0 Loss -0.3319\n",
      "Epoch 238 Batch 50 Loss -1.0449\n",
      "Epoch 238 Batch 100 Loss -1.0144\n",
      "Epoch 238 Batch 150 Loss -1.0224\n",
      "Epoch 238 Batch 200 Loss -1.0688\n",
      "Epoch 238 Batch 250 Loss -1.0930\n",
      "Epoch 238 Batch 300 Loss -1.0996\n",
      "Epoch 238 Batch 350 Loss -1.1147\n",
      "Epoch 238 Loss -1.1245\n",
      "{'Epoch': 238}\n",
      "Epoch 239 Batch 0 Loss -0.3424\n",
      "Epoch 239 Batch 50 Loss -1.0467\n",
      "Epoch 239 Batch 100 Loss -1.0118\n",
      "Epoch 239 Batch 150 Loss -1.0220\n",
      "Epoch 239 Batch 200 Loss -1.0694\n",
      "Epoch 239 Batch 250 Loss -1.0932\n",
      "Epoch 239 Batch 300 Loss -1.0953\n",
      "Epoch 239 Batch 350 Loss -1.1102\n",
      "Epoch 239 Loss -1.1201\n",
      "{'Epoch': 239}\n",
      "Epoch 240 Batch 0 Loss -0.2903\n",
      "Epoch 240 Batch 50 Loss -1.0000\n",
      "Epoch 240 Batch 100 Loss -0.9803\n",
      "Epoch 240 Batch 150 Loss -0.9993\n",
      "Epoch 240 Batch 200 Loss -1.0483\n",
      "Epoch 240 Batch 250 Loss -1.0826\n",
      "Epoch 240 Batch 300 Loss -1.0971\n",
      "Epoch 240 Batch 350 Loss -1.1169\n",
      "Epoch 240 Loss -1.1290\n",
      "{'Epoch': 240}\n",
      "Epoch 241 Batch 0 Loss -0.3650\n",
      "Epoch 241 Batch 50 Loss -1.0739\n",
      "Epoch 241 Batch 100 Loss -1.0451\n",
      "Epoch 241 Batch 150 Loss -1.0581\n",
      "Epoch 241 Batch 200 Loss -1.1033\n",
      "Epoch 241 Batch 250 Loss -1.1257\n",
      "Epoch 241 Batch 300 Loss -1.1289\n",
      "Epoch 241 Batch 350 Loss -1.1427\n",
      "Epoch 241 Loss -1.1520\n",
      "{'Epoch': 241}\n",
      "Epoch 242 Batch 0 Loss -0.3649\n",
      "Epoch 242 Batch 50 Loss -1.0646\n",
      "Epoch 242 Batch 100 Loss -1.0362\n",
      "Epoch 242 Batch 150 Loss -1.0505\n",
      "Epoch 242 Batch 200 Loss -1.0958\n",
      "Epoch 242 Batch 250 Loss -1.1192\n",
      "Epoch 242 Batch 300 Loss -1.1188\n",
      "Epoch 242 Batch 350 Loss -1.1291\n",
      "Epoch 242 Loss -1.1380\n",
      "{'Epoch': 242}\n",
      "Epoch 243 Batch 0 Loss -0.3344\n",
      "Epoch 243 Batch 50 Loss -1.0514\n",
      "Epoch 243 Batch 100 Loss -1.0184\n",
      "Epoch 243 Batch 150 Loss -1.0296\n",
      "Epoch 243 Batch 200 Loss -1.0750\n",
      "Epoch 243 Batch 250 Loss -1.0947\n",
      "Epoch 243 Batch 300 Loss -1.0993\n",
      "Epoch 243 Batch 350 Loss -1.1136\n",
      "Epoch 243 Loss -1.1223\n",
      "{'Epoch': 243}\n",
      "Epoch 244 Batch 0 Loss -0.3272\n",
      "Epoch 244 Batch 50 Loss -1.0464\n",
      "Epoch 244 Batch 100 Loss -1.0151\n",
      "Epoch 244 Batch 150 Loss -1.0297\n",
      "Epoch 244 Batch 200 Loss -1.0748\n",
      "Epoch 244 Batch 250 Loss -1.1001\n",
      "Epoch 244 Batch 300 Loss -1.1035\n",
      "Epoch 244 Batch 350 Loss -1.1169\n",
      "Epoch 244 Loss -1.1260\n",
      "{'Epoch': 244}\n",
      "Epoch 245 Batch 0 Loss -0.3484\n",
      "Epoch 245 Batch 50 Loss -1.0497\n",
      "Epoch 245 Batch 100 Loss -1.0180\n",
      "Epoch 245 Batch 150 Loss -1.0316\n",
      "Epoch 245 Batch 200 Loss -1.0764\n",
      "Epoch 245 Batch 250 Loss -1.1004\n",
      "Epoch 245 Batch 300 Loss -1.1062\n",
      "Epoch 245 Batch 350 Loss -1.1203\n",
      "Epoch 245 Loss -1.1296\n",
      "{'Epoch': 245}\n",
      "Epoch 246 Batch 0 Loss -0.3299\n",
      "Epoch 246 Batch 50 Loss -1.0461\n",
      "Epoch 246 Batch 100 Loss -1.0158\n",
      "Epoch 246 Batch 150 Loss -1.0244\n",
      "Epoch 246 Batch 200 Loss -1.0708\n",
      "Epoch 246 Batch 250 Loss -1.0959\n",
      "Epoch 246 Batch 300 Loss -1.0954\n",
      "Epoch 246 Batch 350 Loss -1.1110\n",
      "Epoch 246 Loss -1.1214\n",
      "{'Epoch': 246}\n",
      "Epoch 247 Batch 0 Loss -0.3373\n",
      "Epoch 247 Batch 50 Loss -1.0522\n",
      "Epoch 247 Batch 100 Loss -1.0172\n",
      "Epoch 247 Batch 150 Loss -1.0285\n",
      "Epoch 247 Batch 200 Loss -1.0750\n",
      "Epoch 247 Batch 250 Loss -1.0938\n",
      "Epoch 247 Batch 300 Loss -1.0949\n",
      "Epoch 247 Batch 350 Loss -1.1098\n",
      "Epoch 247 Loss -1.1192\n",
      "{'Epoch': 247}\n",
      "Epoch 248 Batch 0 Loss -0.3341\n",
      "Epoch 248 Batch 50 Loss -1.0524\n",
      "Epoch 248 Batch 100 Loss -1.0173\n",
      "Epoch 248 Batch 150 Loss -1.0313\n",
      "Epoch 248 Batch 200 Loss -1.0772\n",
      "Epoch 248 Batch 250 Loss -1.1027\n",
      "Epoch 248 Batch 300 Loss -1.1062\n",
      "Epoch 248 Batch 350 Loss -1.1198\n",
      "Epoch 248 Loss -1.1281\n",
      "{'Epoch': 248}\n",
      "Epoch 249 Batch 0 Loss -0.3499\n",
      "Epoch 249 Batch 50 Loss -1.0469\n",
      "Epoch 249 Batch 100 Loss -1.0234\n",
      "Epoch 249 Batch 150 Loss -1.0414\n",
      "Epoch 249 Batch 200 Loss -1.0906\n",
      "Epoch 249 Batch 250 Loss -1.1149\n",
      "Epoch 249 Batch 300 Loss -1.1217\n",
      "Epoch 249 Batch 350 Loss -1.1367\n",
      "Epoch 249 Loss -1.1453\n",
      "{'Epoch': 249}\n",
      "Epoch 250 Batch 0 Loss -0.3378\n",
      "Epoch 250 Batch 50 Loss -1.0709\n",
      "Epoch 250 Batch 100 Loss -1.0401\n",
      "Epoch 250 Batch 150 Loss -1.0527\n",
      "Epoch 250 Batch 200 Loss -1.0982\n",
      "Epoch 250 Batch 250 Loss -1.1193\n",
      "Epoch 250 Batch 300 Loss -1.1245\n",
      "Epoch 250 Batch 350 Loss -1.1388\n",
      "Epoch 250 Loss -1.1478\n",
      "{'Epoch': 250}\n",
      "Epoch 251 Batch 0 Loss -0.3239\n",
      "Epoch 251 Batch 50 Loss -1.0652\n",
      "Epoch 251 Batch 100 Loss -1.0365\n",
      "Epoch 251 Batch 150 Loss -1.0498\n",
      "Epoch 251 Batch 200 Loss -1.0960\n",
      "Epoch 251 Batch 250 Loss -1.1186\n",
      "Epoch 251 Batch 300 Loss -1.1233\n",
      "Epoch 251 Batch 350 Loss -1.1375\n",
      "Epoch 251 Loss -1.1464\n",
      "{'Epoch': 251}\n",
      "Epoch 252 Batch 0 Loss -0.3217\n",
      "Epoch 252 Batch 50 Loss -1.0667\n",
      "Epoch 252 Batch 100 Loss -1.0357\n",
      "Epoch 252 Batch 150 Loss -1.0493\n",
      "Epoch 252 Batch 200 Loss -1.0953\n",
      "Epoch 252 Batch 250 Loss -1.1195\n",
      "Epoch 252 Batch 300 Loss -1.1247\n",
      "Epoch 252 Batch 350 Loss -1.1389\n",
      "Epoch 252 Loss -1.1478\n",
      "{'Epoch': 252}\n",
      "Epoch 253 Batch 0 Loss -0.3317\n",
      "Epoch 253 Batch 50 Loss -1.0705\n",
      "Epoch 253 Batch 100 Loss -1.0405\n",
      "Epoch 253 Batch 150 Loss -1.0528\n",
      "Epoch 253 Batch 200 Loss -1.0980\n",
      "Epoch 253 Batch 250 Loss -1.1191\n",
      "Epoch 253 Batch 300 Loss -1.1239\n",
      "Epoch 253 Batch 350 Loss -1.1385\n",
      "Epoch 253 Loss -1.1476\n",
      "{'Epoch': 253}\n",
      "Epoch 254 Batch 0 Loss -0.3320\n",
      "Epoch 254 Batch 50 Loss -1.0679\n",
      "Epoch 254 Batch 100 Loss -1.0385\n",
      "Epoch 254 Batch 150 Loss -1.0512\n",
      "Epoch 254 Batch 200 Loss -1.0974\n",
      "Epoch 254 Batch 250 Loss -1.1191\n",
      "Epoch 254 Batch 300 Loss -1.1242\n",
      "Epoch 254 Batch 350 Loss -1.1386\n",
      "Epoch 254 Loss -1.1468\n",
      "{'Epoch': 254}\n",
      "Epoch 255 Batch 0 Loss -0.3314\n",
      "Epoch 255 Batch 50 Loss -1.0683\n",
      "Epoch 255 Batch 100 Loss -1.0395\n",
      "Epoch 255 Batch 150 Loss -1.0525\n",
      "Epoch 255 Batch 200 Loss -1.0982\n",
      "Epoch 255 Batch 250 Loss -1.1198\n",
      "Epoch 255 Batch 300 Loss -1.1245\n",
      "Epoch 255 Batch 350 Loss -1.1388\n",
      "Epoch 255 Loss -1.1472\n",
      "{'Epoch': 255}\n",
      "Epoch 256 Batch 0 Loss -0.3285\n",
      "Epoch 256 Batch 50 Loss -1.0707\n",
      "Epoch 256 Batch 100 Loss -1.0402\n",
      "Epoch 256 Batch 150 Loss -1.0525\n",
      "Epoch 256 Batch 200 Loss -1.0985\n",
      "Epoch 256 Batch 250 Loss -1.1209\n",
      "Epoch 256 Batch 300 Loss -1.1253\n",
      "Epoch 256 Batch 350 Loss -1.1394\n",
      "Epoch 256 Loss -1.1476\n",
      "{'Epoch': 256}\n",
      "Epoch 257 Batch 0 Loss -0.3212\n",
      "Epoch 257 Batch 50 Loss -1.0695\n",
      "Epoch 257 Batch 100 Loss -1.0396\n",
      "Epoch 257 Batch 150 Loss -1.0526\n",
      "Epoch 257 Batch 200 Loss -1.0981\n",
      "Epoch 257 Batch 250 Loss -1.1194\n",
      "Epoch 257 Batch 300 Loss -1.1241\n",
      "Epoch 257 Batch 350 Loss -1.1387\n",
      "Epoch 257 Loss -1.1422\n",
      "{'Epoch': 257}\n",
      "Epoch 258 Batch 0 Loss -0.3267\n",
      "Epoch 258 Batch 50 Loss -1.0442\n",
      "Epoch 258 Batch 100 Loss -1.0204\n",
      "Epoch 258 Batch 150 Loss -1.0417\n",
      "Epoch 258 Batch 200 Loss -1.0924\n",
      "Epoch 258 Batch 250 Loss -1.1206\n",
      "Epoch 258 Batch 300 Loss -1.1279\n",
      "Epoch 258 Batch 350 Loss -1.1433\n",
      "Epoch 258 Loss -1.1531\n",
      "{'Epoch': 258}\n",
      "Epoch 259 Batch 0 Loss -0.3924\n",
      "Epoch 259 Batch 50 Loss -1.0743\n",
      "Epoch 259 Batch 100 Loss -1.0388\n",
      "Epoch 259 Batch 150 Loss -1.0523\n",
      "Epoch 259 Batch 200 Loss -1.0981\n",
      "Epoch 259 Batch 250 Loss -1.1226\n",
      "Epoch 259 Batch 300 Loss -1.1250\n",
      "Epoch 259 Batch 350 Loss -1.1316\n",
      "Epoch 259 Loss -1.1382\n",
      "{'Epoch': 259}\n",
      "Epoch 260 Batch 0 Loss -0.3568\n",
      "Epoch 260 Batch 50 Loss -1.0676\n",
      "Epoch 260 Batch 100 Loss -1.0418\n",
      "Epoch 260 Batch 150 Loss -1.0574\n",
      "Epoch 260 Batch 200 Loss -1.1049\n",
      "Epoch 260 Batch 250 Loss -1.1294\n",
      "Epoch 260 Batch 300 Loss -1.1338\n",
      "Epoch 260 Batch 350 Loss -1.1477\n",
      "Epoch 260 Loss -1.1566\n",
      "{'Epoch': 260}\n",
      "Epoch 261 Batch 0 Loss -0.3711\n",
      "Epoch 261 Batch 50 Loss -1.0668\n",
      "Epoch 261 Batch 100 Loss -1.0382\n",
      "Epoch 261 Batch 150 Loss -1.0530\n",
      "Epoch 261 Batch 200 Loss -1.0985\n",
      "Epoch 261 Batch 250 Loss -1.1226\n",
      "Epoch 261 Batch 300 Loss -1.1278\n",
      "Epoch 261 Batch 350 Loss -1.1429\n",
      "Epoch 261 Loss -1.1523\n",
      "{'Epoch': 261}\n",
      "Epoch 262 Batch 0 Loss -0.3803\n",
      "Epoch 262 Batch 50 Loss -1.0669\n",
      "Epoch 262 Batch 100 Loss -1.0381\n",
      "Epoch 262 Batch 150 Loss -1.0529\n",
      "Epoch 262 Batch 200 Loss -1.0987\n",
      "Epoch 262 Batch 250 Loss -1.1229\n",
      "Epoch 262 Batch 300 Loss -1.1249\n",
      "Epoch 262 Batch 350 Loss -1.1330\n",
      "Epoch 262 Loss -1.1368\n",
      "{'Epoch': 262}\n",
      "Epoch 263 Batch 0 Loss -0.3263\n",
      "Epoch 263 Batch 50 Loss -1.0353\n",
      "Epoch 263 Batch 100 Loss -1.0213\n",
      "Epoch 263 Batch 150 Loss -1.0437\n",
      "Epoch 263 Batch 200 Loss -1.0954\n",
      "Epoch 263 Batch 250 Loss -1.1242\n",
      "Epoch 263 Batch 300 Loss -1.1313\n",
      "Epoch 263 Batch 350 Loss -1.1463\n",
      "Epoch 263 Loss -1.1561\n",
      "{'Epoch': 263}\n",
      "Epoch 264 Batch 0 Loss -0.3766\n",
      "Epoch 264 Batch 50 Loss -1.0701\n",
      "Epoch 264 Batch 100 Loss -1.0401\n",
      "Epoch 264 Batch 150 Loss -1.0543\n",
      "Epoch 264 Batch 200 Loss -1.0998\n",
      "Epoch 264 Batch 250 Loss -1.1230\n",
      "Epoch 264 Batch 300 Loss -1.1283\n",
      "Epoch 264 Batch 350 Loss -1.1431\n",
      "Epoch 264 Loss -1.1527\n",
      "{'Epoch': 264}\n",
      "Epoch 265 Batch 0 Loss -0.3798\n",
      "Epoch 265 Batch 50 Loss -1.0668\n",
      "Epoch 265 Batch 100 Loss -1.0376\n",
      "Epoch 265 Batch 150 Loss -1.0523\n",
      "Epoch 265 Batch 200 Loss -1.0986\n",
      "Epoch 265 Batch 250 Loss -1.1225\n",
      "Epoch 265 Batch 300 Loss -1.1214\n",
      "Epoch 265 Batch 350 Loss -1.1348\n",
      "Epoch 265 Loss -1.1451\n",
      "{'Epoch': 265}\n",
      "Epoch 266 Batch 0 Loss -0.3482\n",
      "Epoch 266 Batch 50 Loss -1.0750\n",
      "Epoch 266 Batch 100 Loss -1.0426\n",
      "Epoch 266 Batch 150 Loss -1.0578\n",
      "Epoch 266 Batch 200 Loss -1.1047\n",
      "Epoch 266 Batch 250 Loss -1.1291\n",
      "Epoch 266 Batch 300 Loss -1.1335\n",
      "Epoch 266 Batch 350 Loss -1.1420\n",
      "Epoch 266 Loss -1.1512\n",
      "{'Epoch': 266}\n",
      "Epoch 267 Batch 0 Loss -0.3675\n",
      "Epoch 267 Batch 50 Loss -1.0735\n",
      "Epoch 267 Batch 100 Loss -1.0450\n",
      "Epoch 267 Batch 150 Loss -1.0597\n",
      "Epoch 267 Batch 200 Loss -1.1062\n",
      "Epoch 267 Batch 250 Loss -1.1305\n",
      "Epoch 267 Batch 300 Loss -1.1296\n",
      "Epoch 267 Batch 350 Loss -1.1366\n",
      "Epoch 267 Loss -1.1449\n",
      "{'Epoch': 267}\n",
      "Epoch 268 Batch 0 Loss -0.1785\n",
      "Epoch 268 Batch 50 Loss -1.0036\n",
      "Epoch 268 Batch 100 Loss -0.9746\n",
      "Epoch 268 Batch 150 Loss -1.0010\n",
      "Epoch 268 Batch 200 Loss -1.0526\n",
      "Epoch 268 Batch 250 Loss -1.0798\n",
      "Epoch 268 Batch 300 Loss -1.0852\n",
      "Epoch 268 Batch 350 Loss -1.1030\n",
      "Epoch 268 Loss -1.1110\n",
      "{'Epoch': 268}\n",
      "Epoch 269 Batch 0 Loss -0.3400\n",
      "Epoch 269 Batch 50 Loss -1.0403\n",
      "Epoch 269 Batch 100 Loss -1.0157\n",
      "Epoch 269 Batch 150 Loss -1.0332\n",
      "Epoch 269 Batch 200 Loss -1.0812\n",
      "Epoch 269 Batch 250 Loss -1.1074\n",
      "Epoch 269 Batch 300 Loss -1.1134\n",
      "Epoch 269 Batch 350 Loss -1.1278\n",
      "Epoch 269 Loss -1.1370\n",
      "{'Epoch': 269}\n",
      "Epoch 270 Batch 0 Loss -0.3266\n",
      "Epoch 270 Batch 50 Loss -1.0469\n",
      "Epoch 270 Batch 100 Loss -1.0173\n",
      "Epoch 270 Batch 150 Loss -1.0272\n",
      "Epoch 270 Batch 200 Loss -1.0740\n",
      "Epoch 270 Batch 250 Loss -1.0965\n",
      "Epoch 270 Batch 300 Loss -1.1041\n",
      "Epoch 270 Batch 350 Loss -1.1196\n",
      "Epoch 270 Loss -1.1294\n",
      "{'Epoch': 270}\n",
      "Epoch 271 Batch 0 Loss -0.3395\n",
      "Epoch 271 Batch 50 Loss -1.0533\n",
      "Epoch 271 Batch 100 Loss -1.0188\n",
      "Epoch 271 Batch 150 Loss -1.0308\n",
      "Epoch 271 Batch 200 Loss -1.0771\n",
      "Epoch 271 Batch 250 Loss -1.1016\n",
      "Epoch 271 Batch 300 Loss -1.1054\n",
      "Epoch 271 Batch 350 Loss -1.1197\n",
      "Epoch 271 Loss -1.1283\n",
      "{'Epoch': 271}\n",
      "Epoch 272 Batch 0 Loss -0.3438\n",
      "Epoch 272 Batch 50 Loss -1.0496\n",
      "Epoch 272 Batch 100 Loss -1.0177\n",
      "Epoch 272 Batch 150 Loss -1.0330\n",
      "Epoch 272 Batch 200 Loss -1.0786\n",
      "Epoch 272 Batch 250 Loss -1.1038\n",
      "Epoch 272 Batch 300 Loss -1.1078\n",
      "Epoch 272 Batch 350 Loss -1.1220\n",
      "Epoch 272 Loss -1.1304\n",
      "{'Epoch': 272}\n",
      "Epoch 273 Batch 0 Loss -0.3370\n",
      "Epoch 273 Batch 50 Loss -1.0487\n",
      "Epoch 273 Batch 100 Loss -1.0184\n",
      "Epoch 273 Batch 150 Loss -1.0336\n",
      "Epoch 273 Batch 200 Loss -1.0799\n",
      "Epoch 273 Batch 250 Loss -1.1036\n",
      "Epoch 273 Batch 300 Loss -1.1101\n",
      "Epoch 273 Batch 350 Loss -1.1239\n",
      "Epoch 273 Loss -1.1331\n",
      "{'Epoch': 273}\n",
      "Epoch 274 Batch 0 Loss -0.3350\n",
      "Epoch 274 Batch 50 Loss -1.0498\n",
      "Epoch 274 Batch 100 Loss -1.0196\n",
      "Epoch 274 Batch 150 Loss -1.0296\n",
      "Epoch 274 Batch 200 Loss -1.0765\n",
      "Epoch 274 Batch 250 Loss -1.1010\n",
      "Epoch 274 Batch 300 Loss -1.1039\n",
      "Epoch 274 Batch 350 Loss -1.1196\n",
      "Epoch 274 Loss -1.1298\n",
      "{'Epoch': 274}\n",
      "Epoch 275 Batch 0 Loss -0.3492\n",
      "Epoch 275 Batch 50 Loss -1.0572\n",
      "Epoch 275 Batch 100 Loss -1.0227\n",
      "Epoch 275 Batch 150 Loss -1.0342\n",
      "Epoch 275 Batch 200 Loss -1.0806\n",
      "Epoch 275 Batch 250 Loss -1.1056\n",
      "Epoch 275 Batch 300 Loss -1.1080\n",
      "Epoch 275 Batch 350 Loss -1.1217\n",
      "Epoch 275 Loss -1.1302\n",
      "{'Epoch': 275}\n",
      "Epoch 276 Batch 0 Loss -0.3420\n",
      "Epoch 276 Batch 50 Loss -1.0522\n",
      "Epoch 276 Batch 100 Loss -1.0211\n",
      "Epoch 276 Batch 150 Loss -1.0354\n",
      "Epoch 276 Batch 200 Loss -1.0811\n",
      "Epoch 276 Batch 250 Loss -1.1054\n",
      "Epoch 276 Batch 300 Loss -1.1088\n",
      "Epoch 276 Batch 350 Loss -1.1225\n",
      "Epoch 276 Loss -1.1317\n",
      "{'Epoch': 276}\n",
      "Epoch 277 Batch 0 Loss -0.3570\n",
      "Epoch 277 Batch 50 Loss -1.0543\n",
      "Epoch 277 Batch 100 Loss -1.0207\n",
      "Epoch 277 Batch 150 Loss -1.0351\n",
      "Epoch 277 Batch 200 Loss -1.0808\n",
      "Epoch 277 Batch 250 Loss -1.1054\n",
      "Epoch 277 Batch 300 Loss -1.1113\n",
      "Epoch 277 Batch 350 Loss -1.1254\n",
      "Epoch 277 Loss -1.1348\n",
      "{'Epoch': 277}\n",
      "Epoch 278 Batch 0 Loss -0.3297\n",
      "Epoch 278 Batch 50 Loss -1.0512\n",
      "Epoch 278 Batch 100 Loss -1.0202\n",
      "Epoch 278 Batch 150 Loss -1.0311\n",
      "Epoch 278 Batch 200 Loss -1.0756\n",
      "Epoch 278 Batch 250 Loss -1.1040\n",
      "Epoch 278 Batch 300 Loss -1.1113\n",
      "Epoch 278 Batch 350 Loss -1.1256\n",
      "Epoch 278 Loss -1.1346\n",
      "{'Epoch': 278}\n",
      "Epoch 279 Batch 0 Loss -0.3567\n",
      "Epoch 279 Batch 50 Loss -1.0589\n",
      "Epoch 279 Batch 100 Loss -1.0251\n",
      "Epoch 279 Batch 150 Loss -1.0395\n",
      "Epoch 279 Batch 200 Loss -1.0841\n",
      "Epoch 279 Batch 250 Loss -1.1084\n",
      "Epoch 279 Batch 300 Loss -1.1123\n",
      "Epoch 279 Batch 350 Loss -1.1256\n",
      "Epoch 279 Loss -1.1341\n",
      "{'Epoch': 279}\n",
      "Epoch 280 Batch 0 Loss -0.3375\n",
      "Epoch 280 Batch 50 Loss -1.0493\n",
      "Epoch 280 Batch 100 Loss -1.0130\n",
      "Epoch 280 Batch 150 Loss -1.0286\n",
      "Epoch 280 Batch 200 Loss -1.0732\n",
      "Epoch 280 Batch 250 Loss -1.0995\n",
      "Epoch 280 Batch 300 Loss -1.1115\n",
      "Epoch 280 Batch 350 Loss -1.1299\n",
      "Epoch 280 Loss -1.1416\n",
      "{'Epoch': 280}\n",
      "Epoch 281 Batch 0 Loss -0.3672\n",
      "Epoch 281 Batch 50 Loss -1.0797\n",
      "Epoch 281 Batch 100 Loss -1.0502\n",
      "Epoch 281 Batch 150 Loss -1.0647\n",
      "Epoch 281 Batch 200 Loss -1.1103\n",
      "Epoch 281 Batch 250 Loss -1.1338\n",
      "Epoch 281 Batch 300 Loss -1.1379\n",
      "Epoch 281 Batch 350 Loss -1.1514\n",
      "Epoch 281 Loss -1.1606\n",
      "{'Epoch': 281}\n",
      "Epoch 282 Batch 0 Loss -0.3780\n",
      "Epoch 282 Batch 50 Loss -1.0709\n",
      "Epoch 282 Batch 100 Loss -1.0422\n",
      "Epoch 282 Batch 150 Loss -1.0579\n",
      "Epoch 282 Batch 200 Loss -1.1035\n",
      "Epoch 282 Batch 250 Loss -1.1273\n",
      "Epoch 282 Batch 300 Loss -1.1298\n",
      "Epoch 282 Batch 350 Loss -1.1440\n",
      "Epoch 282 Loss -1.1538\n",
      "{'Epoch': 282}\n",
      "Epoch 283 Batch 0 Loss -0.3688\n",
      "Epoch 283 Batch 50 Loss -1.0708\n",
      "Epoch 283 Batch 100 Loss -1.0419\n",
      "Epoch 283 Batch 150 Loss -1.0574\n",
      "Epoch 283 Batch 200 Loss -1.1033\n",
      "Epoch 283 Batch 250 Loss -1.1266\n",
      "Epoch 283 Batch 300 Loss -1.1322\n",
      "Epoch 283 Batch 350 Loss -1.1467\n",
      "Epoch 283 Loss -1.1561\n",
      "{'Epoch': 283}\n",
      "Epoch 284 Batch 0 Loss -0.3761\n",
      "Epoch 284 Batch 50 Loss -1.0693\n",
      "Epoch 284 Batch 100 Loss -1.0410\n",
      "Epoch 284 Batch 150 Loss -1.0566\n",
      "Epoch 284 Batch 200 Loss -1.1022\n",
      "Epoch 284 Batch 250 Loss -1.1264\n",
      "Epoch 284 Batch 300 Loss -1.1305\n",
      "Epoch 284 Batch 350 Loss -1.1450\n",
      "Epoch 284 Loss -1.1544\n",
      "{'Epoch': 284}\n",
      "Epoch 285 Batch 0 Loss -0.3796\n",
      "Epoch 285 Batch 50 Loss -1.0721\n",
      "Epoch 285 Batch 100 Loss -1.0423\n",
      "Epoch 285 Batch 150 Loss -1.0578\n",
      "Epoch 285 Batch 200 Loss -1.1034\n",
      "Epoch 285 Batch 250 Loss -1.1273\n",
      "Epoch 285 Batch 300 Loss -1.1326\n",
      "Epoch 285 Batch 350 Loss -1.1470\n",
      "Epoch 285 Loss -1.1563\n",
      "{'Epoch': 285}\n",
      "Epoch 286 Batch 0 Loss -0.3804\n",
      "Epoch 286 Batch 50 Loss -1.0698\n",
      "Epoch 286 Batch 100 Loss -1.0414\n",
      "Epoch 286 Batch 150 Loss -1.0572\n",
      "Epoch 286 Batch 200 Loss -1.1032\n",
      "Epoch 286 Batch 250 Loss -1.1270\n",
      "Epoch 286 Batch 300 Loss -1.1222\n",
      "Epoch 286 Batch 350 Loss -1.1337\n",
      "Epoch 286 Loss -1.1442\n",
      "{'Epoch': 286}\n",
      "Epoch 287 Batch 0 Loss -0.3647\n",
      "Epoch 287 Batch 50 Loss -1.0743\n",
      "Epoch 287 Batch 100 Loss -1.0456\n",
      "Epoch 287 Batch 150 Loss -1.0606\n",
      "Epoch 287 Batch 200 Loss -1.1075\n",
      "Epoch 287 Batch 250 Loss -1.1322\n",
      "Epoch 287 Batch 300 Loss -1.1356\n",
      "Epoch 287 Batch 350 Loss -1.1495\n",
      "Epoch 287 Loss -1.1588\n",
      "{'Epoch': 287}\n",
      "Epoch 288 Batch 0 Loss -0.3776\n",
      "Epoch 288 Batch 50 Loss -1.0731\n",
      "Epoch 288 Batch 100 Loss -1.0428\n",
      "Epoch 288 Batch 150 Loss -1.0580\n",
      "Epoch 288 Batch 200 Loss -1.1037\n",
      "Epoch 288 Batch 250 Loss -1.1269\n",
      "Epoch 288 Batch 300 Loss -1.1324\n",
      "Epoch 288 Batch 350 Loss -1.1468\n",
      "Epoch 288 Loss -1.1564\n",
      "{'Epoch': 288}\n",
      "Epoch 289 Batch 0 Loss -0.3768\n",
      "Epoch 289 Batch 50 Loss -1.0701\n",
      "Epoch 289 Batch 100 Loss -1.0413\n",
      "Epoch 289 Batch 150 Loss -1.0572\n",
      "Epoch 289 Batch 200 Loss -1.1028\n",
      "Epoch 289 Batch 250 Loss -1.1269\n",
      "Epoch 289 Batch 300 Loss -1.1322\n",
      "Epoch 289 Batch 350 Loss -1.1468\n",
      "Epoch 289 Loss -1.1563\n",
      "{'Epoch': 289}\n",
      "Epoch 290 Batch 0 Loss -0.3704\n",
      "Epoch 290 Batch 50 Loss -1.0689\n",
      "Epoch 290 Batch 100 Loss -1.0410\n",
      "Epoch 290 Batch 150 Loss -1.0567\n",
      "Epoch 290 Batch 200 Loss -1.1026\n",
      "Epoch 290 Batch 250 Loss -1.1266\n",
      "Epoch 290 Batch 300 Loss -1.1255\n",
      "Epoch 290 Batch 350 Loss -1.1354\n",
      "Epoch 290 Loss -1.1454\n",
      "{'Epoch': 290}\n",
      "Epoch 291 Batch 0 Loss -0.3553\n",
      "Epoch 291 Batch 50 Loss -1.0788\n",
      "Epoch 291 Batch 100 Loss -1.0496\n",
      "Epoch 291 Batch 150 Loss -1.0638\n",
      "Epoch 291 Batch 200 Loss -1.1096\n",
      "Epoch 291 Batch 250 Loss -1.1339\n",
      "Epoch 291 Batch 300 Loss -1.1387\n",
      "Epoch 291 Batch 350 Loss -1.1522\n",
      "Epoch 291 Loss -1.1606\n",
      "{'Epoch': 291}\n",
      "Epoch 292 Batch 0 Loss -0.3456\n",
      "Epoch 292 Batch 50 Loss -1.0736\n",
      "Epoch 292 Batch 100 Loss -1.0440\n",
      "Epoch 292 Batch 150 Loss -1.0563\n",
      "Epoch 292 Batch 200 Loss -1.1031\n",
      "Epoch 292 Batch 250 Loss -1.1265\n",
      "Epoch 292 Batch 300 Loss -1.1315\n",
      "Epoch 292 Batch 350 Loss -1.1453\n",
      "Epoch 292 Loss -1.1537\n",
      "{'Epoch': 292}\n",
      "Epoch 293 Batch 0 Loss -0.3319\n",
      "Epoch 293 Batch 50 Loss -1.0745\n",
      "Epoch 293 Batch 100 Loss -1.0449\n",
      "Epoch 293 Batch 150 Loss -1.0581\n",
      "Epoch 293 Batch 200 Loss -1.1036\n",
      "Epoch 293 Batch 250 Loss -1.1251\n",
      "Epoch 293 Batch 300 Loss -1.1306\n",
      "Epoch 293 Batch 350 Loss -1.1451\n",
      "Epoch 293 Loss -1.1539\n",
      "{'Epoch': 293}\n",
      "Epoch 294 Batch 0 Loss -0.3303\n",
      "Epoch 294 Batch 50 Loss -1.0742\n",
      "Epoch 294 Batch 100 Loss -1.0447\n",
      "Epoch 294 Batch 150 Loss -1.0578\n",
      "Epoch 294 Batch 200 Loss -1.1035\n",
      "Epoch 294 Batch 250 Loss -1.1258\n",
      "Epoch 294 Batch 300 Loss -1.1308\n",
      "Epoch 294 Batch 350 Loss -1.1450\n",
      "Epoch 294 Loss -1.1541\n",
      "{'Epoch': 294}\n",
      "Epoch 295 Batch 0 Loss -0.3291\n",
      "Epoch 295 Batch 50 Loss -1.0748\n",
      "Epoch 295 Batch 100 Loss -1.0453\n",
      "Epoch 295 Batch 150 Loss -1.0588\n",
      "Epoch 295 Batch 200 Loss -1.1041\n",
      "Epoch 295 Batch 250 Loss -1.1238\n",
      "Epoch 295 Batch 300 Loss -1.1299\n",
      "Epoch 295 Batch 350 Loss -1.1439\n",
      "Epoch 295 Loss -1.1524\n",
      "{'Epoch': 295}\n",
      "Epoch 296 Batch 0 Loss -0.3375\n",
      "Epoch 296 Batch 50 Loss -1.0769\n",
      "Epoch 296 Batch 100 Loss -1.0464\n",
      "Epoch 296 Batch 150 Loss -1.0593\n",
      "Epoch 296 Batch 200 Loss -1.1053\n",
      "Epoch 296 Batch 250 Loss -1.1277\n",
      "Epoch 296 Batch 300 Loss -1.1329\n",
      "Epoch 296 Batch 350 Loss -1.1467\n",
      "Epoch 296 Loss -1.1528\n",
      "{'Epoch': 296}\n",
      "Epoch 297 Batch 0 Loss -0.3264\n",
      "Epoch 297 Batch 50 Loss -1.0626\n",
      "Epoch 297 Batch 100 Loss -1.0390\n",
      "Epoch 297 Batch 150 Loss -1.0555\n",
      "Epoch 297 Batch 200 Loss -1.1030\n",
      "Epoch 297 Batch 250 Loss -1.1253\n",
      "Epoch 297 Batch 300 Loss -1.1314\n",
      "Epoch 297 Batch 350 Loss -1.1460\n",
      "Epoch 297 Loss -1.1546\n",
      "{'Epoch': 297}\n",
      "Epoch 298 Batch 0 Loss -0.3360\n",
      "Epoch 298 Batch 50 Loss -1.0762\n",
      "Epoch 298 Batch 100 Loss -1.0464\n",
      "Epoch 298 Batch 150 Loss -1.0590\n",
      "Epoch 298 Batch 200 Loss -1.1052\n",
      "Epoch 298 Batch 250 Loss -1.1279\n",
      "Epoch 298 Batch 300 Loss -1.1329\n",
      "Epoch 298 Batch 350 Loss -1.1470\n",
      "Epoch 298 Loss -1.1536\n",
      "{'Epoch': 298}\n",
      "Epoch 299 Batch 0 Loss -0.3492\n",
      "Epoch 299 Batch 50 Loss -1.0727\n",
      "Epoch 299 Batch 100 Loss -1.0438\n",
      "Epoch 299 Batch 150 Loss -1.0580\n",
      "Epoch 299 Batch 200 Loss -1.1046\n",
      "Epoch 299 Batch 250 Loss -1.1282\n",
      "Epoch 299 Batch 300 Loss -1.1331\n",
      "Epoch 299 Batch 350 Loss -1.1476\n",
      "Epoch 299 Loss -1.1565\n",
      "{'Epoch': 299}\n",
      "Epoch 300 Batch 0 Loss -0.3423\n",
      "Epoch 300 Batch 50 Loss -1.0761\n",
      "Epoch 300 Batch 100 Loss -1.0452\n",
      "Epoch 300 Batch 150 Loss -1.0584\n",
      "Epoch 300 Batch 200 Loss -1.1044\n",
      "Epoch 300 Batch 250 Loss -1.1275\n",
      "Epoch 300 Batch 300 Loss -1.1330\n",
      "Epoch 300 Batch 350 Loss -1.1474\n",
      "Epoch 300 Loss -1.1562\n",
      "{'Epoch': 300}\n",
      "Epoch 301 Batch 0 Loss -0.3421\n",
      "Epoch 301 Batch 50 Loss -1.0750\n",
      "Epoch 301 Batch 100 Loss -1.0453\n",
      "Epoch 301 Batch 150 Loss -1.0591\n",
      "Epoch 301 Batch 200 Loss -1.1049\n",
      "Epoch 301 Batch 250 Loss -1.1261\n",
      "Epoch 301 Batch 300 Loss -1.1304\n",
      "Epoch 301 Batch 350 Loss -1.1449\n",
      "Epoch 301 Loss -1.1531\n",
      "{'Epoch': 301}\n",
      "Epoch 302 Batch 0 Loss -0.3326\n",
      "Epoch 302 Batch 50 Loss -1.0769\n",
      "Epoch 302 Batch 100 Loss -1.0465\n",
      "Epoch 302 Batch 150 Loss -1.0599\n",
      "Epoch 302 Batch 200 Loss -1.1058\n",
      "Epoch 302 Batch 250 Loss -1.1283\n",
      "Epoch 302 Batch 300 Loss -1.1335\n",
      "Epoch 302 Batch 350 Loss -1.1466\n",
      "Epoch 302 Loss -1.1528\n",
      "{'Epoch': 302}\n",
      "Epoch 303 Batch 0 Loss -0.3248\n",
      "Epoch 303 Batch 50 Loss -1.0067\n",
      "Epoch 303 Batch 100 Loss -0.9804\n",
      "Epoch 303 Batch 150 Loss -0.9981\n",
      "Epoch 303 Batch 200 Loss -1.0496\n",
      "Epoch 303 Batch 250 Loss -1.0746\n",
      "Epoch 303 Batch 300 Loss -1.0827\n",
      "Epoch 303 Batch 350 Loss -1.0982\n",
      "Epoch 303 Loss -1.1090\n",
      "{'Epoch': 303}\n",
      "Epoch 304 Batch 0 Loss -0.3755\n",
      "Epoch 304 Batch 50 Loss -1.0329\n",
      "Epoch 304 Batch 100 Loss -0.9897\n",
      "Epoch 304 Batch 150 Loss -1.0057\n",
      "Epoch 304 Batch 200 Loss -1.0560\n",
      "Epoch 304 Batch 250 Loss -1.0709\n",
      "Epoch 304 Batch 300 Loss -1.0807\n",
      "Epoch 304 Batch 350 Loss -1.1005\n",
      "Epoch 304 Loss -1.1131\n",
      "{'Epoch': 304}\n",
      "Epoch 305 Batch 0 Loss -0.3540\n",
      "Epoch 305 Batch 50 Loss -1.0669\n",
      "Epoch 305 Batch 100 Loss -1.0312\n",
      "Epoch 305 Batch 150 Loss -1.0436\n",
      "Epoch 305 Batch 200 Loss -1.0895\n",
      "Epoch 305 Batch 250 Loss -1.1149\n",
      "Epoch 305 Batch 300 Loss -1.1175\n",
      "Epoch 305 Batch 350 Loss -1.1303\n",
      "Epoch 305 Loss -1.1389\n",
      "{'Epoch': 305}\n",
      "Epoch 306 Batch 0 Loss -0.3664\n",
      "Epoch 306 Batch 50 Loss -1.0558\n",
      "Epoch 306 Batch 100 Loss -1.0258\n",
      "Epoch 306 Batch 150 Loss -1.0411\n",
      "Epoch 306 Batch 200 Loss -1.0883\n",
      "Epoch 306 Batch 250 Loss -1.1147\n",
      "Epoch 306 Batch 300 Loss -1.1189\n",
      "Epoch 306 Batch 350 Loss -1.1324\n",
      "Epoch 306 Loss -1.1407\n",
      "{'Epoch': 306}\n",
      "Epoch 307 Batch 0 Loss -0.3521\n",
      "Epoch 307 Batch 50 Loss -1.0534\n",
      "Epoch 307 Batch 100 Loss -1.0198\n",
      "Epoch 307 Batch 150 Loss -1.0359\n",
      "Epoch 307 Batch 200 Loss -1.0832\n",
      "Epoch 307 Batch 250 Loss -1.1082\n",
      "Epoch 307 Batch 300 Loss -1.1150\n",
      "Epoch 307 Batch 350 Loss -1.1296\n",
      "Epoch 307 Loss -1.1389\n",
      "{'Epoch': 307}\n",
      "Epoch 308 Batch 0 Loss -0.3450\n",
      "Epoch 308 Batch 50 Loss -1.0555\n",
      "Epoch 308 Batch 100 Loss -1.0235\n",
      "Epoch 308 Batch 150 Loss -1.0360\n",
      "Epoch 308 Batch 200 Loss -1.0885\n",
      "Epoch 308 Batch 250 Loss -1.1176\n",
      "Epoch 308 Batch 300 Loss -1.1269\n",
      "Epoch 308 Batch 350 Loss -1.1426\n",
      "Epoch 308 Loss -1.1524\n",
      "{'Epoch': 308}\n",
      "Epoch 309 Batch 0 Loss -0.3511\n",
      "Epoch 309 Batch 50 Loss -1.0814\n",
      "Epoch 309 Batch 100 Loss -1.0497\n",
      "Epoch 309 Batch 150 Loss -1.0624\n",
      "Epoch 309 Batch 200 Loss -1.1081\n",
      "Epoch 309 Batch 250 Loss -1.1311\n",
      "Epoch 309 Batch 300 Loss -1.1362\n",
      "Epoch 309 Batch 350 Loss -1.1503\n",
      "Epoch 309 Loss -1.1590\n",
      "{'Epoch': 309}\n",
      "Epoch 310 Batch 0 Loss -0.3385\n",
      "Epoch 310 Batch 50 Loss -1.0765\n",
      "Epoch 310 Batch 100 Loss -1.0465\n",
      "Epoch 310 Batch 150 Loss -1.0598\n",
      "Epoch 310 Batch 200 Loss -1.1057\n",
      "Epoch 310 Batch 250 Loss -1.1280\n",
      "Epoch 310 Batch 300 Loss -1.1333\n",
      "Epoch 310 Batch 350 Loss -1.1477\n",
      "Epoch 310 Loss -1.1547\n",
      "{'Epoch': 310}\n",
      "Epoch 311 Batch 0 Loss -0.3538\n",
      "Epoch 311 Batch 50 Loss -1.0766\n",
      "Epoch 311 Batch 100 Loss -1.0467\n",
      "Epoch 311 Batch 150 Loss -1.0604\n",
      "Epoch 311 Batch 200 Loss -1.1067\n",
      "Epoch 311 Batch 250 Loss -1.1294\n",
      "Epoch 311 Batch 300 Loss -1.1345\n",
      "Epoch 311 Batch 350 Loss -1.1491\n",
      "Epoch 311 Loss -1.1577\n",
      "{'Epoch': 311}\n",
      "Epoch 312 Batch 0 Loss -0.3293\n",
      "Epoch 312 Batch 50 Loss -1.0735\n",
      "Epoch 312 Batch 100 Loss -1.0441\n",
      "Epoch 312 Batch 150 Loss -1.0585\n",
      "Epoch 312 Batch 200 Loss -1.1056\n",
      "Epoch 312 Batch 250 Loss -1.1291\n",
      "Epoch 312 Batch 300 Loss -1.1343\n",
      "Epoch 312 Batch 350 Loss -1.1486\n",
      "Epoch 312 Loss -1.1479\n",
      "{'Epoch': 312}\n",
      "Epoch 313 Batch 0 Loss -0.3232\n",
      "Epoch 313 Batch 50 Loss -1.0338\n",
      "Epoch 313 Batch 100 Loss -1.0141\n",
      "Epoch 313 Batch 150 Loss -1.0336\n",
      "Epoch 313 Batch 200 Loss -1.0825\n",
      "Epoch 313 Batch 250 Loss -1.1092\n",
      "Epoch 313 Batch 300 Loss -1.1165\n",
      "Epoch 313 Batch 350 Loss -1.1314\n",
      "Epoch 313 Loss -1.1408\n",
      "{'Epoch': 313}\n",
      "Epoch 314 Batch 0 Loss -0.3431\n",
      "Epoch 314 Batch 50 Loss -1.0556\n",
      "Epoch 314 Batch 100 Loss -1.0246\n",
      "Epoch 314 Batch 150 Loss -1.0352\n",
      "Epoch 314 Batch 200 Loss -1.0816\n",
      "Epoch 314 Batch 250 Loss -1.1068\n",
      "Epoch 314 Batch 300 Loss -1.1117\n",
      "Epoch 314 Batch 350 Loss -1.1272\n",
      "Epoch 314 Loss -1.1372\n",
      "{'Epoch': 314}\n",
      "Epoch 315 Batch 0 Loss -0.3537\n",
      "Epoch 315 Batch 50 Loss -1.0604\n",
      "Epoch 315 Batch 100 Loss -1.0261\n",
      "Epoch 315 Batch 150 Loss -1.0383\n",
      "Epoch 315 Batch 200 Loss -1.0850\n",
      "Epoch 315 Batch 250 Loss -1.1095\n",
      "Epoch 315 Batch 300 Loss -1.1102\n",
      "Epoch 315 Batch 350 Loss -1.1246\n",
      "Epoch 315 Loss -1.1338\n",
      "{'Epoch': 315}\n",
      "Epoch 316 Batch 0 Loss -0.3832\n",
      "Epoch 316 Batch 50 Loss -1.0761\n",
      "Epoch 316 Batch 100 Loss -1.0479\n",
      "Epoch 316 Batch 150 Loss -1.0632\n",
      "Epoch 316 Batch 200 Loss -1.1109\n",
      "Epoch 316 Batch 250 Loss -1.1362\n",
      "Epoch 316 Batch 300 Loss -1.1428\n",
      "Epoch 316 Batch 350 Loss -1.1566\n",
      "Epoch 316 Loss -1.1653\n",
      "{'Epoch': 316}\n",
      "Epoch 317 Batch 0 Loss -0.3489\n",
      "Epoch 317 Batch 50 Loss -1.0799\n",
      "Epoch 317 Batch 100 Loss -1.0492\n",
      "Epoch 317 Batch 150 Loss -1.0621\n",
      "Epoch 317 Batch 200 Loss -1.1079\n",
      "Epoch 317 Batch 250 Loss -1.1306\n",
      "Epoch 317 Batch 300 Loss -1.1358\n",
      "Epoch 317 Batch 350 Loss -1.1501\n",
      "Epoch 317 Loss -1.1556\n",
      "{'Epoch': 317}\n",
      "Epoch 318 Batch 0 Loss -0.2560\n",
      "Epoch 318 Batch 50 Loss -1.0241\n",
      "Epoch 318 Batch 100 Loss -1.0245\n",
      "Epoch 318 Batch 150 Loss -1.0542\n",
      "Epoch 318 Batch 200 Loss -1.1098\n",
      "Epoch 318 Batch 250 Loss -1.1395\n",
      "Epoch 318 Batch 300 Loss -1.1488\n",
      "Epoch 318 Batch 350 Loss -1.1644\n",
      "Epoch 318 Loss -1.1752\n",
      "{'Epoch': 318}\n",
      "Epoch 319 Batch 0 Loss -0.3850\n",
      "Epoch 319 Batch 50 Loss -1.1035\n",
      "Epoch 319 Batch 100 Loss -1.0727\n",
      "Epoch 319 Batch 150 Loss -1.0864\n",
      "Epoch 319 Batch 200 Loss -1.1308\n",
      "Epoch 319 Batch 250 Loss -1.1562\n",
      "Epoch 319 Batch 300 Loss -1.1608\n",
      "Epoch 319 Batch 350 Loss -1.1748\n",
      "Epoch 319 Loss -1.1842\n",
      "{'Epoch': 319}\n",
      "Epoch 320 Batch 0 Loss -0.3714\n",
      "Epoch 320 Batch 50 Loss -1.0894\n",
      "Epoch 320 Batch 100 Loss -1.0639\n",
      "Epoch 320 Batch 150 Loss -1.0808\n",
      "Epoch 320 Batch 200 Loss -1.1262\n",
      "Epoch 320 Batch 250 Loss -1.1390\n",
      "Epoch 320 Batch 300 Loss -1.1335\n",
      "Epoch 320 Batch 350 Loss -1.1427\n",
      "Epoch 320 Loss -1.1525\n",
      "{'Epoch': 320}\n",
      "Epoch 321 Batch 0 Loss -0.3647\n",
      "Epoch 321 Batch 50 Loss -1.0817\n",
      "Epoch 321 Batch 100 Loss -1.0525\n",
      "Epoch 321 Batch 150 Loss -1.0672\n",
      "Epoch 321 Batch 200 Loss -1.1136\n",
      "Epoch 321 Batch 250 Loss -1.1390\n",
      "Epoch 321 Batch 300 Loss -1.1428\n",
      "Epoch 321 Batch 350 Loss -1.1568\n",
      "Epoch 321 Loss -1.1659\n",
      "{'Epoch': 321}\n",
      "Epoch 322 Batch 0 Loss -0.3793\n",
      "Epoch 322 Batch 50 Loss -1.0751\n",
      "Epoch 322 Batch 100 Loss -1.0458\n",
      "Epoch 322 Batch 150 Loss -1.0609\n",
      "Epoch 322 Batch 200 Loss -1.1072\n",
      "Epoch 322 Batch 250 Loss -1.1322\n",
      "Epoch 322 Batch 300 Loss -1.1372\n",
      "Epoch 322 Batch 350 Loss -1.1516\n",
      "Epoch 322 Loss -1.1613\n",
      "{'Epoch': 322}\n",
      "Epoch 323 Batch 0 Loss -0.3722\n",
      "Epoch 323 Batch 50 Loss -1.0731\n",
      "Epoch 323 Batch 100 Loss -1.0450\n",
      "Epoch 323 Batch 150 Loss -1.0603\n",
      "Epoch 323 Batch 200 Loss -1.1066\n",
      "Epoch 323 Batch 250 Loss -1.1313\n",
      "Epoch 323 Batch 300 Loss -1.1348\n",
      "Epoch 323 Batch 350 Loss -1.1391\n",
      "Epoch 323 Loss -1.1472\n",
      "{'Epoch': 323}\n",
      "Epoch 324 Batch 0 Loss -0.3265\n",
      "Epoch 324 Batch 50 Loss -1.0576\n",
      "Epoch 324 Batch 100 Loss -1.0289\n",
      "Epoch 324 Batch 150 Loss -1.0439\n",
      "Epoch 324 Batch 200 Loss -1.0889\n",
      "Epoch 324 Batch 250 Loss -1.1094\n",
      "Epoch 324 Batch 300 Loss -1.1164\n",
      "Epoch 324 Batch 350 Loss -1.1311\n",
      "Epoch 324 Loss -1.1408\n",
      "{'Epoch': 324}\n",
      "Epoch 325 Batch 0 Loss -0.3372\n",
      "Epoch 325 Batch 50 Loss -1.0579\n",
      "Epoch 325 Batch 100 Loss -1.0277\n",
      "Epoch 325 Batch 150 Loss -1.0394\n",
      "Epoch 325 Batch 200 Loss -1.0854\n",
      "Epoch 325 Batch 250 Loss -1.1099\n",
      "Epoch 325 Batch 300 Loss -1.1097\n",
      "Epoch 325 Batch 350 Loss -1.1250\n",
      "Epoch 325 Loss -1.1354\n",
      "{'Epoch': 325}\n",
      "Epoch 326 Batch 0 Loss -0.3466\n",
      "Epoch 326 Batch 50 Loss -1.0648\n",
      "Epoch 326 Batch 100 Loss -1.0296\n",
      "Epoch 326 Batch 150 Loss -1.0421\n",
      "Epoch 326 Batch 200 Loss -1.0887\n",
      "Epoch 326 Batch 250 Loss -1.1112\n",
      "Epoch 326 Batch 300 Loss -1.1156\n",
      "Epoch 326 Batch 350 Loss -1.1298\n",
      "Epoch 326 Loss -1.1382\n",
      "{'Epoch': 326}\n",
      "Epoch 327 Batch 0 Loss -0.3446\n",
      "Epoch 327 Batch 50 Loss -1.0604\n",
      "Epoch 327 Batch 100 Loss -1.0280\n",
      "Epoch 327 Batch 150 Loss -1.0426\n",
      "Epoch 327 Batch 200 Loss -1.0883\n",
      "Epoch 327 Batch 250 Loss -1.1140\n",
      "Epoch 327 Batch 300 Loss -1.1183\n",
      "Epoch 327 Batch 350 Loss -1.1316\n",
      "Epoch 327 Loss -1.1405\n",
      "{'Epoch': 327}\n",
      "Epoch 328 Batch 0 Loss -0.3507\n",
      "Epoch 328 Batch 50 Loss -1.0597\n",
      "Epoch 328 Batch 100 Loss -1.0277\n",
      "Epoch 328 Batch 150 Loss -1.0421\n",
      "Epoch 328 Batch 200 Loss -1.0881\n",
      "Epoch 328 Batch 250 Loss -1.1125\n",
      "Epoch 328 Batch 300 Loss -1.1179\n",
      "Epoch 328 Batch 350 Loss -1.1304\n",
      "Epoch 328 Loss -1.1399\n",
      "{'Epoch': 328}\n",
      "Epoch 329 Batch 0 Loss -0.3421\n",
      "Epoch 329 Batch 50 Loss -1.0603\n",
      "Epoch 329 Batch 100 Loss -1.0291\n",
      "Epoch 329 Batch 150 Loss -1.0374\n",
      "Epoch 329 Batch 200 Loss -1.0806\n",
      "Epoch 329 Batch 250 Loss -1.1015\n",
      "Epoch 329 Batch 300 Loss -1.0921\n",
      "Epoch 329 Batch 350 Loss -1.1075\n",
      "Epoch 329 Loss -1.1190\n",
      "{'Epoch': 329}\n",
      "Epoch 330 Batch 0 Loss -0.3290\n",
      "Epoch 330 Batch 50 Loss -1.0785\n",
      "Epoch 330 Batch 100 Loss -1.0513\n",
      "Epoch 330 Batch 150 Loss -1.0677\n",
      "Epoch 330 Batch 200 Loss -1.1172\n",
      "Epoch 330 Batch 250 Loss -1.1444\n",
      "Epoch 330 Batch 300 Loss -1.1500\n",
      "Epoch 330 Batch 350 Loss -1.1636\n",
      "Epoch 330 Loss -1.1713\n",
      "{'Epoch': 330}\n",
      "Epoch 331 Batch 0 Loss -0.3333\n",
      "Epoch 331 Batch 50 Loss -1.0843\n",
      "Epoch 331 Batch 100 Loss -1.0531\n",
      "Epoch 331 Batch 150 Loss -1.0652\n",
      "Epoch 331 Batch 200 Loss -1.1113\n",
      "Epoch 331 Batch 250 Loss -1.1345\n",
      "Epoch 331 Batch 300 Loss -1.1395\n",
      "Epoch 331 Batch 350 Loss -1.1534\n",
      "Epoch 331 Loss -1.1615\n",
      "{'Epoch': 331}\n",
      "Epoch 332 Batch 0 Loss -0.3350\n",
      "Epoch 332 Batch 50 Loss -1.0805\n",
      "Epoch 332 Batch 100 Loss -1.0500\n",
      "Epoch 332 Batch 150 Loss -1.0633\n",
      "Epoch 332 Batch 200 Loss -1.1095\n",
      "Epoch 332 Batch 250 Loss -1.1320\n",
      "Epoch 332 Batch 300 Loss -1.1372\n",
      "Epoch 332 Batch 350 Loss -1.1513\n",
      "Epoch 332 Loss -1.1553\n",
      "{'Epoch': 332}\n",
      "Epoch 333 Batch 0 Loss -0.3057\n",
      "Epoch 333 Batch 50 Loss -1.0704\n",
      "Epoch 333 Batch 100 Loss -1.0406\n",
      "Epoch 333 Batch 150 Loss -1.0565\n",
      "Epoch 333 Batch 200 Loss -1.1042\n",
      "Epoch 333 Batch 250 Loss -1.1304\n",
      "Epoch 333 Batch 300 Loss -1.1371\n",
      "Epoch 333 Batch 350 Loss -1.1522\n",
      "Epoch 333 Loss -1.1584\n",
      "{'Epoch': 333}\n",
      "Epoch 334 Batch 0 Loss -0.3286\n",
      "Epoch 334 Batch 50 Loss -1.0415\n",
      "Epoch 334 Batch 100 Loss -0.9921\n",
      "Epoch 334 Batch 150 Loss -1.0134\n",
      "Epoch 334 Batch 200 Loss -1.0708\n",
      "Epoch 334 Batch 250 Loss -1.1061\n",
      "Epoch 334 Batch 300 Loss -1.1181\n",
      "Epoch 334 Batch 350 Loss -1.1366\n",
      "Epoch 334 Loss -1.1481\n",
      "{'Epoch': 334}\n",
      "Epoch 335 Batch 0 Loss -0.3717\n",
      "Epoch 335 Batch 50 Loss -1.0846\n",
      "Epoch 335 Batch 100 Loss -1.0535\n",
      "Epoch 335 Batch 150 Loss -1.0678\n",
      "Epoch 335 Batch 200 Loss -1.1137\n",
      "Epoch 335 Batch 250 Loss -1.1380\n",
      "Epoch 335 Batch 300 Loss -1.1426\n",
      "Epoch 335 Batch 350 Loss -1.1570\n",
      "Epoch 335 Loss -1.1664\n",
      "{'Epoch': 335}\n",
      "Epoch 336 Batch 0 Loss -0.3785\n",
      "Epoch 336 Batch 50 Loss -1.0753\n",
      "Epoch 336 Batch 100 Loss -1.0460\n",
      "Epoch 336 Batch 150 Loss -1.0620\n",
      "Epoch 336 Batch 200 Loss -1.1081\n",
      "Epoch 336 Batch 250 Loss -1.1327\n",
      "Epoch 336 Batch 300 Loss -1.1379\n",
      "Epoch 336 Batch 350 Loss -1.1530\n",
      "Epoch 336 Loss -1.1627\n",
      "{'Epoch': 336}\n",
      "Epoch 337 Batch 0 Loss -0.3823\n",
      "Epoch 337 Batch 50 Loss -1.0794\n",
      "Epoch 337 Batch 100 Loss -1.0491\n",
      "Epoch 337 Batch 150 Loss -1.0641\n",
      "Epoch 337 Batch 200 Loss -1.1104\n",
      "Epoch 337 Batch 250 Loss -1.1350\n",
      "Epoch 337 Batch 300 Loss -1.1341\n",
      "Epoch 337 Batch 350 Loss -1.1418\n",
      "Epoch 337 Loss -1.1487\n",
      "{'Epoch': 337}\n",
      "Epoch 338 Batch 0 Loss -0.3469\n",
      "Epoch 338 Batch 50 Loss -1.0461\n",
      "Epoch 338 Batch 100 Loss -1.0236\n",
      "Epoch 338 Batch 150 Loss -1.0499\n",
      "Epoch 338 Batch 200 Loss -1.1064\n",
      "Epoch 338 Batch 250 Loss -1.1388\n",
      "Epoch 338 Batch 300 Loss -1.1503\n",
      "Epoch 338 Batch 350 Loss -1.1678\n",
      "Epoch 338 Loss -1.1786\n",
      "{'Epoch': 338}\n",
      "Epoch 339 Batch 0 Loss -0.3837\n",
      "Epoch 339 Batch 50 Loss -1.1068\n",
      "Epoch 339 Batch 100 Loss -1.0769\n",
      "Epoch 339 Batch 150 Loss -1.0891\n",
      "Epoch 339 Batch 200 Loss -1.1325\n",
      "Epoch 339 Batch 250 Loss -1.1592\n",
      "Epoch 339 Batch 300 Loss -1.1644\n",
      "Epoch 339 Batch 350 Loss -1.1784\n",
      "Epoch 339 Loss -1.1874\n",
      "{'Epoch': 339}\n",
      "Epoch 340 Batch 0 Loss -0.3759\n",
      "Epoch 340 Batch 50 Loss -1.0904\n",
      "Epoch 340 Batch 100 Loss -1.0656\n",
      "Epoch 340 Batch 150 Loss -1.0838\n",
      "Epoch 340 Batch 200 Loss -1.1297\n",
      "Epoch 340 Batch 250 Loss -1.1364\n",
      "Epoch 340 Batch 300 Loss -1.1330\n",
      "Epoch 340 Batch 350 Loss -1.1445\n",
      "Epoch 340 Loss -1.1527\n",
      "{'Epoch': 340}\n",
      "Epoch 341 Batch 0 Loss -0.3443\n",
      "Epoch 341 Batch 50 Loss -1.0617\n",
      "Epoch 341 Batch 100 Loss -1.0292\n",
      "Epoch 341 Batch 150 Loss -1.0419\n",
      "Epoch 341 Batch 200 Loss -1.0870\n",
      "Epoch 341 Batch 250 Loss -1.1117\n",
      "Epoch 341 Batch 300 Loss -1.1173\n",
      "Epoch 341 Batch 350 Loss -1.1307\n",
      "Epoch 341 Loss -1.1403\n",
      "{'Epoch': 341}\n",
      "Epoch 342 Batch 0 Loss -0.3439\n",
      "Epoch 342 Batch 50 Loss -1.0619\n",
      "Epoch 342 Batch 100 Loss -1.0312\n",
      "Epoch 342 Batch 150 Loss -1.0419\n",
      "Epoch 342 Batch 200 Loss -1.0882\n",
      "Epoch 342 Batch 250 Loss -1.1130\n",
      "Epoch 342 Batch 300 Loss -1.1165\n",
      "Epoch 342 Batch 350 Loss -1.1316\n",
      "Epoch 342 Loss -1.1415\n",
      "{'Epoch': 342}\n",
      "Epoch 343 Batch 0 Loss -0.3517\n",
      "Epoch 343 Batch 50 Loss -1.0646\n",
      "Epoch 343 Batch 100 Loss -1.0306\n",
      "Epoch 343 Batch 150 Loss -1.0430\n",
      "Epoch 343 Batch 200 Loss -1.0895\n",
      "Epoch 343 Batch 250 Loss -1.1136\n",
      "Epoch 343 Batch 300 Loss -1.1167\n",
      "Epoch 343 Batch 350 Loss -1.1309\n",
      "Epoch 343 Loss -1.1396\n",
      "{'Epoch': 343}\n",
      "Epoch 344 Batch 0 Loss -0.3330\n",
      "Epoch 344 Batch 50 Loss -1.0591\n",
      "Epoch 344 Batch 100 Loss -1.0282\n",
      "Epoch 344 Batch 150 Loss -1.0434\n",
      "Epoch 344 Batch 200 Loss -1.0894\n",
      "Epoch 344 Batch 250 Loss -1.1150\n",
      "Epoch 344 Batch 300 Loss -1.1186\n",
      "Epoch 344 Batch 350 Loss -1.1325\n",
      "Epoch 344 Loss -1.1412\n",
      "{'Epoch': 344}\n",
      "Epoch 345 Batch 0 Loss -0.3410\n",
      "Epoch 345 Batch 50 Loss -1.0447\n",
      "Epoch 345 Batch 100 Loss -1.0021\n",
      "Epoch 345 Batch 150 Loss -1.0188\n",
      "Epoch 345 Batch 200 Loss -1.0711\n",
      "Epoch 345 Batch 250 Loss -1.1056\n",
      "Epoch 345 Batch 300 Loss -1.1190\n",
      "Epoch 345 Batch 350 Loss -1.1379\n",
      "Epoch 345 Loss -1.1494\n",
      "{'Epoch': 345}\n",
      "Epoch 346 Batch 0 Loss -0.3824\n",
      "Epoch 346 Batch 50 Loss -1.0916\n",
      "Epoch 346 Batch 100 Loss -1.0581\n",
      "Epoch 346 Batch 150 Loss -1.0726\n",
      "Epoch 346 Batch 200 Loss -1.1189\n",
      "Epoch 346 Batch 250 Loss -1.1440\n",
      "Epoch 346 Batch 300 Loss -1.1475\n",
      "Epoch 346 Batch 350 Loss -1.1607\n",
      "Epoch 346 Loss -1.1694\n",
      "{'Epoch': 346}\n",
      "Epoch 347 Batch 0 Loss -0.3821\n",
      "Epoch 347 Batch 50 Loss -1.0830\n",
      "Epoch 347 Batch 100 Loss -1.0517\n",
      "Epoch 347 Batch 150 Loss -1.0660\n",
      "Epoch 347 Batch 200 Loss -1.1118\n",
      "Epoch 347 Batch 250 Loss -1.1366\n",
      "Epoch 347 Batch 300 Loss -1.1401\n",
      "Epoch 347 Batch 350 Loss -1.1546\n",
      "Epoch 347 Loss -1.1636\n",
      "{'Epoch': 347}\n",
      "Epoch 348 Batch 0 Loss -0.3844\n",
      "Epoch 348 Batch 50 Loss -1.0826\n",
      "Epoch 348 Batch 100 Loss -1.0493\n",
      "Epoch 348 Batch 150 Loss -1.0648\n",
      "Epoch 348 Batch 200 Loss -1.1115\n",
      "Epoch 348 Batch 250 Loss -1.1367\n",
      "Epoch 348 Batch 300 Loss -1.1409\n",
      "Epoch 348 Batch 350 Loss -1.1555\n",
      "Epoch 348 Loss -1.1644\n",
      "{'Epoch': 348}\n",
      "Epoch 349 Batch 0 Loss -0.3768\n",
      "Epoch 349 Batch 50 Loss -1.0826\n",
      "Epoch 349 Batch 100 Loss -1.0495\n",
      "Epoch 349 Batch 150 Loss -1.0645\n",
      "Epoch 349 Batch 200 Loss -1.1111\n",
      "Epoch 349 Batch 250 Loss -1.1362\n",
      "Epoch 349 Batch 300 Loss -1.1404\n",
      "Epoch 349 Batch 350 Loss -1.1551\n",
      "Epoch 349 Loss -1.1643\n",
      "{'Epoch': 349}\n",
      "Epoch 350 Batch 0 Loss -0.3806\n",
      "Epoch 350 Batch 50 Loss -1.0827\n",
      "Epoch 350 Batch 100 Loss -1.0500\n",
      "Epoch 350 Batch 150 Loss -1.0650\n",
      "Epoch 350 Batch 200 Loss -1.1113\n",
      "Epoch 350 Batch 250 Loss -1.1363\n",
      "Epoch 350 Batch 300 Loss -1.1408\n",
      "Epoch 350 Batch 350 Loss -1.1551\n",
      "Epoch 350 Loss -1.1643\n",
      "{'Epoch': 350}\n",
      "Epoch 351 Batch 0 Loss -0.3877\n",
      "Epoch 351 Batch 50 Loss -1.0823\n",
      "Epoch 351 Batch 100 Loss -1.0432\n",
      "Epoch 351 Batch 150 Loss -1.0597\n",
      "Epoch 351 Batch 200 Loss -1.1081\n",
      "Epoch 351 Batch 250 Loss -1.1343\n",
      "Epoch 351 Batch 300 Loss -1.1392\n",
      "Epoch 351 Batch 350 Loss -1.1541\n",
      "Epoch 351 Loss -1.1634\n",
      "{'Epoch': 351}\n",
      "Epoch 352 Batch 0 Loss -0.3840\n",
      "Epoch 352 Batch 50 Loss -1.0846\n",
      "Epoch 352 Batch 100 Loss -1.0495\n",
      "Epoch 352 Batch 150 Loss -1.0642\n",
      "Epoch 352 Batch 200 Loss -1.1108\n",
      "Epoch 352 Batch 250 Loss -1.1362\n",
      "Epoch 352 Batch 300 Loss -1.1406\n",
      "Epoch 352 Batch 350 Loss -1.1546\n",
      "Epoch 352 Loss -1.1639\n",
      "{'Epoch': 352}\n",
      "Epoch 353 Batch 0 Loss -0.3910\n",
      "Epoch 353 Batch 50 Loss -1.0843\n",
      "Epoch 353 Batch 100 Loss -1.0509\n",
      "Epoch 353 Batch 150 Loss -1.0651\n",
      "Epoch 353 Batch 200 Loss -1.1113\n",
      "Epoch 353 Batch 250 Loss -1.1367\n",
      "Epoch 353 Batch 300 Loss -1.1405\n",
      "Epoch 353 Batch 350 Loss -1.1552\n",
      "Epoch 353 Loss -1.1644\n",
      "{'Epoch': 353}\n",
      "Epoch 354 Batch 0 Loss -0.3752\n",
      "Epoch 354 Batch 50 Loss -1.0803\n",
      "Epoch 354 Batch 100 Loss -1.0503\n",
      "Epoch 354 Batch 150 Loss -1.0652\n",
      "Epoch 354 Batch 200 Loss -1.1122\n",
      "Epoch 354 Batch 250 Loss -1.1373\n",
      "Epoch 354 Batch 300 Loss -1.1419\n",
      "Epoch 354 Batch 350 Loss -1.1559\n",
      "Epoch 354 Loss -1.1651\n",
      "{'Epoch': 354}\n",
      "Epoch 355 Batch 0 Loss -0.3860\n",
      "Epoch 355 Batch 50 Loss -1.0835\n",
      "Epoch 355 Batch 100 Loss -1.0513\n",
      "Epoch 355 Batch 150 Loss -1.0657\n",
      "Epoch 355 Batch 200 Loss -1.1120\n",
      "Epoch 355 Batch 250 Loss -1.1367\n",
      "Epoch 355 Batch 300 Loss -1.1413\n",
      "Epoch 355 Batch 350 Loss -1.1557\n",
      "Epoch 355 Loss -1.1649\n",
      "{'Epoch': 355}\n",
      "Epoch 356 Batch 0 Loss -0.3935\n",
      "Epoch 356 Batch 50 Loss -1.0846\n",
      "Epoch 356 Batch 100 Loss -1.0513\n",
      "Epoch 356 Batch 150 Loss -1.0653\n",
      "Epoch 356 Batch 200 Loss -1.1116\n",
      "Epoch 356 Batch 250 Loss -1.1367\n",
      "Epoch 356 Batch 300 Loss -1.1413\n",
      "Epoch 356 Batch 350 Loss -1.1556\n",
      "Epoch 356 Loss -1.1645\n",
      "{'Epoch': 356}\n",
      "Epoch 357 Batch 0 Loss -0.3768\n",
      "Epoch 357 Batch 50 Loss -1.0835\n",
      "Epoch 357 Batch 100 Loss -1.0467\n",
      "Epoch 357 Batch 150 Loss -1.0598\n",
      "Epoch 357 Batch 200 Loss -1.1088\n",
      "Epoch 357 Batch 250 Loss -1.1361\n",
      "Epoch 357 Batch 300 Loss -1.1418\n",
      "Epoch 357 Batch 350 Loss -1.1562\n",
      "Epoch 357 Loss -1.1656\n",
      "{'Epoch': 357}\n",
      "Epoch 358 Batch 0 Loss -0.3848\n",
      "Epoch 358 Batch 50 Loss -1.0870\n",
      "Epoch 358 Batch 100 Loss -1.0540\n",
      "Epoch 358 Batch 150 Loss -1.0671\n",
      "Epoch 358 Batch 200 Loss -1.1130\n",
      "Epoch 358 Batch 250 Loss -1.1380\n",
      "Epoch 358 Batch 300 Loss -1.1423\n",
      "Epoch 358 Batch 350 Loss -1.1567\n",
      "Epoch 358 Loss -1.1660\n",
      "{'Epoch': 358}\n",
      "Epoch 359 Batch 0 Loss -0.3927\n",
      "Epoch 359 Batch 50 Loss -1.0834\n",
      "Epoch 359 Batch 100 Loss -1.0437\n",
      "Epoch 359 Batch 150 Loss -1.0629\n",
      "Epoch 359 Batch 200 Loss -1.1159\n",
      "Epoch 359 Batch 250 Loss -1.1454\n",
      "Epoch 359 Batch 300 Loss -1.1538\n",
      "Epoch 359 Batch 350 Loss -1.1705\n",
      "Epoch 359 Loss -1.1811\n",
      "{'Epoch': 359}\n",
      "Epoch 360 Batch 0 Loss -0.4113\n",
      "Epoch 360 Batch 50 Loss -1.0980\n",
      "Epoch 360 Batch 100 Loss -1.0473\n",
      "Epoch 360 Batch 150 Loss -1.0528\n",
      "Epoch 360 Batch 200 Loss -1.1033\n",
      "Epoch 360 Batch 250 Loss -1.1321\n",
      "Epoch 360 Batch 300 Loss -1.1390\n",
      "Epoch 360 Batch 350 Loss -1.1542\n",
      "Epoch 360 Loss -1.1640\n",
      "{'Epoch': 360}\n",
      "Epoch 361 Batch 0 Loss -0.3873\n",
      "Epoch 361 Batch 50 Loss -1.0869\n",
      "Epoch 361 Batch 100 Loss -1.0584\n",
      "Epoch 361 Batch 150 Loss -1.0716\n",
      "Epoch 361 Batch 200 Loss -1.1163\n",
      "Epoch 361 Batch 250 Loss -1.1398\n",
      "Epoch 361 Batch 300 Loss -1.1379\n",
      "Epoch 361 Batch 350 Loss -1.1453\n",
      "Epoch 361 Loss -1.1516\n",
      "{'Epoch': 361}\n",
      "Epoch 362 Batch 0 Loss -0.3491\n",
      "Epoch 362 Batch 50 Loss -1.0539\n",
      "Epoch 362 Batch 100 Loss -1.0030\n",
      "Epoch 362 Batch 150 Loss -1.0315\n",
      "Epoch 362 Batch 200 Loss -1.0883\n",
      "Epoch 362 Batch 250 Loss -1.1217\n",
      "Epoch 362 Batch 300 Loss -1.1319\n",
      "Epoch 362 Batch 350 Loss -1.1486\n",
      "Epoch 362 Loss -1.1593\n",
      "{'Epoch': 362}\n",
      "Epoch 363 Batch 0 Loss -0.3845\n",
      "Epoch 363 Batch 50 Loss -1.0918\n",
      "Epoch 363 Batch 100 Loss -1.0589\n",
      "Epoch 363 Batch 150 Loss -1.0718\n",
      "Epoch 363 Batch 200 Loss -1.1172\n",
      "Epoch 363 Batch 250 Loss -1.1416\n",
      "Epoch 363 Batch 300 Loss -1.1464\n",
      "Epoch 363 Batch 350 Loss -1.1602\n",
      "Epoch 363 Loss -1.1694\n",
      "{'Epoch': 363}\n",
      "Epoch 364 Batch 0 Loss -0.3883\n",
      "Epoch 364 Batch 50 Loss -1.0866\n",
      "Epoch 364 Batch 100 Loss -1.0524\n",
      "Epoch 364 Batch 150 Loss -1.0666\n",
      "Epoch 364 Batch 200 Loss -1.1124\n",
      "Epoch 364 Batch 250 Loss -1.1374\n",
      "Epoch 364 Batch 300 Loss -1.1422\n",
      "Epoch 364 Batch 350 Loss -1.1565\n",
      "Epoch 364 Loss -1.1658\n",
      "{'Epoch': 364}\n",
      "Epoch 365 Batch 0 Loss -0.3828\n",
      "Epoch 365 Batch 50 Loss -1.0850\n",
      "Epoch 365 Batch 100 Loss -1.0531\n",
      "Epoch 365 Batch 150 Loss -1.0671\n",
      "Epoch 365 Batch 200 Loss -1.1129\n",
      "Epoch 365 Batch 250 Loss -1.1378\n",
      "Epoch 365 Batch 300 Loss -1.1424\n",
      "Epoch 365 Batch 350 Loss -1.1568\n",
      "Epoch 365 Loss -1.1659\n",
      "{'Epoch': 365}\n",
      "Epoch 366 Batch 0 Loss -0.3917\n",
      "Epoch 366 Batch 50 Loss -1.0852\n",
      "Epoch 366 Batch 100 Loss -1.0515\n",
      "Epoch 366 Batch 150 Loss -1.0659\n",
      "Epoch 366 Batch 200 Loss -1.1122\n",
      "Epoch 366 Batch 250 Loss -1.1375\n",
      "Epoch 366 Batch 300 Loss -1.1427\n",
      "Epoch 366 Batch 350 Loss -1.1569\n",
      "Epoch 366 Loss -1.1659\n",
      "{'Epoch': 366}\n",
      "Epoch 367 Batch 0 Loss -0.3897\n",
      "Epoch 367 Batch 50 Loss -1.0865\n",
      "Epoch 367 Batch 100 Loss -1.0515\n",
      "Epoch 367 Batch 150 Loss -1.0657\n",
      "Epoch 367 Batch 200 Loss -1.1120\n",
      "Epoch 367 Batch 250 Loss -1.1375\n",
      "Epoch 367 Batch 300 Loss -1.1419\n",
      "Epoch 367 Batch 350 Loss -1.1565\n",
      "Epoch 367 Loss -1.1656\n",
      "{'Epoch': 367}\n",
      "Epoch 368 Batch 0 Loss -0.3769\n",
      "Epoch 368 Batch 50 Loss -1.0852\n",
      "Epoch 368 Batch 100 Loss -1.0538\n",
      "Epoch 368 Batch 150 Loss -1.0676\n",
      "Epoch 368 Batch 200 Loss -1.1138\n",
      "Epoch 368 Batch 250 Loss -1.1385\n",
      "Epoch 368 Batch 300 Loss -1.1430\n",
      "Epoch 368 Batch 350 Loss -1.1575\n",
      "Epoch 368 Loss -1.1667\n",
      "{'Epoch': 368}\n",
      "Epoch 369 Batch 0 Loss -0.3777\n",
      "Epoch 369 Batch 50 Loss -1.0854\n",
      "Epoch 369 Batch 100 Loss -1.0484\n",
      "Epoch 369 Batch 150 Loss -1.0640\n",
      "Epoch 369 Batch 200 Loss -1.1113\n",
      "Epoch 369 Batch 250 Loss -1.1373\n",
      "Epoch 369 Batch 300 Loss -1.1421\n",
      "Epoch 369 Batch 350 Loss -1.1565\n",
      "Epoch 369 Loss -1.1657\n",
      "{'Epoch': 369}\n",
      "Epoch 370 Batch 0 Loss -0.3936\n",
      "Epoch 370 Batch 50 Loss -1.0863\n",
      "Epoch 370 Batch 100 Loss -1.0535\n",
      "Epoch 370 Batch 150 Loss -1.0674\n",
      "Epoch 370 Batch 200 Loss -1.1138\n",
      "Epoch 370 Batch 250 Loss -1.1384\n",
      "Epoch 370 Batch 300 Loss -1.1424\n",
      "Epoch 370 Batch 350 Loss -1.1559\n",
      "Epoch 370 Loss -1.1654\n",
      "{'Epoch': 370}\n",
      "Epoch 371 Batch 0 Loss -0.3956\n",
      "Epoch 371 Batch 50 Loss -1.0841\n",
      "Epoch 371 Batch 100 Loss -1.0545\n",
      "Epoch 371 Batch 150 Loss -1.0685\n",
      "Epoch 371 Batch 200 Loss -1.1145\n",
      "Epoch 371 Batch 250 Loss -1.1388\n",
      "Epoch 371 Batch 300 Loss -1.1433\n",
      "Epoch 371 Batch 350 Loss -1.1578\n",
      "Epoch 371 Loss -1.1670\n",
      "{'Epoch': 371}\n",
      "Epoch 372 Batch 0 Loss -0.3875\n",
      "Epoch 372 Batch 50 Loss -1.0870\n",
      "Epoch 372 Batch 100 Loss -1.0532\n",
      "Epoch 372 Batch 150 Loss -1.0671\n",
      "Epoch 372 Batch 200 Loss -1.1134\n",
      "Epoch 372 Batch 250 Loss -1.1380\n",
      "Epoch 372 Batch 300 Loss -1.1427\n",
      "Epoch 372 Batch 350 Loss -1.1568\n",
      "Epoch 372 Loss -1.1662\n",
      "{'Epoch': 372}\n",
      "Epoch 373 Batch 0 Loss -0.3895\n",
      "Epoch 373 Batch 50 Loss -1.0877\n",
      "Epoch 373 Batch 100 Loss -1.0533\n",
      "Epoch 373 Batch 150 Loss -1.0673\n",
      "Epoch 373 Batch 200 Loss -1.1140\n",
      "Epoch 373 Batch 250 Loss -1.1392\n",
      "Epoch 373 Batch 300 Loss -1.1432\n",
      "Epoch 373 Batch 350 Loss -1.1576\n",
      "Epoch 373 Loss -1.1669\n",
      "{'Epoch': 373}\n",
      "Epoch 374 Batch 0 Loss -0.3926\n",
      "Epoch 374 Batch 50 Loss -1.0861\n",
      "Epoch 374 Batch 100 Loss -1.0544\n",
      "Epoch 374 Batch 150 Loss -1.0682\n",
      "Epoch 374 Batch 200 Loss -1.1145\n",
      "Epoch 374 Batch 250 Loss -1.1391\n",
      "Epoch 374 Batch 300 Loss -1.1428\n",
      "Epoch 374 Batch 350 Loss -1.1561\n",
      "Epoch 374 Loss -1.1659\n",
      "{'Epoch': 374}\n",
      "Epoch 375 Batch 0 Loss -0.3996\n",
      "Epoch 375 Batch 50 Loss -1.0886\n",
      "Epoch 375 Batch 100 Loss -1.0561\n",
      "Epoch 375 Batch 150 Loss -1.0699\n",
      "Epoch 375 Batch 200 Loss -1.1156\n",
      "Epoch 375 Batch 250 Loss -1.1406\n",
      "Epoch 375 Batch 300 Loss -1.1450\n",
      "Epoch 375 Batch 350 Loss -1.1592\n",
      "Epoch 375 Loss -1.1682\n",
      "{'Epoch': 375}\n",
      "Epoch 376 Batch 0 Loss -0.3859\n",
      "Epoch 376 Batch 50 Loss -1.0869\n",
      "Epoch 376 Batch 100 Loss -1.0543\n",
      "Epoch 376 Batch 150 Loss -1.0682\n",
      "Epoch 376 Batch 200 Loss -1.1143\n",
      "Epoch 376 Batch 250 Loss -1.1388\n",
      "Epoch 376 Batch 300 Loss -1.1436\n",
      "Epoch 376 Batch 350 Loss -1.1580\n",
      "Epoch 376 Loss -1.1676\n",
      "{'Epoch': 376}\n",
      "Epoch 377 Batch 0 Loss -0.3895\n",
      "Epoch 377 Batch 50 Loss -1.0877\n",
      "Epoch 377 Batch 100 Loss -1.0529\n",
      "Epoch 377 Batch 150 Loss -1.0671\n",
      "Epoch 377 Batch 200 Loss -1.1135\n",
      "Epoch 377 Batch 250 Loss -1.1383\n",
      "Epoch 377 Batch 300 Loss -1.1429\n",
      "Epoch 377 Batch 350 Loss -1.1576\n",
      "Epoch 377 Loss -1.1667\n",
      "{'Epoch': 377}\n",
      "Epoch 378 Batch 0 Loss -0.3874\n",
      "Epoch 378 Batch 50 Loss -1.0875\n",
      "Epoch 378 Batch 100 Loss -1.0530\n",
      "Epoch 378 Batch 150 Loss -1.0670\n",
      "Epoch 378 Batch 200 Loss -1.1139\n",
      "Epoch 378 Batch 250 Loss -1.1387\n",
      "Epoch 378 Batch 300 Loss -1.1436\n",
      "Epoch 378 Batch 350 Loss -1.1580\n",
      "Epoch 378 Loss -1.1674\n",
      "{'Epoch': 378}\n",
      "Epoch 379 Batch 0 Loss -0.3991\n",
      "Epoch 379 Batch 50 Loss -1.0878\n",
      "Epoch 379 Batch 100 Loss -1.0543\n",
      "Epoch 379 Batch 150 Loss -1.0679\n",
      "Epoch 379 Batch 200 Loss -1.1140\n",
      "Epoch 379 Batch 250 Loss -1.1388\n",
      "Epoch 379 Batch 300 Loss -1.1432\n",
      "Epoch 379 Batch 350 Loss -1.1574\n",
      "Epoch 379 Loss -1.1666\n",
      "{'Epoch': 379}\n",
      "Epoch 380 Batch 0 Loss -0.3900\n",
      "Epoch 380 Batch 50 Loss -1.0890\n",
      "Epoch 380 Batch 100 Loss -1.0561\n",
      "Epoch 380 Batch 150 Loss -1.0697\n",
      "Epoch 380 Batch 200 Loss -1.1154\n",
      "Epoch 380 Batch 250 Loss -1.1394\n",
      "Epoch 380 Batch 300 Loss -1.1437\n",
      "Epoch 380 Batch 350 Loss -1.1581\n",
      "Epoch 380 Loss -1.1673\n",
      "{'Epoch': 380}\n",
      "Epoch 381 Batch 0 Loss -0.3953\n",
      "Epoch 381 Batch 50 Loss -1.0888\n",
      "Epoch 381 Batch 100 Loss -1.0468\n",
      "Epoch 381 Batch 150 Loss -1.0299\n",
      "Epoch 381 Batch 200 Loss -1.0736\n",
      "Epoch 381 Batch 250 Loss -1.1000\n",
      "Epoch 381 Batch 300 Loss -1.1139\n",
      "Epoch 381 Batch 350 Loss -1.1332\n",
      "Epoch 381 Loss -1.1452\n",
      "{'Epoch': 381}\n",
      "Epoch 382 Batch 0 Loss -0.3821\n",
      "Epoch 382 Batch 50 Loss -1.0950\n",
      "Epoch 382 Batch 100 Loss -1.0639\n",
      "Epoch 382 Batch 150 Loss -1.0755\n",
      "Epoch 382 Batch 200 Loss -1.1216\n",
      "Epoch 382 Batch 250 Loss -1.1472\n",
      "Epoch 382 Batch 300 Loss -1.1511\n",
      "Epoch 382 Batch 350 Loss -1.1647\n",
      "Epoch 382 Loss -1.1731\n",
      "{'Epoch': 382}\n",
      "Epoch 383 Batch 0 Loss -0.3957\n",
      "Epoch 383 Batch 50 Loss -1.0881\n",
      "Epoch 383 Batch 100 Loss -1.0545\n",
      "Epoch 383 Batch 150 Loss -1.0685\n",
      "Epoch 383 Batch 200 Loss -1.1147\n",
      "Epoch 383 Batch 250 Loss -1.1396\n",
      "Epoch 383 Batch 300 Loss -1.1441\n",
      "Epoch 383 Batch 350 Loss -1.1584\n",
      "Epoch 383 Loss -1.1674\n",
      "{'Epoch': 383}\n",
      "Epoch 384 Batch 0 Loss -0.3906\n",
      "Epoch 384 Batch 50 Loss -1.0889\n",
      "Epoch 384 Batch 100 Loss -1.0512\n",
      "Epoch 384 Batch 150 Loss -1.0309\n",
      "Epoch 384 Batch 200 Loss -1.0767\n",
      "Epoch 384 Batch 250 Loss -1.1049\n",
      "Epoch 384 Batch 300 Loss -1.1144\n",
      "Epoch 384 Batch 350 Loss -1.1305\n",
      "Epoch 384 Loss -1.1416\n",
      "{'Epoch': 384}\n",
      "Epoch 385 Batch 0 Loss -0.3451\n",
      "Epoch 385 Batch 50 Loss -1.0748\n",
      "Epoch 385 Batch 100 Loss -1.0451\n",
      "Epoch 385 Batch 150 Loss -1.0556\n",
      "Epoch 385 Batch 200 Loss -1.0999\n",
      "Epoch 385 Batch 250 Loss -1.1235\n",
      "Epoch 385 Batch 300 Loss -1.1268\n",
      "Epoch 385 Batch 350 Loss -1.1408\n",
      "Epoch 385 Loss -1.1499\n",
      "{'Epoch': 385}\n",
      "Epoch 386 Batch 0 Loss -0.3691\n",
      "Epoch 386 Batch 50 Loss -1.0711\n",
      "Epoch 386 Batch 100 Loss -1.0370\n",
      "Epoch 386 Batch 150 Loss -1.0480\n",
      "Epoch 386 Batch 200 Loss -1.0940\n",
      "Epoch 386 Batch 250 Loss -1.1183\n",
      "Epoch 386 Batch 300 Loss -1.1199\n",
      "Epoch 386 Batch 350 Loss -1.1361\n",
      "Epoch 386 Loss -1.1473\n",
      "{'Epoch': 386}\n",
      "Epoch 387 Batch 0 Loss -0.3912\n",
      "Epoch 387 Batch 50 Loss -1.0864\n",
      "Epoch 387 Batch 100 Loss -1.0583\n",
      "Epoch 387 Batch 150 Loss -1.0730\n",
      "Epoch 387 Batch 200 Loss -1.1192\n",
      "Epoch 387 Batch 250 Loss -1.1444\n",
      "Epoch 387 Batch 300 Loss -1.1483\n",
      "Epoch 387 Batch 350 Loss -1.1608\n",
      "Epoch 387 Loss -1.1701\n",
      "{'Epoch': 387}\n",
      "Epoch 388 Batch 0 Loss -0.3939\n",
      "Epoch 388 Batch 50 Loss -1.0895\n",
      "Epoch 388 Batch 100 Loss -1.0584\n",
      "Epoch 388 Batch 150 Loss -1.0722\n",
      "Epoch 388 Batch 200 Loss -1.1180\n",
      "Epoch 388 Batch 250 Loss -1.1413\n",
      "Epoch 388 Batch 300 Loss -1.1458\n",
      "Epoch 388 Batch 350 Loss -1.1600\n",
      "Epoch 388 Loss -1.1691\n",
      "{'Epoch': 388}\n",
      "Epoch 389 Batch 0 Loss -0.3922\n",
      "Epoch 389 Batch 50 Loss -1.0885\n",
      "Epoch 389 Batch 100 Loss -1.0535\n",
      "Epoch 389 Batch 150 Loss -1.0684\n",
      "Epoch 389 Batch 200 Loss -1.1150\n",
      "Epoch 389 Batch 250 Loss -1.1402\n",
      "Epoch 389 Batch 300 Loss -1.1449\n",
      "Epoch 389 Batch 350 Loss -1.1594\n",
      "Epoch 389 Loss -1.1684\n",
      "{'Epoch': 389}\n",
      "Epoch 390 Batch 0 Loss -0.3945\n",
      "Epoch 390 Batch 50 Loss -1.0903\n",
      "Epoch 390 Batch 100 Loss -1.0560\n",
      "Epoch 390 Batch 150 Loss -1.0702\n",
      "Epoch 390 Batch 200 Loss -1.1164\n",
      "Epoch 390 Batch 250 Loss -1.1411\n",
      "Epoch 390 Batch 300 Loss -1.1453\n",
      "Epoch 390 Batch 350 Loss -1.1593\n",
      "Epoch 390 Loss -1.1685\n",
      "{'Epoch': 390}\n",
      "Epoch 391 Batch 0 Loss -0.3967\n",
      "Epoch 391 Batch 50 Loss -1.0897\n",
      "Epoch 391 Batch 100 Loss -1.0568\n",
      "Epoch 391 Batch 150 Loss -1.0706\n",
      "Epoch 391 Batch 200 Loss -1.1166\n",
      "Epoch 391 Batch 250 Loss -1.1416\n",
      "Epoch 391 Batch 300 Loss -1.1456\n",
      "Epoch 391 Batch 350 Loss -1.1600\n",
      "Epoch 391 Loss -1.1690\n",
      "{'Epoch': 391}\n",
      "Epoch 392 Batch 0 Loss -0.3915\n",
      "Epoch 392 Batch 50 Loss -1.0891\n",
      "Epoch 392 Batch 100 Loss -1.0541\n",
      "Epoch 392 Batch 150 Loss -1.0689\n",
      "Epoch 392 Batch 200 Loss -1.1157\n",
      "Epoch 392 Batch 250 Loss -1.1410\n",
      "Epoch 392 Batch 300 Loss -1.1454\n",
      "Epoch 392 Batch 350 Loss -1.1597\n",
      "Epoch 392 Loss -1.1691\n",
      "{'Epoch': 392}\n",
      "Epoch 393 Batch 0 Loss -0.4030\n",
      "Epoch 393 Batch 50 Loss -1.0901\n",
      "Epoch 393 Batch 100 Loss -1.0559\n",
      "Epoch 393 Batch 150 Loss -1.0697\n",
      "Epoch 393 Batch 200 Loss -1.1160\n",
      "Epoch 393 Batch 250 Loss -1.1414\n",
      "Epoch 393 Batch 300 Loss -1.1448\n",
      "Epoch 393 Batch 350 Loss -1.1592\n",
      "Epoch 393 Loss -1.1685\n",
      "{'Epoch': 393}\n",
      "Epoch 394 Batch 0 Loss -0.3924\n",
      "Epoch 394 Batch 50 Loss -1.0890\n",
      "Epoch 394 Batch 100 Loss -1.0568\n",
      "Epoch 394 Batch 150 Loss -1.0707\n",
      "Epoch 394 Batch 200 Loss -1.1166\n",
      "Epoch 394 Batch 250 Loss -1.1414\n",
      "Epoch 394 Batch 300 Loss -1.1456\n",
      "Epoch 394 Batch 350 Loss -1.1599\n",
      "Epoch 394 Loss -1.1690\n",
      "{'Epoch': 394}\n",
      "Epoch 395 Batch 0 Loss -0.4040\n",
      "Epoch 395 Batch 50 Loss -1.0894\n",
      "Epoch 395 Batch 100 Loss -1.0551\n",
      "Epoch 395 Batch 150 Loss -1.0520\n",
      "Epoch 395 Batch 200 Loss -1.0989\n",
      "Epoch 395 Batch 250 Loss -1.1176\n",
      "Epoch 395 Batch 300 Loss -1.1233\n",
      "Epoch 395 Batch 350 Loss -1.1368\n",
      "Epoch 395 Loss -1.1456\n",
      "{'Epoch': 395}\n",
      "Epoch 396 Batch 0 Loss -0.3740\n",
      "Epoch 396 Batch 50 Loss -1.0718\n",
      "Epoch 396 Batch 100 Loss -1.0396\n",
      "Epoch 396 Batch 150 Loss -1.0520\n",
      "Epoch 396 Batch 200 Loss -1.0970\n",
      "Epoch 396 Batch 250 Loss -1.1215\n",
      "Epoch 396 Batch 300 Loss -1.1236\n",
      "Epoch 396 Batch 350 Loss -1.1377\n",
      "Epoch 396 Loss -1.1467\n",
      "{'Epoch': 396}\n",
      "Epoch 397 Batch 0 Loss -0.3660\n",
      "Epoch 397 Batch 50 Loss -1.0714\n",
      "Epoch 397 Batch 100 Loss -1.0410\n",
      "Epoch 397 Batch 150 Loss -1.0531\n",
      "Epoch 397 Batch 200 Loss -1.0981\n",
      "Epoch 397 Batch 250 Loss -1.1218\n",
      "Epoch 397 Batch 300 Loss -1.1276\n",
      "Epoch 397 Batch 350 Loss -1.1415\n",
      "Epoch 397 Loss -1.1506\n",
      "{'Epoch': 397}\n",
      "Epoch 398 Batch 0 Loss -0.3646\n",
      "Epoch 398 Batch 50 Loss -1.0675\n",
      "Epoch 398 Batch 100 Loss -1.0376\n",
      "Epoch 398 Batch 150 Loss -1.0479\n",
      "Epoch 398 Batch 200 Loss -1.0940\n",
      "Epoch 398 Batch 250 Loss -1.1189\n",
      "Epoch 398 Batch 300 Loss -1.1142\n",
      "Epoch 398 Batch 350 Loss -1.1313\n",
      "Epoch 398 Loss -1.1432\n",
      "{'Epoch': 398}\n",
      "Epoch 399 Batch 0 Loss -0.4085\n",
      "Epoch 399 Batch 50 Loss -1.0916\n",
      "Epoch 399 Batch 100 Loss -1.0641\n",
      "Epoch 399 Batch 150 Loss -1.0785\n",
      "Epoch 399 Batch 200 Loss -1.1259\n",
      "Epoch 399 Batch 250 Loss -1.1518\n",
      "Epoch 399 Batch 300 Loss -1.1567\n",
      "Epoch 399 Batch 350 Loss -1.1700\n",
      "Epoch 399 Loss -1.1790\n",
      "{'Epoch': 399}\n",
      "Epoch 400 Batch 0 Loss -0.4026\n",
      "Epoch 400 Batch 50 Loss -1.0890\n",
      "Epoch 400 Batch 100 Loss -1.0602\n",
      "Epoch 400 Batch 150 Loss -1.0741\n",
      "Epoch 400 Batch 200 Loss -1.1199\n",
      "Epoch 400 Batch 250 Loss -1.1440\n",
      "Epoch 400 Batch 300 Loss -1.1485\n",
      "Epoch 400 Batch 350 Loss -1.1627\n",
      "Epoch 400 Loss -1.1715\n",
      "{'Epoch': 400}\n",
      "Epoch 401 Batch 0 Loss -0.3923\n",
      "Epoch 401 Batch 50 Loss -1.0861\n",
      "Epoch 401 Batch 100 Loss -1.0564\n",
      "Epoch 401 Batch 150 Loss -1.0706\n",
      "Epoch 401 Batch 200 Loss -1.1158\n",
      "Epoch 401 Batch 250 Loss -1.1412\n",
      "Epoch 401 Batch 300 Loss -1.1465\n",
      "Epoch 401 Batch 350 Loss -1.1612\n",
      "Epoch 401 Loss -1.1707\n",
      "{'Epoch': 401}\n",
      "Epoch 402 Batch 0 Loss -0.3980\n",
      "Epoch 402 Batch 50 Loss -1.0871\n",
      "Epoch 402 Batch 100 Loss -1.0585\n",
      "Epoch 402 Batch 150 Loss -1.0731\n",
      "Epoch 402 Batch 200 Loss -1.1187\n",
      "Epoch 402 Batch 250 Loss -1.1432\n",
      "Epoch 402 Batch 300 Loss -1.1478\n",
      "Epoch 402 Batch 350 Loss -1.1622\n",
      "Epoch 402 Loss -1.1716\n",
      "{'Epoch': 402}\n",
      "Epoch 403 Batch 0 Loss -0.3978\n",
      "Epoch 403 Batch 50 Loss -1.0883\n",
      "Epoch 403 Batch 100 Loss -1.0587\n",
      "Epoch 403 Batch 150 Loss -1.0735\n",
      "Epoch 403 Batch 200 Loss -1.1190\n",
      "Epoch 403 Batch 250 Loss -1.1413\n",
      "Epoch 403 Batch 300 Loss -1.1472\n",
      "Epoch 403 Batch 350 Loss -1.1617\n",
      "Epoch 403 Loss -1.1713\n",
      "{'Epoch': 403}\n",
      "Epoch 404 Batch 0 Loss -0.3904\n",
      "Epoch 404 Batch 50 Loss -1.0898\n",
      "Epoch 404 Batch 100 Loss -1.0607\n",
      "Epoch 404 Batch 150 Loss -1.0747\n",
      "Epoch 404 Batch 200 Loss -1.1207\n",
      "Epoch 404 Batch 250 Loss -1.1448\n",
      "Epoch 404 Batch 300 Loss -1.1499\n",
      "Epoch 404 Batch 350 Loss -1.1641\n",
      "Epoch 404 Loss -1.1734\n",
      "{'Epoch': 404}\n",
      "Epoch 405 Batch 0 Loss -0.3995\n",
      "Epoch 405 Batch 50 Loss -1.0869\n",
      "Epoch 405 Batch 100 Loss -1.0589\n",
      "Epoch 405 Batch 150 Loss -1.0731\n",
      "Epoch 405 Batch 200 Loss -1.1192\n",
      "Epoch 405 Batch 250 Loss -1.1439\n",
      "Epoch 405 Batch 300 Loss -1.1468\n",
      "Epoch 405 Batch 350 Loss -1.1591\n",
      "Epoch 405 Loss -1.1689\n",
      "{'Epoch': 405}\n",
      "Epoch 406 Batch 0 Loss -0.3926\n",
      "Epoch 406 Batch 50 Loss -1.0944\n",
      "Epoch 406 Batch 100 Loss -1.0631\n",
      "Epoch 406 Batch 150 Loss -1.0768\n",
      "Epoch 406 Batch 200 Loss -1.1224\n",
      "Epoch 406 Batch 250 Loss -1.1465\n",
      "Epoch 406 Batch 300 Loss -1.1506\n",
      "Epoch 406 Batch 350 Loss -1.1646\n",
      "Epoch 406 Loss -1.1737\n",
      "{'Epoch': 406}\n",
      "Epoch 407 Batch 0 Loss -0.4066\n",
      "Epoch 407 Batch 50 Loss -1.0915\n",
      "Epoch 407 Batch 100 Loss -1.0572\n",
      "Epoch 407 Batch 150 Loss -1.0715\n",
      "Epoch 407 Batch 200 Loss -1.1181\n",
      "Epoch 407 Batch 250 Loss -1.1434\n",
      "Epoch 407 Batch 300 Loss -1.1480\n",
      "Epoch 407 Batch 350 Loss -1.1622\n",
      "Epoch 407 Loss -1.1713\n",
      "{'Epoch': 407}\n",
      "Epoch 408 Batch 0 Loss -0.4029\n",
      "Epoch 408 Batch 50 Loss -1.0914\n",
      "Epoch 408 Batch 100 Loss -1.0590\n",
      "Epoch 408 Batch 150 Loss -1.0730\n",
      "Epoch 408 Batch 200 Loss -1.1194\n",
      "Epoch 408 Batch 250 Loss -1.1446\n",
      "Epoch 408 Batch 300 Loss -1.1488\n",
      "Epoch 408 Batch 350 Loss -1.1626\n",
      "Epoch 408 Loss -1.1719\n",
      "{'Epoch': 408}\n",
      "Epoch 409 Batch 0 Loss -0.4003\n",
      "Epoch 409 Batch 50 Loss -1.0921\n",
      "Epoch 409 Batch 100 Loss -1.0595\n",
      "Epoch 409 Batch 150 Loss -1.0736\n",
      "Epoch 409 Batch 200 Loss -1.1199\n",
      "Epoch 409 Batch 250 Loss -1.1451\n",
      "Epoch 409 Batch 300 Loss -1.1491\n",
      "Epoch 409 Batch 350 Loss -1.1632\n",
      "Epoch 409 Loss -1.1724\n",
      "{'Epoch': 409}\n",
      "Epoch 410 Batch 0 Loss -0.3941\n",
      "Epoch 410 Batch 50 Loss -1.0911\n",
      "Epoch 410 Batch 100 Loss -1.0585\n",
      "Epoch 410 Batch 150 Loss -1.0726\n",
      "Epoch 410 Batch 200 Loss -1.1192\n",
      "Epoch 410 Batch 250 Loss -1.1442\n",
      "Epoch 410 Batch 300 Loss -1.1480\n",
      "Epoch 410 Batch 350 Loss -1.1609\n",
      "Epoch 410 Loss -1.1703\n",
      "{'Epoch': 410}\n",
      "Epoch 411 Batch 0 Loss -0.3959\n",
      "Epoch 411 Batch 50 Loss -1.0937\n",
      "Epoch 411 Batch 100 Loss -1.0613\n",
      "Epoch 411 Batch 150 Loss -1.0751\n",
      "Epoch 411 Batch 200 Loss -1.1214\n",
      "Epoch 411 Batch 250 Loss -1.1459\n",
      "Epoch 411 Batch 300 Loss -1.1498\n",
      "Epoch 411 Batch 350 Loss -1.1640\n",
      "Epoch 411 Loss -1.1731\n",
      "{'Epoch': 411}\n",
      "Epoch 412 Batch 0 Loss -0.4088\n",
      "Epoch 412 Batch 50 Loss -1.0912\n",
      "Epoch 412 Batch 100 Loss -1.0581\n",
      "Epoch 412 Batch 150 Loss -1.0721\n",
      "Epoch 412 Batch 200 Loss -1.1184\n",
      "Epoch 412 Batch 250 Loss -1.1440\n",
      "Epoch 412 Batch 300 Loss -1.1485\n",
      "Epoch 412 Batch 350 Loss -1.1628\n",
      "Epoch 412 Loss -1.1720\n",
      "{'Epoch': 412}\n",
      "Epoch 413 Batch 0 Loss -0.3993\n",
      "Epoch 413 Batch 50 Loss -1.0916\n",
      "Epoch 413 Batch 100 Loss -1.0598\n",
      "Epoch 413 Batch 150 Loss -1.0738\n",
      "Epoch 413 Batch 200 Loss -1.1202\n",
      "Epoch 413 Batch 250 Loss -1.1452\n",
      "Epoch 413 Batch 300 Loss -1.1487\n",
      "Epoch 413 Batch 350 Loss -1.1628\n",
      "Epoch 413 Loss -1.1720\n",
      "{'Epoch': 413}\n",
      "Epoch 414 Batch 0 Loss -0.4014\n",
      "Epoch 414 Batch 50 Loss -1.0935\n",
      "Epoch 414 Batch 100 Loss -1.0589\n",
      "Epoch 414 Batch 150 Loss -1.0732\n",
      "Epoch 414 Batch 200 Loss -1.1196\n",
      "Epoch 414 Batch 250 Loss -1.1450\n",
      "Epoch 414 Batch 300 Loss -1.1492\n",
      "Epoch 414 Batch 350 Loss -1.1635\n",
      "Epoch 414 Loss -1.1728\n",
      "{'Epoch': 414}\n",
      "Epoch 415 Batch 0 Loss -0.3981\n",
      "Epoch 415 Batch 50 Loss -1.0922\n",
      "Epoch 415 Batch 100 Loss -1.0549\n",
      "Epoch 415 Batch 150 Loss -1.0705\n",
      "Epoch 415 Batch 200 Loss -1.1177\n",
      "Epoch 415 Batch 250 Loss -1.1431\n",
      "Epoch 415 Batch 300 Loss -1.1475\n",
      "Epoch 415 Batch 350 Loss -1.1622\n",
      "Epoch 415 Loss -1.1716\n",
      "{'Epoch': 415}\n",
      "Epoch 416 Batch 0 Loss -0.4146\n",
      "Epoch 416 Batch 50 Loss -1.0924\n",
      "Epoch 416 Batch 100 Loss -1.0570\n",
      "Epoch 416 Batch 150 Loss -1.0716\n",
      "Epoch 416 Batch 200 Loss -1.1189\n",
      "Epoch 416 Batch 250 Loss -1.1440\n",
      "Epoch 416 Batch 300 Loss -1.1480\n",
      "Epoch 416 Batch 350 Loss -1.1617\n",
      "Epoch 416 Loss -1.1706\n",
      "{'Epoch': 416}\n",
      "Epoch 417 Batch 0 Loss -0.4014\n",
      "Epoch 417 Batch 50 Loss -1.0937\n",
      "Epoch 417 Batch 100 Loss -1.0609\n",
      "Epoch 417 Batch 150 Loss -1.0745\n",
      "Epoch 417 Batch 200 Loss -1.1208\n",
      "Epoch 417 Batch 250 Loss -1.1458\n",
      "Epoch 417 Batch 300 Loss -1.1498\n",
      "Epoch 417 Batch 350 Loss -1.1638\n",
      "Epoch 417 Loss -1.1728\n",
      "{'Epoch': 417}\n",
      "Epoch 418 Batch 0 Loss -0.4062\n",
      "Epoch 418 Batch 50 Loss -1.0926\n",
      "Epoch 418 Batch 100 Loss -1.0605\n",
      "Epoch 418 Batch 150 Loss -1.0733\n",
      "Epoch 418 Batch 200 Loss -1.1196\n",
      "Epoch 418 Batch 250 Loss -1.1438\n",
      "Epoch 418 Batch 300 Loss -1.1481\n",
      "Epoch 418 Batch 350 Loss -1.1625\n",
      "Epoch 418 Loss -1.1717\n",
      "{'Epoch': 418}\n",
      "Epoch 419 Batch 0 Loss -0.3953\n",
      "Epoch 419 Batch 50 Loss -1.0936\n",
      "Epoch 419 Batch 100 Loss -1.0592\n",
      "Epoch 419 Batch 150 Loss -1.0731\n",
      "Epoch 419 Batch 200 Loss -1.1191\n",
      "Epoch 419 Batch 250 Loss -1.1446\n",
      "Epoch 419 Batch 300 Loss -1.1484\n",
      "Epoch 419 Batch 350 Loss -1.1629\n",
      "Epoch 419 Loss -1.1722\n",
      "{'Epoch': 419}\n",
      "Epoch 420 Batch 0 Loss -0.3960\n",
      "Epoch 420 Batch 50 Loss -1.0918\n",
      "Epoch 420 Batch 100 Loss -1.0602\n",
      "Epoch 420 Batch 150 Loss -1.0738\n",
      "Epoch 420 Batch 200 Loss -1.1202\n",
      "Epoch 420 Batch 250 Loss -1.1447\n",
      "Epoch 420 Batch 300 Loss -1.1491\n",
      "Epoch 420 Batch 350 Loss -1.1632\n",
      "Epoch 420 Loss -1.1722\n",
      "{'Epoch': 420}\n",
      "Epoch 421 Batch 0 Loss -0.4006\n",
      "Epoch 421 Batch 50 Loss -1.0935\n",
      "Epoch 421 Batch 100 Loss -1.0583\n",
      "Epoch 421 Batch 150 Loss -1.0723\n",
      "Epoch 421 Batch 200 Loss -1.1194\n",
      "Epoch 421 Batch 250 Loss -1.1449\n",
      "Epoch 421 Batch 300 Loss -1.1488\n",
      "Epoch 421 Batch 350 Loss -1.1632\n",
      "Epoch 421 Loss -1.1724\n",
      "{'Epoch': 421}\n",
      "Epoch 422 Batch 0 Loss -0.4106\n",
      "Epoch 422 Batch 50 Loss -1.0937\n",
      "Epoch 422 Batch 100 Loss -1.0599\n",
      "Epoch 422 Batch 150 Loss -1.0739\n",
      "Epoch 422 Batch 200 Loss -1.1202\n",
      "Epoch 422 Batch 250 Loss -1.1449\n",
      "Epoch 422 Batch 300 Loss -1.1488\n",
      "Epoch 422 Batch 350 Loss -1.1632\n",
      "Epoch 422 Loss -1.1725\n",
      "{'Epoch': 422}\n",
      "Epoch 423 Batch 0 Loss -0.4038\n",
      "Epoch 423 Batch 50 Loss -1.0941\n",
      "Epoch 423 Batch 100 Loss -1.0612\n",
      "Epoch 423 Batch 150 Loss -1.0747\n",
      "Epoch 423 Batch 200 Loss -1.1212\n",
      "Epoch 423 Batch 250 Loss -1.1462\n",
      "Epoch 423 Batch 300 Loss -1.1494\n",
      "Epoch 423 Batch 350 Loss -1.1637\n",
      "Epoch 423 Loss -1.1728\n",
      "{'Epoch': 423}\n",
      "Epoch 424 Batch 0 Loss -0.3995\n",
      "Epoch 424 Batch 50 Loss -1.0942\n",
      "Epoch 424 Batch 100 Loss -1.0597\n",
      "Epoch 424 Batch 150 Loss -1.0742\n",
      "Epoch 424 Batch 200 Loss -1.1205\n",
      "Epoch 424 Batch 250 Loss -1.1454\n",
      "Epoch 424 Batch 300 Loss -1.1497\n",
      "Epoch 424 Batch 350 Loss -1.1630\n",
      "Epoch 424 Loss -1.1720\n",
      "{'Epoch': 424}\n",
      "Epoch 425 Batch 0 Loss -0.4018\n",
      "Epoch 425 Batch 50 Loss -1.0942\n",
      "Epoch 425 Batch 100 Loss -1.0583\n",
      "Epoch 425 Batch 150 Loss -1.0789\n",
      "Epoch 425 Batch 200 Loss -1.1302\n",
      "Epoch 425 Batch 250 Loss -1.1391\n",
      "Epoch 425 Batch 300 Loss -1.1424\n",
      "Epoch 425 Batch 350 Loss -1.1533\n",
      "Epoch 425 Loss -1.1622\n",
      "{'Epoch': 425}\n",
      "Epoch 426 Batch 0 Loss -0.2534\n",
      "Epoch 426 Batch 50 Loss -1.0636\n",
      "Epoch 426 Batch 100 Loss -1.0396\n",
      "Epoch 426 Batch 150 Loss -1.0522\n",
      "Epoch 426 Batch 200 Loss -1.1003\n",
      "Epoch 426 Batch 250 Loss -1.1242\n",
      "Epoch 426 Batch 300 Loss -1.1270\n",
      "Epoch 426 Batch 350 Loss -1.1413\n",
      "Epoch 426 Loss -1.1510\n",
      "{'Epoch': 426}\n",
      "Epoch 427 Batch 0 Loss -0.3744\n",
      "Epoch 427 Batch 50 Loss -1.0767\n",
      "Epoch 427 Batch 100 Loss -1.0431\n",
      "Epoch 427 Batch 150 Loss -1.0537\n",
      "Epoch 427 Batch 200 Loss -1.1002\n",
      "Epoch 427 Batch 250 Loss -1.1250\n",
      "Epoch 427 Batch 300 Loss -1.1272\n",
      "Epoch 427 Batch 350 Loss -1.1407\n",
      "Epoch 427 Loss -1.1492\n",
      "{'Epoch': 427}\n",
      "Epoch 428 Batch 0 Loss -0.3716\n",
      "Epoch 428 Batch 50 Loss -1.0736\n",
      "Epoch 428 Batch 100 Loss -1.0408\n",
      "Epoch 428 Batch 150 Loss -1.0543\n",
      "Epoch 428 Batch 200 Loss -1.1001\n",
      "Epoch 428 Batch 250 Loss -1.1258\n",
      "Epoch 428 Batch 300 Loss -1.1303\n",
      "Epoch 428 Batch 350 Loss -1.1436\n",
      "Epoch 428 Loss -1.1525\n",
      "{'Epoch': 428}\n",
      "Epoch 429 Batch 0 Loss -0.3747\n",
      "Epoch 429 Batch 50 Loss -1.0718\n",
      "Epoch 429 Batch 100 Loss -1.0415\n",
      "Epoch 429 Batch 150 Loss -1.0552\n",
      "Epoch 429 Batch 200 Loss -1.1012\n",
      "Epoch 429 Batch 250 Loss -1.1248\n",
      "Epoch 429 Batch 300 Loss -1.1310\n",
      "Epoch 429 Batch 350 Loss -1.1446\n",
      "Epoch 429 Loss -1.1537\n",
      "{'Epoch': 429}\n",
      "Epoch 430 Batch 0 Loss -0.3687\n",
      "Epoch 430 Batch 50 Loss -1.0725\n",
      "Epoch 430 Batch 100 Loss -1.0427\n",
      "Epoch 430 Batch 150 Loss -1.0521\n",
      "Epoch 430 Batch 200 Loss -1.0989\n",
      "Epoch 430 Batch 250 Loss -1.1249\n",
      "Epoch 430 Batch 300 Loss -1.1262\n",
      "Epoch 430 Batch 350 Loss -1.1398\n",
      "Epoch 430 Loss -1.1498\n",
      "{'Epoch': 430}\n",
      "Epoch 431 Batch 0 Loss -0.3680\n",
      "Epoch 431 Batch 50 Loss -1.0807\n",
      "Epoch 431 Batch 100 Loss -1.0463\n",
      "Epoch 431 Batch 150 Loss -1.0574\n",
      "Epoch 431 Batch 200 Loss -1.1035\n",
      "Epoch 431 Batch 250 Loss -1.1271\n",
      "Epoch 431 Batch 300 Loss -1.1296\n",
      "Epoch 431 Batch 350 Loss -1.1430\n",
      "Epoch 431 Loss -1.1517\n",
      "{'Epoch': 431}\n",
      "Epoch 432 Batch 0 Loss -0.3595\n",
      "Epoch 432 Batch 50 Loss -1.0758\n",
      "Epoch 432 Batch 100 Loss -1.0426\n",
      "Epoch 432 Batch 150 Loss -1.0566\n",
      "Epoch 432 Batch 200 Loss -1.1026\n",
      "Epoch 432 Batch 250 Loss -1.1283\n",
      "Epoch 432 Batch 300 Loss -1.1324\n",
      "Epoch 432 Batch 350 Loss -1.1453\n",
      "Epoch 432 Loss -1.1541\n",
      "{'Epoch': 432}\n",
      "Epoch 433 Batch 0 Loss -0.3757\n",
      "Epoch 433 Batch 50 Loss -1.0756\n",
      "Epoch 433 Batch 100 Loss -1.0438\n",
      "Epoch 433 Batch 150 Loss -1.0571\n",
      "Epoch 433 Batch 200 Loss -1.1028\n",
      "Epoch 433 Batch 250 Loss -1.1276\n",
      "Epoch 433 Batch 300 Loss -1.1334\n",
      "Epoch 433 Batch 350 Loss -1.1468\n",
      "Epoch 433 Loss -1.1560\n",
      "{'Epoch': 433}\n",
      "Epoch 434 Batch 0 Loss -0.3721\n",
      "Epoch 434 Batch 50 Loss -1.0730\n",
      "Epoch 434 Batch 100 Loss -1.0432\n",
      "Epoch 434 Batch 150 Loss -1.0521\n",
      "Epoch 434 Batch 200 Loss -1.0988\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/don/Writing-Transformer/Writing-Transformer.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/don/Writing-Transformer/Writing-Transformer.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m train_loss\u001b[39m.\u001b[39mreset_states()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/don/Writing-Transformer/Writing-Transformer.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m({\u001b[39m\"\u001b[39m\u001b[39mEpoch\u001b[39m\u001b[39m\"\u001b[39m: step})\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/don/Writing-Transformer/Writing-Transformer.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m (batch, (b, _)) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data_iter):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/don/Writing-Transformer/Writing-Transformer.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m   params, opt_state, loss, rng \u001b[39m=\u001b[39m update(params, rng, opt_state, b)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/don/Writing-Transformer/Writing-Transformer.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m   train_loss(loss)\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py:4635\u001b[0m, in \u001b[0;36m_NumpyIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4632\u001b[0m     numpy\u001b[39m.\u001b[39msetflags(write\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   4633\u001b[0m   \u001b[39mreturn\u001b[39;00m numpy\n\u001b[0;32m-> 4635\u001b[0m \u001b[39mreturn\u001b[39;00m nest\u001b[39m.\u001b[39mmap_structure(to_numpy, \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator))\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:766\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    765\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 766\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_internal()\n\u001b[1;32m    767\u001b[0m   \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    768\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py:749\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[39m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[39m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[39mwith\u001b[39;00m context\u001b[39m.\u001b[39mexecution_mode(context\u001b[39m.\u001b[39mSYNC):\n\u001b[0;32m--> 749\u001b[0m   ret \u001b[39m=\u001b[39m gen_dataset_ops\u001b[39m.\u001b[39;49miterator_get_next(\n\u001b[1;32m    750\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_iterator_resource,\n\u001b[1;32m    751\u001b[0m       output_types\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_types,\n\u001b[1;32m    752\u001b[0m       output_shapes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_output_shapes)\n\u001b[1;32m    754\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    755\u001b[0m     \u001b[39m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    756\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_element_spec\u001b[39m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3012\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3010\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[1;32m   3011\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3012\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[1;32m   3013\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mIteratorGetNext\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, iterator, \u001b[39m\"\u001b[39;49m\u001b[39moutput_types\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_types,\n\u001b[1;32m   3014\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39moutput_shapes\u001b[39;49m\u001b[39m\"\u001b[39;49m, output_shapes)\n\u001b[1;32m   3015\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[1;32m   3016\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#EPOCHS = 200\n",
    "\n",
    "#opt_state = optimiser.init(params)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "  data_iter = train.batched_onehot_set.prefetch(tf.data.experimental.AUTOTUNE).as_numpy_iterator()\n",
    "\n",
    "  train_loss.reset_states()\n",
    "\n",
    "  print({\"Epoch\": step})\n",
    "  for (batch, (b, _)) in enumerate(data_iter):\n",
    "    params, opt_state, loss, rng = update(params, rng, opt_state, b)\n",
    "\n",
    "    train_loss(loss)\n",
    "\n",
    "    if batch % 50 == 0:\n",
    "      print(f'Epoch {step + 1} Batch {batch} Loss {train_loss.result():.4f}')\n",
    "\n",
    "  print(f'Epoch {step + 1} Loss {train_loss.result():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need our own sampling function in this case\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "\n",
    "def fill_diagonal(a, val):\n",
    "  assert a.ndim >= 2\n",
    "  i, j = jnp.diag_indices(min(a.shape[-2:]))\n",
    "  return a.at[..., i, j].set(val)\n",
    "\n",
    "# sample the mixture model\n",
    "# input: res (mixture_components)\n",
    "#        b (temperature)\n",
    "# output: sample, pis, mean, variance\n",
    "def sample_mix_model(res, rng, b):\n",
    "      #print(res.shape)\n",
    "      \n",
    "      pis, mu, sig, rho, eos = jnp.array_split(res, [NUM_MIX_COM, NUM_MIX_COM*3, NUM_MIX_COM*5, NUM_MIX_COM*6], axis=-1)\n",
    "\n",
    "      # weights - must be a probability distribution so softmax over all components\n",
    "      pis = jax.nn.softmax(pis * (1.+b))\n",
    "\n",
    "\n",
    "      # means - no transformation needed\n",
    "      mu_x1, mu_x2 = jnp.array_split(mu, 2, axis=-1)\n",
    "      \n",
    "      # standard deviations - must be strictly positive so exponent\n",
    "      sig = jnp.exp(sig - b)\n",
    "\n",
    "      sig_x1, sig_x2 = jnp.array_split(sig, 2, axis=-1)\n",
    "            \n",
    "      # correlations - squish to -1 to 1 with tanh activation\n",
    "      rho = jnp.tanh(rho)\n",
    "\n",
    "      a = jnp.zeros((NUM_MIX_COM, 2, 2))\n",
    "\n",
    "      S = fill_diagonal(a, jnp.stack([sig_x1, sig_x2], axis=-1))\n",
    "\n",
    "      #print(S)\n",
    "\n",
    "      #print(S.shape)\n",
    "\n",
    "      #E = jnp.eye(2, batch_shape=[NUM_MIX_COM])\n",
    "      E = jnp.repeat(jnp.eye(2)[None, :], NUM_MIX_COM, axis=0)\n",
    "\n",
    "      rho_exp = jnp.reshape(jnp.repeat(rho, 4), [NUM_MIX_COM, 2, 2])\n",
    "    \n",
    "      corr_mat = jnp.where(jnp.equal(E, 1.), E, rho_exp)\n",
    "      \n",
    "      cov_mat = jnp.matmul(S, corr_mat)\n",
    "      cov_mat = jnp.matmul(cov_mat, S)\n",
    "\n",
    "      #print(cov_mat)\n",
    "\n",
    "      #print(cov_mat.shape)\n",
    "\n",
    "      # The distribution is a mixture of gaussians\n",
    "      gm = tfp.distributions.MixtureSameFamily(mixture_distribution=tfp.distributions.Categorical(probs=pis),\n",
    "            components_distribution=tfp.distributions.MultivariateNormalTriL(loc=jnp.stack([mu_x1, mu_x2], axis=-1),\n",
    "                                                                    scale_tril=jax.lax.linalg.cholesky(cov_mat)))\n",
    "\n",
    "      # End of stroke\n",
    "      eos = jax.nn.sigmoid(eos)\n",
    "      \n",
    "      bd = tfp.distributions.Bernoulli(probs=eos, dtype=float)\n",
    "\n",
    "      #print(bd.sample(seed=jax.random.PRNGKey(seed=1)))\n",
    "      \n",
    "      rng, new_rng = jax.random.split(rng)\n",
    "\n",
    "      bd_samp = bd.sample(seed=new_rng)\n",
    "      \n",
    "      #print(tf.concat([gm.sample(), eos], axis=-1))\n",
    "\n",
    "      rng, new_rng = jax.random.split(rng)\n",
    "      \n",
    "      gm_samp = gm.sample(seed=new_rng)\n",
    "\n",
    "      return jnp.hstack((gm_samp, bd_samp, gm.mean(), gm.covariance().ravel(), pis)), rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-1.8900048e-02, -3.3042753e-01,  0.0000000e+00,\n",
       "             -1.8898826e-02, -3.2954726e-01,  2.7309941e-05,\n",
       "             -6.3517196e-05, -6.3517196e-05,  3.7825466e-04,\n",
       "              4.3236162e-04,  1.2162946e-10,  1.2430813e-13,\n",
       "              5.7854582e-20,  4.3210040e-11,  9.7065145e-10,\n",
       "              1.6838317e-27,  2.7926050e-09,  9.9632746e-01,\n",
       "              1.1438686e-16,  2.2631118e-04,  6.9324026e-12,\n",
       "              1.0700577e-06,  5.8040485e-05,  1.0115902e-20,\n",
       "              2.9548528e-03,  5.8673964e-15,  8.0648543e-21,\n",
       "              8.7738051e-14,  5.4580181e-20], dtype=float32)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=10\n",
    "\n",
    "rng = jax.random.PRNGKey(5)\n",
    "rng, new_rng = jax.random.split(rng)\n",
    "dec_input = jnp.zeros((1, 1, 3))\n",
    "predictions_all = writing_transformer.apply(params, new_rng, one_hot_sentence, dec_input, False)\n",
    "\n",
    "predictions_all[0, -1, :]\n",
    "\n",
    "predictions, rng = sample_mix_model(predictions_all[0, -1, :], rng, b)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sample_mix_model() missing 1 required positional argument: 'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [257], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m42\u001b[39m) \n\u001b[1;32m      6\u001b[0m predictions_all \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mapply(params, key, one_hot_sentence, dec_input)\n\u001b[0;32m----> 8\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43msample_mix_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions_all\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m pred_strokes \u001b[38;5;241m=\u001b[39m predictions[:\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     11\u001b[0m pred_strokes \u001b[38;5;241m=\u001b[39m pred_strokes[jnp\u001b[38;5;241m.\u001b[39mnewaxis, jnp\u001b[38;5;241m.\u001b[39mnewaxis, :]\n",
      "\u001b[0;31mTypeError\u001b[0m: sample_mix_model() missing 1 required positional argument: 'b'"
     ]
    }
   ],
   "source": [
    "# Test the sample function\n",
    "dec_input = jnp.zeros((1, 1, 3))\n",
    "one_hot_sentence = convert_sentence(encoding_sent)\n",
    "\n",
    "key = jax.random.PRNGKey(42) \n",
    "predictions_all = network.apply(params, key, one_hot_sentence, dec_input)\n",
    "\n",
    "predictions = sample_mix_model(predictions_all[0, -1, :], 1)\n",
    "\n",
    "pred_strokes = predictions[:3]\n",
    "pred_strokes = pred_strokes[jnp.newaxis, jnp.newaxis, :]\n",
    "\n",
    "pred_strokes.shape\n",
    "\n",
    "dec_input = jax.lax.concatenate([dec_input, pred_strokes], 1)\n",
    "#dec_input = jnp.stack([dec_input, pred_strokes])\n",
    "\n",
    "dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def fill_diagonal(a, val):\n",
    "  assert a.ndim >= 2\n",
    "  i, j = jnp.diag_indices(min(a.shape[-2:]))\n",
    "  return a.at[..., i, j].set(val)\n",
    "\n",
    "a = jnp.zeros((2, 3, 4, 4))\n",
    "\n",
    "# works for scalars\n",
    "a1 = fill_diagonal(a, 2)\n",
    "\n",
    "# or for batched vectors\n",
    "a2 = fill_diagonal(a, jnp.arange(24).reshape(2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4, 4)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need an evaluate function that will take in a character sequence and \n",
    "# generate some writing\n",
    "\n",
    "# Convert a sentence to a one-hot-encoded vector\n",
    "def convert_sentence(sentence):\n",
    "  # Convert it to a one-hot encoded vector for the encoder\n",
    "  U_conv = tf.keras.backend.one_hot(train.text_to_int(sentence), len(train.vocab)+1)\n",
    "  #U_conv = train.text_to_int(sentence)\n",
    "  # Pad it to match the original data that was input into the encoder\n",
    "  U_conv = tf.keras.preprocessing.sequence.pad_sequences([U_conv],\n",
    "                                                         maxlen=train.MAX_CHAR_SEQ_LEN,\n",
    "                                                         padding='post',\n",
    "                                                         value=train.char_padding_value);\n",
    "  #U_conv = tf.convert_to_tensor(U_conv, dtype='float32')\n",
    "\n",
    "  return jnp.asarray(U_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20, 101)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_sent = 'Hello World!'\n",
    "\n",
    "one_hot_sentence = convert_sentence(encoding_sent)\n",
    "\n",
    "one_hot_sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@hk.transform\n",
    "def writing_transformer(inp: jnp.ndarray, tar: jnp.ndarray, training: bool) -> jnp.ndarray:\n",
    "    tra = Writing_Transformer(num_layers, key_size, d_model, num_heads, dff, pe_encoding=250, pe_target=1000, dropout_rate=dropout_rate)\n",
    "\n",
    "    return tra(inp, tar, training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "output_name = \"test_attention.mp4\"\n",
    "\n",
    "b = 20.0\n",
    "\n",
    "def evaluate(U_conv):\n",
    "  #gen_sequence = np.zeros((1, 3))\n",
    "\n",
    "  dec_input = jnp.zeros((1, 1, 3))\n",
    "\n",
    "  MAX_LEN = 250\n",
    "\n",
    " # transformer.reset_states()\n",
    "\n",
    "  #fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    " # ims = []\n",
    "\n",
    "  rng = jax.random.PRNGKey(5) \n",
    "\n",
    "#  for t in range(int(train.MAX_STROKE_LEN)):\n",
    "  for t in range(int(MAX_LEN)):\n",
    "    # Create masks.  Even in the inference stage we may create input that is \n",
    "    # padded, such as the one-hot_sentence, and we always need a look-ahead \n",
    "    # mask\n",
    "    #enc_padding_mask, combined_mask, dec_padding_mask = create_masks(U_conv, dec_input)\n",
    "\n",
    "    rng, new_rng = jax.random.split(rng)\n",
    "\n",
    "    predictions_all = writing_transformer.apply(params, new_rng, U_conv, dec_input, False)\n",
    "\n",
    "    #rng = new_rng\n",
    "\n",
    "    #predictions_all = transformer(U_conv,\n",
    "    #                                                     dec_input,\n",
    "     #                                                    False,\n",
    "     #                                                    enc_padding_mask,\n",
    "     #                                                    combined_mask,\n",
    "     #                                                    dec_padding_mask)\n",
    "\n",
    "    #data = tf.squeeze(attention_weights['decoder_layer1_block2'], 0)[0]\n",
    "  \n",
    "    #data_all = np.zeros((MAX_LEN, 20))\n",
    "\n",
    "    #data_all[:data.shape[0], :] = data\n",
    "\n",
    "    #ax = fig.add_subplot(1, 1, 1)\n",
    "    #im = plt.imshow(data_all, cmap='viridis', interpolation='nearest', aspect='auto', animated=True)\n",
    "\n",
    "    #ax = plt.gca()\n",
    "\n",
    "    #labels = 'Eye tracking....'\n",
    "\n",
    "    #ax.set_xticks(range(0, train.MAX_CHAR_SEQ_LEN-1))\n",
    "    #ax.set_xticklabels(labels)\n",
    "\n",
    "    #ax.set_xlabel('Characters to be Written')\n",
    "    #ax.set_ylabel('Stroke Number')\n",
    "\n",
    "    #ims.append([im])\n",
    "\n",
    "    predictions, rng = sample_mix_model(predictions_all[0, -1, :], rng, b)\n",
    "\n",
    "    pred_strokes = predictions[:3]\n",
    "    pred_strokes = pred_strokes[jnp.newaxis, jnp.newaxis, :]\n",
    "\n",
    "    #pred_strokes.shape\n",
    "\n",
    "    dec_input = jax.lax.concatenate([dec_input, pred_strokes], 1)\n",
    "\n",
    "    #print(dec_input.shape)\n",
    "\n",
    "    #print(dec_input.shape)\n",
    "\n",
    "  #ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True,\n",
    "   #                             repeat_delay=1000)\n",
    "  \n",
    "  #ani.save(output_name)\n",
    "\n",
    "  #plt.show()\n",
    "\n",
    "  #return dec_input.numpy(), attention_weights\n",
    "  return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(one_hot_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "             [-2.62234118e-02, -1.38593838e-02,  0.00000000e+00],\n",
       "             [-1.08747827e-02, -2.62626763e-02,  0.00000000e+00],\n",
       "             [-9.55815800e-03, -4.14243490e-02,  0.00000000e+00],\n",
       "             [-1.21826949e-02, -6.44295514e-02,  0.00000000e+00],\n",
       "             [-1.47026591e-02, -8.74672756e-02,  0.00000000e+00],\n",
       "             [-1.64138954e-02, -1.13638744e-01,  0.00000000e+00],\n",
       "             [-1.72382463e-02, -1.38699055e-01,  0.00000000e+00],\n",
       "             [-1.77466553e-02, -1.55204430e-01,  0.00000000e+00],\n",
       "             [-1.74144190e-02, -1.58052787e-01,  0.00000000e+00],\n",
       "             [-1.41755464e-02, -1.43733799e-01,  0.00000000e+00],\n",
       "             [-1.11637209e-02, -1.12657376e-01,  0.00000000e+00],\n",
       "             [-8.88523553e-03, -6.59643114e-02,  0.00000000e+00],\n",
       "             [-6.71610748e-03, -2.42305025e-02,  0.00000000e+00],\n",
       "             [-5.62475855e-03, -1.06374444e-02,  0.00000000e+00],\n",
       "             [-6.54318975e-03, -1.20536215e-03,  0.00000000e+00],\n",
       "             [-7.85897579e-03,  5.50868083e-03,  0.00000000e+00],\n",
       "             [-8.70024320e-03,  7.88452104e-03,  0.00000000e+00],\n",
       "             [-9.30302870e-03,  6.62808400e-03,  0.00000000e+00],\n",
       "             [-9.42914374e-03,  9.81702935e-04,  0.00000000e+00],\n",
       "             [-9.97190550e-03, -4.98497440e-03,  0.00000000e+00],\n",
       "             [-1.14298034e-02, -9.32533387e-03,  0.00000000e+00],\n",
       "             [-1.35245342e-02, -1.06454622e-02,  1.00000000e+00],\n",
       "             [ 1.88142002e+00,  5.58124967e-02,  0.00000000e+00],\n",
       "             [-2.06169114e-02, -1.00186216e-02,  0.00000000e+00],\n",
       "             [-1.23381251e-02, -2.14502215e-02,  0.00000000e+00],\n",
       "             [-1.57329403e-02, -2.69111954e-02,  0.00000000e+00],\n",
       "             [-2.55184043e-02, -2.85867210e-02,  0.00000000e+00],\n",
       "             [-2.80802622e-02, -2.70290654e-02,  0.00000000e+00],\n",
       "             [-2.58519761e-02, -2.40201484e-02,  0.00000000e+00],\n",
       "             [-2.30237469e-02, -2.05378216e-02,  0.00000000e+00],\n",
       "             [-2.06311904e-02, -1.65912863e-02,  0.00000000e+00],\n",
       "             [-1.90338232e-02, -1.14807505e-02,  0.00000000e+00],\n",
       "             [-1.76871587e-02, -6.78032031e-03,  0.00000000e+00],\n",
       "             [-1.47435451e-02, -4.65022260e-03,  0.00000000e+00],\n",
       "             [-1.13817677e-02, -5.53691154e-03,  0.00000000e+00],\n",
       "             [-9.89410654e-03, -7.42035313e-03,  0.00000000e+00],\n",
       "             [-9.63930599e-03, -8.94608535e-03,  0.00000000e+00],\n",
       "             [-1.00424290e-02, -1.13497842e-02,  0.00000000e+00],\n",
       "             [-1.08244168e-02, -1.34485941e-02,  0.00000000e+00],\n",
       "             [-1.02735637e-02, -1.46323573e-02,  0.00000000e+00],\n",
       "             [-8.16228613e-03, -1.71101019e-02,  0.00000000e+00],\n",
       "             [-5.91757149e-03, -1.69128999e-02,  0.00000000e+00],\n",
       "             [-4.63799434e-03, -1.63705554e-02,  0.00000000e+00],\n",
       "             [-4.53732023e-03, -1.36772869e-02,  0.00000000e+00],\n",
       "             [-4.99489577e-03, -8.66643060e-03,  0.00000000e+00],\n",
       "             [-5.13026910e-03, -6.75673969e-03,  0.00000000e+00],\n",
       "             [-4.61933622e-03, -4.93179122e-03,  0.00000000e+00],\n",
       "             [-2.91369087e-03, -5.25870454e-03,  0.00000000e+00],\n",
       "             [-1.55696191e-03, -9.15038958e-03,  0.00000000e+00],\n",
       "             [-1.22132967e-03, -1.43047888e-02,  0.00000000e+00],\n",
       "             [-1.83814450e-03, -1.68539342e-02,  0.00000000e+00],\n",
       "             [-1.77757139e-03, -1.76853538e-02,  0.00000000e+00],\n",
       "             [-1.23772339e-03, -1.53812980e-02,  0.00000000e+00],\n",
       "             [ 1.63160323e-04, -1.15723033e-02,  0.00000000e+00],\n",
       "             [ 1.67471624e-03, -7.70939421e-03,  0.00000000e+00],\n",
       "             [ 2.70013209e-03, -6.76319329e-03,  0.00000000e+00],\n",
       "             [ 2.86673638e-03, -7.27989897e-03,  0.00000000e+00],\n",
       "             [ 2.42224033e-03, -9.78833064e-03,  0.00000000e+00],\n",
       "             [ 1.88171619e-03, -1.12534398e-02,  0.00000000e+00],\n",
       "             [ 1.56528025e-03, -1.21214855e-02,  0.00000000e+00],\n",
       "             [ 7.99415226e-04, -1.12128723e-02,  0.00000000e+00],\n",
       "             [ 9.32656752e-04, -1.05586341e-02,  0.00000000e+00],\n",
       "             [ 1.72304036e-03, -9.06257704e-03,  0.00000000e+00],\n",
       "             [ 1.18446536e-03, -8.12966004e-03,  0.00000000e+00],\n",
       "             [ 1.24184252e-03, -1.11435801e-02,  0.00000000e+00],\n",
       "             [ 3.07267066e-03, -1.31985741e-02,  0.00000000e+00],\n",
       "             [ 4.07672627e-03, -1.38260201e-02,  0.00000000e+00],\n",
       "             [ 2.84505892e-03, -1.33514451e-02,  0.00000000e+00],\n",
       "             [ 2.55808351e-04, -1.51067656e-02,  0.00000000e+00],\n",
       "             [-2.07855459e-03, -1.91151537e-02,  0.00000000e+00],\n",
       "             [-3.02603911e-03, -2.06112955e-02,  0.00000000e+00],\n",
       "             [-3.22450278e-03, -2.03671139e-02,  0.00000000e+00],\n",
       "             [-2.34048138e-03, -2.05657575e-02,  0.00000000e+00],\n",
       "             [-1.60842738e-03, -1.70308594e-02,  0.00000000e+00],\n",
       "             [-5.77666564e-04, -1.36553356e-02,  0.00000000e+00],\n",
       "             [-9.35711549e-04, -1.25014791e-02,  0.00000000e+00],\n",
       "             [-1.27926539e-03, -1.27403289e-02,  0.00000000e+00],\n",
       "             [-1.71907339e-03, -1.19354529e-02,  0.00000000e+00],\n",
       "             [-1.03467994e-03, -1.42819472e-02,  0.00000000e+00],\n",
       "             [ 5.19363850e-04, -1.42561868e-02,  0.00000000e+00],\n",
       "             [ 8.03891569e-04, -1.27568338e-02,  0.00000000e+00],\n",
       "             [-1.01370283e-03, -1.31951934e-02,  0.00000000e+00],\n",
       "             [-3.91204888e-03, -1.69297140e-02,  0.00000000e+00],\n",
       "             [-6.34898711e-03, -2.04196088e-02,  0.00000000e+00],\n",
       "             [-7.20554823e-03, -2.31402293e-02,  0.00000000e+00],\n",
       "             [-6.12429250e-03, -2.60166954e-02,  0.00000000e+00],\n",
       "             [-3.64074274e-03, -2.65915673e-02,  0.00000000e+00],\n",
       "             [-5.63845271e-04, -2.51978301e-02,  0.00000000e+00],\n",
       "             [ 1.69706612e-03, -2.24806219e-02,  0.00000000e+00],\n",
       "             [ 2.10667797e-03, -2.03702860e-02,  0.00000000e+00],\n",
       "             [ 1.54421164e-03, -1.84461661e-02,  0.00000000e+00],\n",
       "             [-3.32864583e-05, -1.61973275e-02,  0.00000000e+00],\n",
       "             [-1.89286529e-03, -1.23189362e-02,  0.00000000e+00],\n",
       "             [-3.87049303e-03, -7.24216038e-03,  0.00000000e+00],\n",
       "             [-3.91382538e-03, -5.74220624e-03,  0.00000000e+00],\n",
       "             [-3.05693178e-03, -5.89618040e-03,  0.00000000e+00],\n",
       "             [-2.61206389e-03, -6.67266082e-03,  0.00000000e+00],\n",
       "             [-2.06545508e-03, -8.93404055e-03,  0.00000000e+00],\n",
       "             [-1.44912268e-03, -8.88662599e-03,  0.00000000e+00],\n",
       "             [-1.99841429e-03, -7.47069111e-03,  0.00000000e+00],\n",
       "             [-2.76938942e-03, -5.80376061e-03,  0.00000000e+00],\n",
       "             [-2.64389790e-03, -5.36342664e-03,  0.00000000e+00],\n",
       "             [-2.29174504e-03, -6.35169167e-03,  0.00000000e+00],\n",
       "             [-2.12879525e-03, -8.14334955e-03,  0.00000000e+00],\n",
       "             [-1.40583992e-03, -1.18574053e-02,  0.00000000e+00],\n",
       "             [-4.70066843e-05, -1.52828237e-02,  0.00000000e+00],\n",
       "             [ 1.71548373e-03, -1.47767430e-02,  0.00000000e+00],\n",
       "             [ 3.17302928e-03, -1.25264991e-02,  0.00000000e+00],\n",
       "             [ 4.06463491e-03, -9.39027406e-03,  0.00000000e+00],\n",
       "             [ 5.82382362e-03, -7.42281321e-03,  0.00000000e+00],\n",
       "             [ 6.79731742e-03, -7.79294083e-03,  0.00000000e+00],\n",
       "             [ 6.64154254e-03, -8.86312500e-03,  0.00000000e+00],\n",
       "             [ 4.95431479e-03, -8.35508388e-03,  0.00000000e+00],\n",
       "             [ 2.43207673e-03, -6.60648616e-03,  0.00000000e+00],\n",
       "             [-1.52639739e-04, -4.64222487e-03,  0.00000000e+00],\n",
       "             [-3.38067464e-03, -4.04507760e-03,  0.00000000e+00],\n",
       "             [-4.93021775e-03, -4.46328055e-03,  0.00000000e+00],\n",
       "             [-4.64561023e-03, -4.17647418e-03,  0.00000000e+00],\n",
       "             [-4.37997375e-03, -3.13918758e-03,  0.00000000e+00],\n",
       "             [-3.42093944e-03, -5.31704957e-03,  0.00000000e+00],\n",
       "             [-2.63810228e-03, -6.96001388e-03,  0.00000000e+00],\n",
       "             [-3.05993622e-03, -6.71429746e-03,  0.00000000e+00],\n",
       "             [-4.39058058e-03, -7.50822574e-03,  0.00000000e+00],\n",
       "             [-5.23948390e-03, -7.60859624e-03,  0.00000000e+00],\n",
       "             [-5.46166161e-03, -7.61397323e-03,  0.00000000e+00],\n",
       "             [-4.59380960e-03, -8.58060922e-03,  0.00000000e+00],\n",
       "             [-2.93042627e-03, -9.09270719e-03,  0.00000000e+00],\n",
       "             [-3.21310188e-04, -9.23517160e-03,  0.00000000e+00],\n",
       "             [ 6.70643116e-04, -6.85589947e-03,  0.00000000e+00],\n",
       "             [-9.97376628e-04, -5.35336928e-03,  0.00000000e+00],\n",
       "             [-3.97705007e-03, -3.41884536e-03,  0.00000000e+00],\n",
       "             [-6.22955058e-03, -5.73924859e-04,  0.00000000e+00],\n",
       "             [-6.19877456e-03,  2.48319935e-04,  0.00000000e+00],\n",
       "             [-4.55089239e-03,  1.80154154e-03,  0.00000000e+00],\n",
       "             [-1.16001035e-03,  1.00230321e-03,  0.00000000e+00],\n",
       "             [ 1.75544864e-03, -2.18406465e-04,  0.00000000e+00],\n",
       "             [ 2.43141875e-03, -3.18824546e-03,  0.00000000e+00],\n",
       "             [ 8.38422682e-04, -6.46476308e-03,  0.00000000e+00],\n",
       "             [-2.51735724e-03, -8.16971157e-03,  0.00000000e+00],\n",
       "             [-5.46042854e-03, -8.14584270e-03,  0.00000000e+00],\n",
       "             [-7.49202492e-03, -4.45754780e-03,  0.00000000e+00],\n",
       "             [-8.82394705e-03, -1.42171176e-03,  0.00000000e+00],\n",
       "             [-8.80908035e-03, -1.16427685e-03,  0.00000000e+00],\n",
       "             [-7.92511646e-03, -1.78325956e-03,  0.00000000e+00],\n",
       "             [-7.60359922e-03, -7.85475422e-04,  0.00000000e+00],\n",
       "             [-9.25032236e-03, -6.50944479e-04,  0.00000000e+00],\n",
       "             [-1.08273402e-02, -7.91448343e-04,  0.00000000e+00],\n",
       "             [-1.02669168e-02, -2.08200305e-03,  0.00000000e+00],\n",
       "             [-8.59608315e-03, -4.85076243e-03,  0.00000000e+00],\n",
       "             [-7.01973448e-03, -7.48207048e-03,  0.00000000e+00],\n",
       "             [-7.03021279e-03, -8.76354706e-03,  0.00000000e+00],\n",
       "             [-8.84597376e-03, -9.20970831e-03,  0.00000000e+00],\n",
       "             [-1.05564017e-02, -7.19942572e-03,  0.00000000e+00],\n",
       "             [-1.07172960e-02, -6.90725166e-03,  0.00000000e+00],\n",
       "             [-1.03851473e-02, -2.11591879e-03,  0.00000000e+00],\n",
       "             [-8.31298903e-03,  2.75978493e-03,  0.00000000e+00],\n",
       "             [-5.60952350e-03,  5.29149966e-03,  0.00000000e+00],\n",
       "             [-4.72243922e-03,  3.68580478e-03,  0.00000000e+00],\n",
       "             [-7.03502307e-03,  2.79356306e-03,  0.00000000e+00],\n",
       "             [-9.18429717e-03,  3.75112565e-03,  0.00000000e+00],\n",
       "             [-9.76051111e-03,  6.91098860e-04,  0.00000000e+00],\n",
       "             [-9.42363124e-03, -2.07883219e-04,  0.00000000e+00],\n",
       "             [-9.48729739e-03, -2.70043686e-03,  0.00000000e+00],\n",
       "             [-9.03398637e-03, -6.60371128e-03,  0.00000000e+00],\n",
       "             [-7.74204545e-03, -9.84146260e-03,  1.00000000e+00],\n",
       "             [ 5.14808130e+00,  4.02490377e+00,  0.00000000e+00],\n",
       "             [-2.20690072e-02,  3.84963874e-04,  0.00000000e+00],\n",
       "             [-9.67341196e-03, -5.66175859e-03,  0.00000000e+00],\n",
       "             [-8.87629203e-03, -1.95716713e-02,  0.00000000e+00],\n",
       "             [-9.00973566e-03, -3.65040600e-02,  0.00000000e+00],\n",
       "             [-1.12183793e-02, -4.08463664e-02,  0.00000000e+00],\n",
       "             [-1.05368653e-02, -4.03998680e-02,  0.00000000e+00],\n",
       "             [-9.02061164e-03, -3.63357738e-02,  0.00000000e+00],\n",
       "             [-8.56425241e-03, -2.89414320e-02,  0.00000000e+00],\n",
       "             [-7.59798801e-03, -2.33477112e-02,  0.00000000e+00],\n",
       "             [-5.95318852e-03, -1.73603147e-02,  0.00000000e+00],\n",
       "             [-4.86110663e-03, -1.44848134e-02,  0.00000000e+00],\n",
       "             [-5.73424809e-03, -1.09423222e-02,  0.00000000e+00],\n",
       "             [-4.81207110e-03, -7.05119409e-03,  0.00000000e+00],\n",
       "             [-3.05697345e-03, -1.52834831e-03,  0.00000000e+00],\n",
       "             [-1.99783640e-03,  2.21956940e-03,  0.00000000e+00],\n",
       "             [-1.78076245e-03,  3.94036109e-03,  0.00000000e+00],\n",
       "             [-3.55530647e-03,  2.94918637e-03,  0.00000000e+00],\n",
       "             [-6.55409554e-03,  1.08326778e-04,  0.00000000e+00],\n",
       "             [-9.47029609e-03, -3.01908702e-03,  0.00000000e+00],\n",
       "             [-1.17885983e-02, -4.69155423e-03,  0.00000000e+00],\n",
       "             [-1.13112088e-02, -3.86658451e-03,  0.00000000e+00],\n",
       "             [-8.90981127e-03, -1.15536393e-04,  0.00000000e+00],\n",
       "             [-6.29942119e-03,  2.37422693e-03,  0.00000000e+00],\n",
       "             [-3.98327550e-03,  4.49617766e-03,  0.00000000e+00],\n",
       "             [-2.00122921e-03,  4.63289954e-03,  0.00000000e+00],\n",
       "             [-1.44464558e-03,  6.32395130e-03,  0.00000000e+00],\n",
       "             [-1.36473228e-03,  6.99818879e-03,  0.00000000e+00],\n",
       "             [-2.44642072e-03,  7.60868145e-03,  0.00000000e+00],\n",
       "             [-3.24998191e-03,  7.82440789e-03,  0.00000000e+00],\n",
       "             [-3.47614824e-03,  8.27610772e-03,  0.00000000e+00],\n",
       "             [-2.67502666e-03,  7.36207655e-03,  0.00000000e+00],\n",
       "             [-2.01593945e-03,  7.12552620e-03,  0.00000000e+00],\n",
       "             [-2.10703956e-03,  5.04129333e-03,  0.00000000e+00],\n",
       "             [-2.40574474e-03,  4.56797285e-03,  0.00000000e+00],\n",
       "             [-2.90976488e-03,  5.63225988e-03,  0.00000000e+00],\n",
       "             [-3.10079916e-03,  5.06898807e-03,  0.00000000e+00],\n",
       "             [-3.15942708e-03,  1.73580984e-03,  0.00000000e+00],\n",
       "             [-3.61543102e-03, -1.67502963e-03,  1.00000000e+00],\n",
       "             [ 5.05525780e+00,  2.85519958e+00,  0.00000000e+00],\n",
       "             [-2.10543778e-02, -6.59309514e-03,  0.00000000e+00],\n",
       "             [-1.13138268e-02,  6.80014957e-03,  0.00000000e+00],\n",
       "             [-5.92013448e-03, -8.50245077e-03,  0.00000000e+00],\n",
       "             [-9.49727930e-03, -7.49440212e-03,  0.00000000e+00],\n",
       "             [-9.96119343e-03, -5.33546647e-03,  1.00000000e+00],\n",
       "             [ 1.45235860e+00, -2.29960585e+00,  0.00000000e+00],\n",
       "             [-5.99070394e-04, -2.91061047e-02,  0.00000000e+00],\n",
       "             [-2.20162119e-03,  3.18627222e-03,  0.00000000e+00],\n",
       "             [ 5.13185514e-05, -4.66476008e-03,  0.00000000e+00],\n",
       "             [-1.44136790e-03, -1.39251468e-03,  0.00000000e+00],\n",
       "             [-4.61325841e-03,  4.46883729e-03,  0.00000000e+00],\n",
       "             [-5.02586644e-03,  5.31506678e-03,  0.00000000e+00],\n",
       "             [-4.88333171e-03,  4.90139285e-03,  0.00000000e+00],\n",
       "             [-4.92673134e-03,  5.94827672e-03,  0.00000000e+00],\n",
       "             [-4.08653496e-03,  9.10017360e-03,  0.00000000e+00],\n",
       "             [-3.02993366e-03,  1.02587957e-02,  0.00000000e+00],\n",
       "             [-1.48626475e-03,  9.05735511e-03,  0.00000000e+00],\n",
       "             [ 5.14565269e-04,  5.34031540e-03,  1.00000000e+00],\n",
       "             [ 5.12412024e+00,  3.38041139e+00,  0.00000000e+00],\n",
       "             [-1.45423105e-02, -2.15719044e-02,  0.00000000e+00],\n",
       "             [-1.08041260e-02,  4.44244186e-04,  0.00000000e+00],\n",
       "             [-7.28309434e-03, -2.10911166e-02,  0.00000000e+00],\n",
       "             [-4.73884260e-03, -2.84726564e-02,  0.00000000e+00],\n",
       "             [-3.04748258e-03, -2.93895844e-02,  0.00000000e+00],\n",
       "             [ 7.72568106e-04, -2.83204969e-02,  0.00000000e+00],\n",
       "             [ 1.70294370e-03, -2.38261279e-02,  0.00000000e+00],\n",
       "             [ 2.59653246e-03, -1.67848840e-02,  0.00000000e+00],\n",
       "             [ 2.04357039e-03, -9.54882521e-03,  0.00000000e+00],\n",
       "             [ 2.98769493e-03, -1.33630470e-03,  0.00000000e+00],\n",
       "             [ 4.52691782e-03,  2.76216515e-03,  0.00000000e+00],\n",
       "             [ 7.79497577e-03,  8.97416752e-03,  0.00000000e+00],\n",
       "             [ 1.16493469e-02,  1.35128591e-02,  0.00000000e+00],\n",
       "             [ 1.02651585e-02,  1.72470193e-02,  0.00000000e+00],\n",
       "             [ 7.07337586e-03,  1.74145941e-02,  0.00000000e+00],\n",
       "             [ 3.62475100e-03,  1.59171857e-02,  0.00000000e+00],\n",
       "             [ 2.65948451e-03,  1.35562327e-02,  0.00000000e+00],\n",
       "             [ 3.93716618e-03,  9.01020505e-03,  0.00000000e+00],\n",
       "             [ 7.16487365e-03,  6.26635179e-03,  0.00000000e+00],\n",
       "             [ 1.13558155e-02,  3.53495614e-03,  0.00000000e+00],\n",
       "             [ 1.28247952e-02,  1.14913529e-03,  0.00000000e+00],\n",
       "             [ 1.02106165e-02, -3.29065183e-03,  0.00000000e+00],\n",
       "             [ 5.28777856e-03, -5.95443603e-03,  0.00000000e+00],\n",
       "             [ 9.82229598e-04, -5.49991801e-03,  0.00000000e+00],\n",
       "             [-1.78057759e-03, -5.42518497e-03,  0.00000000e+00],\n",
       "             [-3.72573291e-03, -2.19869427e-03,  0.00000000e+00]],            dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_stroke(results[0, :250, :], one_hot_sentence[0, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ML-tests': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b163f0480e5ecb2ed0d5f04556597a429b2f653f97b785151c27d861fe015d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
