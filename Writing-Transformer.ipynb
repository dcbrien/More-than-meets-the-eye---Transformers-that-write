{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/don/mltests-venv/lib/python3.8/site-packages/haiku/_src/data_structures.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.\n",
      "  PyTreeDef = type(jax.tree_structure(None))\n",
      "/home/don/mltests-venv/lib/python3.8/site-packages/chex/_src/pytypes.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.\n",
      "  PyTreeDef = type(jax.tree_structure(None))\n",
      "2022-09-17 19:55:01.787090: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-17 19:55:02.850027: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-17 19:55:02.850148: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-17 19:55:02.850157: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# First going to work out some ideas in a jupyter notebook - Later I will clean this\n",
    "# up and and create some proper code.\n",
    "\n",
    "from typing import Iterator, NamedTuple\n",
    "\n",
    "# Getting some tensorflow warnings, but don't care about those right now\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import pathlib\n",
    "import string\n",
    "import glob2\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dataclasses\n",
    "\n",
    "tf.config.experimental.set_visible_devices([], \"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The IAM On-Line Handwriting Database (IAM-OnDB) is required for this project, so you will need to ask for\n",
    "# permission for that and download. https://fki.tic.heia-fr.ch/databases/iam-on-line-handwriting-database \n",
    "# Here we will create a wrapper class to give us some tensorflow dataset summaries of just the writing portion.\n",
    "\n",
    "class WritingGenerator():\n",
    "    def __init__(self, f_name, batch_size=32):   \n",
    "        self.all_x = []\n",
    "        self.all_y = []\n",
    "\n",
    "        # How we might pad each stroke to a consistent length and batch it for fitting - With the amount \n",
    "        # of data in the database and the complexity of a Transformer, this should probably be kept to\n",
    "        # under 400 strokes and 20 characters. There is an issue here though that different people use \n",
    "        # different amounts of strokes/char and this can confuse the network. I am not sure there is an\n",
    "        # elegant way to handle that problem with a Transformer network, as we would need about 1500\n",
    "        # tokens to read in every writing sample fully with padding and this is beyond a standard \n",
    "        # Transformer. Perhaps a PerceiverAR is next?\n",
    "        self.MAX_STROKE_LEN = 250\n",
    "        self.MAX_CHAR_SEQ_LEN = 20         \n",
    "        \n",
    "        self.f_name = f_name\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.padding_value = -1.\n",
    "        self.char_padding_value = 0.\n",
    "\n",
    "        # You will need to change this and point to your own database where you unzipped all of the \n",
    "        # IAM-OnDB strokes and corresponding ascii\n",
    "        base_dir_strokes='../IamONDB/lineStrokes'\n",
    "        base_dir_ascii='../IamONDB/ascii'\n",
    "        \n",
    "        try:\n",
    "            f = open(f_name, 'r')\n",
    "        except IOError:\n",
    "            print(\"Error opening file\")\n",
    "            return 0\n",
    "      \n",
    "        f_train = list(f)\n",
    "        f.close()\n",
    "        \n",
    "        self.n_samp = len(f_train)\n",
    "\n",
    "        print('Reading ' + str(self.n_samp) + ' files')\n",
    "\n",
    "        # This will contain a list of all stroke files\n",
    "        self.f_sub_list_strokes = []\n",
    "        # This will contain a list of corresponding ascii line files\n",
    "        self.f_sub_list_ascii = []\n",
    "\n",
    "        # First create a list of all subfiles in the .txt list - we are going to treat each line as a separate sample here\n",
    "        for i, fname in enumerate(f_train):\n",
    "            path_stroke = glob2.glob(base_dir_strokes + '/' + fname.strip()[0:3] + '/' + fname.strip()[0:7] + '/' + fname.strip() + '-*.xml')    \n",
    "              \n",
    "            self.f_sub_list_strokes += path_stroke\n",
    "\n",
    "            path_line = glob2.glob(base_dir_ascii + '/' + fname.strip()[0:3] + '/' + fname.strip()[0:7] + '/' + fname.strip() + '.txt')   \n",
    "            # We want a 1 to 1 matching of strokes to ascii.  We will pull out the appropriate line when we create the dataset\n",
    "            self.f_sub_list_ascii += path_line * len(path_stroke)\n",
    "                        \n",
    "        # list datasets\n",
    "        self.list_ds_strokes = tf.data.Dataset.from_tensor_slices(self.f_sub_list_strokes)\n",
    "        self.list_ds_ascii = tf.data.Dataset.from_tensor_slices(self.f_sub_list_ascii)\n",
    "\n",
    "        # Text helper functions and variables.  All lines of text need to be one-hot-encoded for proper integration into the attention\n",
    "        # mechanism of the model\n",
    "        self.vocab = string.printable\n",
    "\n",
    "        # I am adding 1 to all character enumerations so that 0 is reserved for padding only and can be ignored in the model\n",
    "        self.char2idx = {u: i+1 for i, u in enumerate(self.vocab)}\n",
    "        self.idx2char = {i+1: u for i, u in enumerate(self.vocab)}\n",
    "\n",
    "        self.invert_one_hot = lambda x: tf.argmax(x, -1).numpy()\n",
    "\n",
    "        self.text_to_int = lambda x: np.array([self.char2idx[c] for c in x])\n",
    "        self.int_to_text = lambda x: ''.join(np.array([self.idx2char[i] for i in x]))\n",
    "\n",
    "        # Combine\n",
    "        self.list_ds = tf.data.Dataset.zip((self.list_ds_strokes, self.list_ds_ascii))\n",
    "        \n",
    "        # Create a datbase of tuples (strokes, matching ascii)\n",
    "        self.labeled_ds = self.list_ds.map(lambda x, y: tf.py_function(self.process_stroke, (x, y), (tf.float32, tf.float32)))\n",
    "        \n",
    "        self.cached_example_dataset = self.labeled_ds.shuffle(buffer_size=1024).cache().take(1024)\n",
    "        \n",
    "    # Create a dataset of strokes and matching lines - As mentioned before, each line is a training sample in this version\n",
    "    def process_stroke(self, file_path_stroke, file_path_lines):\n",
    "        line_num = int(file_path_stroke.numpy()[-6:-4])\n",
    "        strokes = self.get_strokes(file_path_stroke.numpy())\n",
    "        # Not sure the best way to combine two files\n",
    "        lines = self.get_ascii(file_path_lines)\n",
    "\n",
    "        U = lines[line_num-1]\n",
    "        U = U[:self.MAX_CHAR_SEQ_LEN]\n",
    "        U_conv = tf.keras.backend.one_hot(self.text_to_int(U), len(self.vocab)+1)\n",
    "\n",
    "        return strokes, U_conv\n",
    "\n",
    "    # Returns only the strokes of the dataset as a tuple with a label of the same data 1 timestamp ahead\n",
    "    @property\n",
    "    def batched_set(self):\n",
    "        # We don't care about the line data for this version, so remove that first\n",
    "\n",
    "        stroke_only_ds=self.labeled_ds.map(lambda x, y: x)\n",
    "        \n",
    "        # All sequences will be strictly right padded so that tensorflow will run them on a GPU\n",
    "        batched_dataset = stroke_only_ds.padded_batch(self.batch_size, padded_shapes=([self.MAX_STROKE_LEN, 3]), \n",
    "                                                      drop_remainder=True, padding_values=self.padding_value)\n",
    "\n",
    "        return batched_dataset.map(self.dense_1_step).cache()\n",
    "        return batched_dataset.cache()\n",
    "\n",
    "    # Returns only the strokes of the dataset as a tuple with a label of the same data 1 timestamp ahead\n",
    "    # This one also returns the character sequence U being written as a one-hot-encoded tensor\n",
    "    @property\n",
    "    def batched_onehot_set(self):\n",
    "        batched_dataset_one_hot = self.labeled_ds.padded_batch(\n",
    "            self.batch_size, padded_shapes=([self.MAX_STROKE_LEN, 3], \n",
    "                                            [self.MAX_CHAR_SEQ_LEN, len(self.vocab)+1]), \n",
    "                                            drop_remainder=False, padding_values=(self.padding_value, self.char_padding_value))        \n",
    "\n",
    "        return batched_dataset_one_hot.map(self.dense_1_step)\n",
    "\n",
    "    # We will make our prediction 1 step ahead\n",
    "    def dense_1_step(self, batch_stroke, batch_char_seq):\n",
    "        # Shift features and labels one step relative to each other.\n",
    "        return (batch_stroke[:, :, :], batch_char_seq ), batch_stroke[:, :, :]\n",
    "    \n",
    "    def get_examples(self, num_examples):\n",
    "        example_dataset = self.labeled_ds.shuffle(100).take(num_examples)\n",
    "        \n",
    "        #example_dataset = self.labeled_ds.batch(1).shuffle(100).take(num_examples)\n",
    "        \n",
    "        return example_dataset\n",
    "        \n",
    "    def get_strokes(self, fname):\n",
    "        root = ET.parse(fname).getroot()\n",
    "\n",
    "        # Parse one xml file\n",
    "        strokeset = root.find('StrokeSet')\n",
    "\n",
    "        x_samp = []\n",
    "\n",
    "        for stroke in strokeset.iter('Stroke'):\n",
    "            for child in stroke:\n",
    "                x_samp.append([float(child.attrib.get('x')), -1*float(child.attrib.get('y')), 0.])\n",
    "\n",
    "            # As in Graves, 2013, we add a binary vector indicating the end of a stroke\n",
    "            x_samp[-1][-1]=1.0\n",
    "\n",
    "        x_samp = np.asarray(x_samp)\n",
    "        x_samp = x_samp[:self.MAX_STROKE_LEN, :]\n",
    "\n",
    "        # We want the data as offsets though, not raw strokes - easier to train a network to predict small changes in the next timestamp\n",
    "        x_off = np.hstack(([x_samp[1:, :2]-x_samp[:-1, :2], x_samp[1:, 2:3]]))\n",
    "        x_off = np.vstack(([0, 0, 0], x_off))\n",
    "\n",
    "        x_off[:, 0] /= np.std(x_off[:, 0])\n",
    "        x_off[:, 1] /= np.std(x_off[:, 1])\n",
    "\n",
    "        return x_off\n",
    " \n",
    "    # Read an ascii file form the iamONDB and return all of the lines as strings\n",
    "    def get_ascii(self, fname):\n",
    "        text_file = open(fname.numpy(), \"r\")\n",
    "        lines = text_file.read()\n",
    "        lines = lines.split('CSR:')\n",
    "\n",
    "        return lines[1].strip().split('\\n')       \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Writing Dataset for: {self.f_name}'\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing function for strokes\n",
    "# TODO: This should be in the writing class\n",
    "\n",
    "def plot_stroke(offsets, lines):\n",
    "    plt.figure(num=None, figsize=(15, 4))\n",
    "    strokes=np.array([np.cumsum(offsets[:,0]), np.cumsum(offsets[:,1]), offsets[:,2]]);    \n",
    "    stroke=[]\n",
    "\n",
    "    strokes[-1, -1] = 1\n",
    "\n",
    "    for x, y, eos in strokes.T:\n",
    "        stroke.append([x, y])\n",
    "        if eos > 0.1:\n",
    "            stroke=np.asarray(stroke);\n",
    "            #print(stroke.shape)\n",
    "            plt.plot(stroke[:,0], stroke[:,1], 'k')\n",
    "            stroke = []\n",
    "\n",
    "    clean_txt = lines\n",
    "\n",
    "    clean_txt = np.delete(clean_txt, np.argmax(clean_txt, -1) == 0.0, axis=0)\n",
    "\n",
    "    # TODO: This should be passed in\n",
    "    plt.title(train.int_to_text(train.invert_one_hot(clean_txt)))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1726 files\n"
     ]
    }
   ],
   "source": [
    "# I concatenated all data into one set as we just want the maximum amount of data to train the \n",
    "# network to write and don't really care about evaluation or test sets for this project\n",
    "train = WritingGenerator('../IamONDB/trainset_d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect our dataset.  It returns a tuple of stroke offsets and matching ascii strings\n",
    "# You can see that there isn't an exact match for every subject for strokes to ascii \n",
    "# because of the token limit in Transformers\n",
    "SUB = 12\n",
    "\n",
    "for s, l in train.batched_onehot_set.take(2).cache():\n",
    "    plot_stroke(s[0][SUB, :, :], s[1][SUB, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Much of the following code is from the excellent Tensorflow tutorial on Transformers: https://www.tensorflow.org/text/tutorials/transformer\n",
    "\n",
    "# STEP 1 - Positional Embeddings from the original paper. Although, you can also just add a randomized vector and I may try that next\n",
    "# TODO: Switch to a random vector and see if performance suffers vs this complex embedding.\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / jnp.power(10000, (2 * (i//2)) / jnp.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(jnp.arange(position)[:, jnp.newaxis],\n",
    "                          jnp.arange(d_model)[jnp.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads = angle_rads.at[:, 0::2].set(jnp.sin(angle_rads[:, 0::2]))\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads = angle_rads.at[:, 1::2].set(jnp.cos(angle_rads[:, 1::2]))\n",
    "\n",
    "  pos_encoding = angle_rads[jnp.newaxis, ...]\n",
    "\n",
    "  return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up some pieces in haiku. See: https://github.com/deepmind/dm-haiku/tree/main/examples/transformer\n",
    "\n",
    "def layer_norm(x: jnp.ndarray) -> jnp.ndarray:\n",
    "  \"\"\"Applies a unique LayerNorm to x with default settings.\"\"\"\n",
    "  ln = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)\n",
    "  return ln(x)\n",
    "\n",
    "def point_wise_feed_forward(x: jnp.ndarray, d_model: int, dff: int) -> jnp.ndarray:\n",
    "  mlp = hk.Sequential([\n",
    "      hk.Linear(dff, name='Lin1'), jax.nn.relu, # (batch_size, seq_len, dff)\n",
    "      hk.Linear(d_model, name='Lin2'),          # (batch_size, seq_len, d_model)\n",
    "  ])\n",
    "  return mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out the point_wise_feed_forward network\n",
    "network = hk.transform(point_wise_feed_forward)\n",
    "params = network.init(rng=jax.random.PRNGKey(42), x=jnp.zeros((32, 100)), d_model=128, dff=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Encoder - Self attention over the input characters - The only mask needed is padded characters\n",
    "class Encoder_Layer(hk.Module):\n",
    "    # The Encoder Layer is one stack of the Encoder, putting the multihead together \n",
    "    # with the point wise network and some normalization layers\n",
    "    def __init__(self, key_size, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__(name='EncoderLayer')\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # In haiku the key_size is specified manually instead of d_model/num_heads. Internally, it \n",
    "        # will project Q, K, and V to dimensions (*leading_dims, num_heads, head_size) before \n",
    "        # computing attention logits. After that you can futher modify it to project to d_model.\n",
    "        # TODO: Define an initializer here?\n",
    "        self.mha = hk.MultiHeadAttention(num_heads=num_heads, key_size=key_size, model_size=d_model, w_init_scale=1)\n",
    "\n",
    "    # I don't think haiku has any method for dealing with removing dropout automatically, so we will need\n",
    "    # to always pass in a training flag to remove it if necessary during inference\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "        mask,\n",
    "        training=True\n",
    "    ) -> jnp.ndarray:\n",
    "        # Need to format the mask properly across q_vals * k_vals\n",
    "        mask1 = mask[:, None, :, None] \n",
    "        mask2 = mask[:, None, None, :] \n",
    "        mask = mask1 & mask2  # [B, H=1, T, T]\n",
    "\n",
    "        attn_out = self.mha(x, x, x, mask)\n",
    "        if training:\n",
    "            attn_out = hk.dropout(hk.next_rng_key(), self.dropout_rate, attn_out)\n",
    "\n",
    "        # residual 1\n",
    "        attn_out = x + attn_out\n",
    "        attn_out1 = layer_norm(attn_out)\n",
    "\n",
    "        ffn_out = point_wise_feed_forward(attn_out1, self.d_model, self.dff)\n",
    "        if training:\n",
    "            ffn_out = hk.dropout(hk.next_rng_key(), self.dropout_rate, ffn_out)\n",
    "        \n",
    "        # residual 2\n",
    "        ffn_out = attn_out1 + ffn_out\n",
    "        attn_out2 = layer_norm(ffn_out)\n",
    "\n",
    "        return attn_out2\n",
    "\n",
    "# The Encoder module handles the pre-processing of the character data - embedding + positional encoding\n",
    "# and looping over the requested number of encoder attention layers\n",
    "class Encoder(hk.Module):\n",
    "    def __init__(self, num_layers, key_size, d_model, num_heads, dff, maximum_positional_encoding, \n",
    "        dropout_rate=0.1):\n",
    "        super().__init__(name='Encoder')\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.enc_layers = [Encoder_Layer(key_size, d_model, num_heads, dff, dropout_rate) \n",
    "                       for _ in range(num_layers)]\n",
    "\n",
    "        # Postional encodings - enocodings are static in this case and not learned parameters\n",
    "        # TODO: Compare this to random\n",
    "        self.positional_embeddings = positional_encoding(maximum_positional_encoding, d_model)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "        mask,\n",
    "        training=True\n",
    "    ) -> jnp.ndarray:\n",
    "        # The mask for the encoder needs to be broadcastable to the last 2 dimensions (1, 1, T, T)\n",
    "        # because the multihead attention is parallel - See https://www.tensorflow.org/text/tutorials/transformer\n",
    "        \n",
    "        seq_len = jnp.shape(x)[1]\n",
    "        \n",
    "        # We are using one-hot encoded characters and not embedded words, so we will just use a\n",
    "        # prenet to connect that to our model of depth d_model instead\n",
    "        x = hk.Linear(self.d_model, name='prenet')(x)\n",
    "\n",
    "        x = x + self.positional_embeddings[:, :seq_len, :]\n",
    "\n",
    "        if training:\n",
    "            x = hk.dropout(hk.next_rng_key(), self.dropout_rate, x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask, training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Encoder - x input will be [B, T, d_model] embedded characters with positional encoding added\n",
    "x=s[1].numpy()\n",
    "\n",
    "mask = jnp.not_equal(jnp.sum(x, -1), 0)\n",
    "\n",
    "def encoder(x: jnp.ndarray, mask: jnp.ndarray) -> jnp.ndarray:\n",
    "    enc = Encoder(4, 32, 128, 4, 128, 200)\n",
    "\n",
    "    return enc(x, mask)\n",
    "\n",
    "network = hk.transform(encoder)\n",
    "key = jax.random.PRNGKey(42) \n",
    "params = network.init(rng=key, x=jnp.ones((32, train.MAX_CHAR_SEQ_LEN, 101)), mask=mask)\n",
    "\n",
    "out = network.apply(params, key, x=x, mask=mask)\n",
    "\n",
    "#params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Decoder - Very similar to the Encoder with a self-attention mechanism, but there is a second cross-attention mechanism with\n",
    "# the output of the Encoder as K, V and the outputs of the self-attention mechanism as Q. That is, as the Decoder attemps to draw\n",
    "# hand written text based on the Encoder characters, it asks what parts of the encoding are important. Hopefully it learns this \n",
    "# relationship and we should see that reflected in the attention weights. It uses additional causal-masking to prevent future tokens\n",
    "# from being attended to as it attempts to predict the next token.\n",
    "\n",
    "class Decoder_Layer(hk.Module):\n",
    "    # The Decoder Layer is one stack of the Decoder, putting the 2 multihead attention blocks together \n",
    "    # with the point wise network and some normalization layers\n",
    "    def __init__(self, key_size, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__(name='DecoderLayer')\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.mha_self = hk.MultiHeadAttention(num_heads=num_heads, key_size=key_size, model_size=d_model, w_init_scale=1)\n",
    "        self.mha_cross = hk.MultiHeadAttention(num_heads=num_heads, key_size=key_size, model_size=d_model, w_init_scale=1)\n",
    "\n",
    "    # Mask here will only deal with the padding mask. We will compute the causal mask as needed in the calling function\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "        enc_output: jnp.ndarray,\n",
    "        mask,       # Mask of stroke padding\n",
    "        enc_mask,   # Mask of character padding\n",
    "        training=True\n",
    "    ) -> jnp.ndarray:\n",
    "        seq_len = jnp.shape(x)[1]\n",
    "\n",
    "        # Need to format the mask properly across q_vals * k_vals\n",
    "        mask1 = mask[:, None, :, None] \n",
    "        mask2 = mask[:, None, None, :] \n",
    "        mask2 = mask1 & mask2  # [B, H=1, T, T]\n",
    "\n",
    "        # Compute the causal mask and combine with the padding mask for the strokes\n",
    "        causal_mask = np.tril(np.ones((1, 1, seq_len, seq_len)))  # [B=1, H=1, T, T]\n",
    "        self_mask = mask2 * causal_mask  # [B, H=1, T, T]\n",
    "\n",
    "        # Self-attention\n",
    "        attn_out = self.mha_self(x, x, x, self_mask)\n",
    "        if training:\n",
    "            attn_out = hk.dropout(hk.next_rng_key(), self.dropout_rate, attn_out)\n",
    "\n",
    "        # residual 1\n",
    "        attn_out = x + attn_out\n",
    "        attn_out1 = layer_norm(attn_out)\n",
    "\n",
    "        # Cross-attention\n",
    "        # Combine the 2 padding masks. We don't need to attend to encodings that are padded or \n",
    "        # query decodings that are padded\n",
    "\n",
    "        # Need to format the mask properly across q_vals * k_vals\n",
    "        # TODO: This code is repeated a lot. This needs to be refactored.\n",
    "        mask1 = mask[:, None, :, None] \n",
    "        mask2 = enc_mask[:, None, None, :] \n",
    "        cross_mask = mask1 & mask2  # [B, H=1, T, T]\n",
    "        attn_out2 = self.mha_cross(attn_out1, enc_output, enc_output, cross_mask)\n",
    "        if training:\n",
    "            attn_out2 = hk.dropout(hk.next_rng_key(), self.dropout_rate, attn_out2)\n",
    "\n",
    "        # residual 2\n",
    "        attn_out2 = attn_out1 + attn_out2\n",
    "        attn_out2 = layer_norm(attn_out2)\n",
    "\n",
    "        attn_out3 = point_wise_feed_forward(attn_out2, self.d_model, self.dff)\n",
    "        if training:\n",
    "            attn_out3 = hk.dropout(hk.next_rng_key(), self.dropout_rate, attn_out3)\n",
    "        \n",
    "        # residual 3\n",
    "        attn_out3 = attn_out2 + attn_out3\n",
    "        out_all = layer_norm(attn_out3)\n",
    "\n",
    "        # TODO: It looks like the haiku transformer does not allow the return of the attent weights,\n",
    "        # only the final projection. I am going to fork my own repo and add that (maybe pull request as well)\n",
    "        return out_all\n",
    "\n",
    "def decoder_prenet(x: jnp.ndarray, d_model: int) -> jnp.ndarray:\n",
    "  mlp = hk.Sequential([\n",
    "      hk.Linear(d_model, name='D_Prenet1'), jax.nn.relu,    # (batch_size, seq_len, d_model)\n",
    "      hk.Linear(d_model, name='D_Prenet2'), jax.nn.relu,   # (batch_size, seq_len, d_model)\n",
    "      hk.Linear(d_model, name='D_Prenet3')                 # (batch_size, seq_len, d_model)\n",
    "  ])\n",
    "  return mlp(x)\n",
    "\n",
    "# The Decoder module handles the pre-processing of the stroke data - embedding + positional encoding\n",
    "# and looping over the requested number of decoder attention layers\n",
    "class Decoder(hk.Module):\n",
    "    def __init__(self, num_layers, key_size, d_model, num_heads, dff, maximum_positional_encoding, \n",
    "        dropout_rate=0.1):\n",
    "        super().__init__(name='Decoder')\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.dec_layers = [Decoder_Layer(key_size, d_model, num_heads, dff, dropout_rate) \n",
    "                       for _ in range(num_layers)]\n",
    "\n",
    "        # Postional encodings - enocodings are static in this case and not learned parameters\n",
    "        # TODO: Compare this to random\n",
    "        self.positional_embeddings = positional_encoding(maximum_positional_encoding, d_model)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "        enc_output: jnp.ndarray,\n",
    "        enc_mask,\n",
    "        training=True\n",
    "    ) -> jnp.ndarray:\n",
    "        # The mask for the encoder needs to be broadcastable to the last 2 dimensions (1, 1, T, T)\n",
    "        # because the multihead attention is parallel - See https://www.tensorflow.org/text/tutorials/transformer\n",
    "        # We just check the pen up for a negative value to indicate a masked stroke (otherwise is should be \n",
    "        # 0 or 1)\n",
    "        # TODO: padding_value should be passed in or made global\n",
    "        mask = jnp.not_equal(x[:,:,2], train.padding_value)\n",
    "\n",
    "        seq_len = jnp.shape(x)[1]\n",
    "        \n",
    "        # Adding a small MLP here to give the network an opportunity to construct filters and non-linear relationships\n",
    "        # among the raw stroke data\n",
    "        x = decoder_prenet(x, self.d_model)\n",
    "\n",
    "        x = x + self.positional_embeddings[:, :seq_len, :]\n",
    "\n",
    "        if training:\n",
    "            x = hk.dropout(hk.next_rng_key(), self.dropout_rate, x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, enc_output, mask, enc_mask, training)\n",
    "\n",
    "        # TODO: Modify the mha from haiku to output attention_weights as well\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 200, 128)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Decoder - x input will be [B, T, d_model] embedded characters with positional encoding added\n",
    "x=s[1].numpy()\n",
    "\n",
    "mask = jnp.not_equal(jnp.sum(x, -1), 0)\n",
    "\n",
    "x=s[0].numpy()\n",
    "\n",
    "def decoder(x: jnp.ndarray, enc_output: jnp.ndarray, enc_mask: jnp.ndarray) -> jnp.ndarray:\n",
    "    dec = Decoder(4, 32, 128, 4, 128, 200)\n",
    "\n",
    "    return dec(x, enc_output, mask)\n",
    "\n",
    "network = hk.transform(decoder)\n",
    "key = jax.random.PRNGKey(42) \n",
    "params = network.init(rng=key, x=jnp.ones((32, train.MAX_STROKE_LEN, 3)), enc_output=out, enc_mask=mask)\n",
    "\n",
    "out2 = network.apply(params, key, x=x, enc_output=out, enc_mask=mask)\n",
    "\n",
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Writing Transformer\n",
    "\n",
    "# Output space - number of parameters in the mixture model\n",
    "NUM_MIX_COM = 20\n",
    "# weights + means (x + y) + std. devs. (x + y) + correlations + end_of_stroke\n",
    "# Unlike the Mixture Density Network notebook we are going to add cross correlation\n",
    "# terms to our loss and sampling functions for added complexity of the density \n",
    "# estimations\n",
    "NUM_PARAMS = NUM_MIX_COM + NUM_MIX_COM*2 + NUM_MIX_COM*2 + NUM_MIX_COM + 1\n",
    "\n",
    "class Writing_Transformer(hk.Module):\n",
    "    def __init__(self, num_layers, key_size, d_model, num_heads, dff, pe_encoding, pe_target, \n",
    "        dropout_rate=0.1):\n",
    "        super().__init__(name='Writing_Transformer')\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.enc = Encoder(num_layers, key_size, d_model, num_heads, dff, pe_encoding, dropout_rate)\n",
    "        self.dec = Decoder(num_layers, key_size, d_model, num_heads, dff, pe_target, dropout_rate)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        inp: jnp.ndarray,\n",
    "        tar: jnp.ndarray,\n",
    "        training=True\n",
    "    ) -> jnp.ndarray:\n",
    "        enc_mask = jnp.not_equal(jnp.sum(inp, -1), 0)\n",
    "\n",
    "        # The Encoder\n",
    "        enc_output = self.enc(inp, enc_mask, training)\n",
    "\n",
    "        # The Decoder\n",
    "        dec_output = self.dec(tar, enc_output, enc_mask, training)\n",
    "\n",
    "        # The final layer to give us our logits\n",
    "        final_output = hk.Linear(NUM_PARAMS, name='final_layer')(dec_output)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 200, 121)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test The full network\n",
    "inp=s[1].numpy()\n",
    "tar=s[0].numpy()\n",
    "\n",
    "def writing_transformer(inp: jnp.ndarray, tar: jnp.ndarray) -> jnp.ndarray:\n",
    "    tra = Writing_Transformer(4, 32, 128, 4, 128, 200, 1000, 0.2)\n",
    "\n",
    "    return tra(inp, tar)\n",
    "\n",
    "network = hk.transform(writing_transformer)\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "rng, init_rng = jax.random.split(key)\n",
    "params = network.init(rng=key, inp=inp, tar=tar)\n",
    "\n",
    "out3 = network.apply(params, key, inp=inp, tar=tar)\n",
    "\n",
    "out3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for the loss function\n",
    "# TODO: I think there are a lot of edge cases here that will result in NaNs when training.\n",
    "\n",
    "EPS = 0.000001\n",
    "\n",
    "@hk.transform\n",
    "def my_loss_fun_MDN(batch: tuple, training: bool) -> jnp.ndarray:\n",
    "    # Predict the next strokes\n",
    "\n",
    "    # We split the input and then use 1 sample ahead as the target (y_true)\n",
    "    inp = batch[0][:, :-1]\n",
    "    y_true = batch[0][:, 1:]\n",
    "\n",
    "    logits = writing_transformer(batch[1], inp, training)\n",
    "\n",
    "    pis, mu, sig, rho, eos = jnp.array_split(logits, [NUM_MIX_COM, NUM_MIX_COM*3, NUM_MIX_COM*5, NUM_MIX_COM*6], axis=-1)\n",
    "    \n",
    "    #print(eos.shape)\n",
    "\n",
    "    # weights - must be a probability distribution so softmax over all components\n",
    "    pis = jax.nn.softmax(pis)\n",
    "    \n",
    "    # means - no transformation needed\n",
    "    mu_x1, mu_x2 = jnp.array_split(mu, 2, axis=-1)\n",
    "    \n",
    "    # standard deviations - must be strictly positive so exponent\n",
    "    sig = jnp.exp(sig)\n",
    "    \n",
    "    sig = jnp.clip(sig, EPS, np.inf)\n",
    "    \n",
    "    sig_x1, sig_x2 = jnp.array_split(sig, 2, axis=-1)\n",
    "    \n",
    "    x1, x2, eos_true = jnp.array_split(y_true, 3, axis=-1)\n",
    "    \n",
    "    eos_true = jnp.squeeze(eos_true)\n",
    "        \n",
    "    # correlations - squish to -1 to 1 with tanh activation\n",
    "    rho = jnp.tanh(rho)\n",
    "    \n",
    "    rho = jnp.clip(rho, -1.+EPS, 1.-EPS)\n",
    "    \n",
    "    # Define Z as in Graves, 2013\n",
    "    Z = jnp.square( ( x1-mu_x1 ) / sig_x1 ) + jnp.square( ( x2-mu_x2 ) / sig_x2 ) - ( 2 * rho * (x1-mu_x1) * (x2-mu_x2) ) / ( sig_x1*sig_x2 )\n",
    "    \n",
    "    one_minus_rho_square = 1. - jnp.square(rho)\n",
    "    \n",
    "    # Now form Gaussian mixtures\n",
    "    term1 = jnp.divide(1., ( 2. * np.pi * sig_x1 * sig_x2 * jnp.sqrt( one_minus_rho_square ) ))\n",
    "    term2 = jnp.exp( jnp.divide ( (-1. * Z) , (2.*( one_minus_rho_square )) ))\n",
    "    \n",
    "    mix_loss = jnp.sum(pis * term1 * term2, axis=-1)       \n",
    "    \n",
    "    mix_loss = jnp.clip(mix_loss, EPS, np.inf)\n",
    "\n",
    "    # end of stroke loss\n",
    "    eos = jnp.squeeze(jax.nn.sigmoid(eos))\n",
    "    \n",
    "    eos = jnp.clip(eos, EPS, 1.-EPS)\n",
    "\n",
    "    eos_loss = jnp.where(jnp.equal(eos_true, 1.), eos, 1.-eos)\n",
    "    \n",
    "    # Only the valid parts of the sequence should count towards the loss.  The invalid parts are tagged with -2200\n",
    "    val_seq = jnp.squeeze(jnp.not_equal(eos_true, train.padding_value))\n",
    "\n",
    "    # This is the total loss for each element (batch * num_timepoints)\n",
    "    tot_loss = -(jnp.log(mix_loss) + jnp.log(eos_loss))\n",
    "\n",
    "    # The sequence loss is the sum of only the valid timepoints\n",
    "    \n",
    "    tot_loss = jnp.where(val_seq, tot_loss, 0.)   \n",
    "    \n",
    "    seq_tot = jnp.sum(val_seq, axis=-1, dtype=float)\n",
    "\n",
    "    tot_loss = jnp.sum(tot_loss, axis=-1) / seq_tot \n",
    "\n",
    "    return jnp.mean(tot_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Transformed' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [158], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test out the loss function\u001b[39;00m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mbatched_onehot_set\u001b[38;5;241m.\u001b[39mprefetch(tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mAUTOTUNE)\u001b[38;5;241m.\u001b[39mas_numpy_iterator()\n\u001b[0;32m----> 4\u001b[0m out4 \u001b[38;5;241m=\u001b[39m \u001b[43mmy_loss_fun_MDN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m out4\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Transformed' object is not callable"
     ]
    }
   ],
   "source": [
    "# Test out the loss function\n",
    "data = train.batched_onehot_set.prefetch(tf.data.experimental.AUTOTUNE).as_numpy_iterator()\n",
    "\n",
    "out4 = my_loss_fun_MDN(params, data.next()[0])\n",
    "\n",
    "out4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "num_layers = 3\n",
    "key_size = 64\n",
    "d_model = 256\n",
    "dff = 512\n",
    "num_heads = 4\n",
    "dropout_rate = 0.1\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "GRAD_CLIP_VALUE = 1\n",
    "LEARNING_RATE = 0.00001\n",
    "\n",
    "total_steps = EPOCHS*350 + EPOCHS\n",
    "warmup_cosine_decay_scheduler = optax.warmup_cosine_decay_schedule(init_value=0.0001, peak_value=0.0003,\n",
    "                                                                   warmup_steps=int(total_steps*0.2),\n",
    "                                                                   decay_steps=total_steps, end_value=0.00001)\n",
    "\n",
    "optimiser = optax.chain(\n",
    "      optax.clip_by_global_norm(GRAD_CLIP_VALUE),\n",
    "      optax.adam(LEARNING_RATE, b1=0.9, b2=0.99),\n",
    "  )\n",
    "\n",
    "def writing_transformer(inp: jnp.ndarray, tar: jnp.ndarray, training: bool) -> jnp.ndarray:\n",
    "    tra = Writing_Transformer(num_layers, key_size, d_model, num_heads, dff, pe_encoding=250, pe_target=1000, dropout_rate=dropout_rate)\n",
    "\n",
    "    return tra(inp, tar, training)\n",
    "\n",
    "#network = hk.transform(writing_transformer)\n",
    "\n",
    "@jax.jit\n",
    "def update(params: hk.Params, rng, opt_state: optax.OptState, batch: tuple):\n",
    "  rng, new_rng = jax.random.split(rng)\n",
    "  loss_and_grad_fn = jax.value_and_grad(my_loss_fun_MDN.apply)\n",
    "  #grad = jax.grad(my_loss_fun_MDN)(params, batch)\n",
    "  loss, gradients = loss_and_grad_fn(params, rng, batch, True)\n",
    "  updates, opt_state = optimiser.update(gradients, opt_state)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "\n",
    "  return params, opt_state, loss, new_rng\n",
    "\n",
    "# TODO: make fetching the iterator more elegant and does the conversion from numpy to jax slow things down?\n",
    "#b = train.batched_onehot_set.prefetch(tf.data.experimental.AUTOTUNE).as_numpy_iterator()\n",
    "#key = jax.random.PRNGKey(42) \n",
    "#s = next(b)[0]\n",
    "\n",
    "data_iter = train.batched_onehot_set.prefetch(tf.data.experimental.AUTOTUNE).as_numpy_iterator()\n",
    "\n",
    "rng = jax.random.PRNGKey(SEED)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "params = my_loss_fun_MDN.init(rng=init_rng, batch=data_iter.next()[0], training=True)\n",
    "opt_state = optimiser.init(params)\n",
    "\n",
    "  # Average loss?\n",
    "  #data_iter = train.batched_onehot_set.prefetch(tf.data.experimental.AUTOTUNE).as_numpy_iterator()\n",
    "  #total_loss = 0\n",
    "  #for b, _ in data_iter:\n",
    "  #  total_loss = total_loss + my_loss_fun_MDN(params, b)\n",
    "\n",
    "  #print(\"   loss {:0.4f}\".format(total_loss*BATCH/20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.00001\n",
    "\n",
    "optimiser = optax.chain(\n",
    "      optax.clip_by_global_norm(GRAD_CLIP_VALUE),\n",
    "      optax.adam(LEARNING_RATE, b1=0.9, b2=0.99),\n",
    "  )\n",
    "\n",
    "opt_state = optimiser.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Epoch': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/don/mltests-venv/lib/python3.8/site-packages/optax/_src/linear_algebra.py:29: FutureWarning: jax.tree_leaves is deprecated, and will be removed in a future release. Use jax.tree_util.tree_leaves instead.\n",
      "  sum([jnp.sum(numerics.abs_sq(x)) for x in jax.tree_leaves(updates)]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 3.8572\n",
      "Epoch 1 Batch 50 Loss 1.9608\n",
      "Epoch 1 Batch 100 Loss 1.6183\n",
      "Epoch 1 Batch 150 Loss 1.3245\n",
      "Epoch 1 Batch 200 Loss 1.1072\n",
      "Epoch 1 Batch 250 Loss 0.9477\n",
      "Epoch 1 Batch 300 Loss 0.8292\n",
      "Epoch 1 Batch 350 Loss 0.7288\n",
      "Epoch 1 Loss 0.6752\n",
      "{'Epoch': 1}\n",
      "Epoch 2 Batch 0 Loss 0.5085\n",
      "Epoch 2 Batch 50 Loss 0.1183\n",
      "Epoch 2 Batch 100 Loss 0.1171\n",
      "Epoch 2 Batch 150 Loss 0.0986\n",
      "Epoch 2 Batch 200 Loss 0.0527\n",
      "Epoch 2 Batch 250 Loss 0.0190\n",
      "Epoch 2 Batch 300 Loss -0.0020\n",
      "Epoch 2 Batch 350 Loss -0.0275\n",
      "Epoch 2 Loss -0.0451\n",
      "{'Epoch': 2}\n",
      "Epoch 3 Batch 0 Loss 0.3522\n",
      "Epoch 3 Batch 50 Loss -0.1327\n",
      "Epoch 3 Batch 100 Loss -0.1141\n",
      "Epoch 3 Batch 150 Loss -0.1282\n",
      "Epoch 3 Batch 200 Loss -0.1671\n",
      "Epoch 3 Batch 250 Loss -0.1891\n",
      "Epoch 3 Batch 300 Loss -0.2017\n",
      "Epoch 3 Batch 350 Loss -0.2192\n",
      "Epoch 3 Loss -0.2325\n",
      "{'Epoch': 3}\n",
      "Epoch 4 Batch 0 Loss 0.3483\n",
      "Epoch 4 Batch 50 Loss -0.2492\n",
      "Epoch 4 Batch 100 Loss -0.2331\n",
      "Epoch 4 Batch 150 Loss -0.2452\n",
      "Epoch 4 Batch 200 Loss -0.2822\n",
      "Epoch 4 Batch 250 Loss -0.2964\n",
      "Epoch 4 Batch 300 Loss -0.3017\n",
      "Epoch 4 Batch 350 Loss -0.3160\n",
      "Epoch 4 Loss -0.3269\n",
      "{'Epoch': 4}\n",
      "Epoch 5 Batch 0 Loss 0.2408\n",
      "Epoch 5 Batch 50 Loss -0.3264\n",
      "Epoch 5 Batch 100 Loss -0.2978\n",
      "Epoch 5 Batch 150 Loss -0.3057\n",
      "Epoch 5 Batch 200 Loss -0.3468\n",
      "Epoch 5 Batch 250 Loss -0.3710\n",
      "Epoch 5 Batch 300 Loss -0.3746\n",
      "Epoch 5 Batch 350 Loss -0.3866\n",
      "Epoch 5 Loss -0.3983\n",
      "{'Epoch': 5}\n",
      "Epoch 6 Batch 0 Loss 0.2349\n",
      "Epoch 6 Batch 50 Loss -0.3780\n",
      "Epoch 6 Batch 100 Loss -0.3473\n",
      "Epoch 6 Batch 150 Loss -0.3560\n",
      "Epoch 6 Batch 200 Loss -0.3968\n",
      "Epoch 6 Batch 250 Loss -0.4186\n",
      "Epoch 6 Batch 300 Loss -0.4179\n",
      "Epoch 6 Batch 350 Loss -0.4315\n",
      "Epoch 6 Loss -0.4426\n",
      "{'Epoch': 6}\n",
      "Epoch 7 Batch 0 Loss 0.1821\n",
      "Epoch 7 Batch 50 Loss -0.4218\n",
      "Epoch 7 Batch 100 Loss -0.3890\n",
      "Epoch 7 Batch 150 Loss -0.3922\n",
      "Epoch 7 Batch 200 Loss -0.4383\n",
      "Epoch 7 Batch 250 Loss -0.4595\n",
      "Epoch 7 Batch 300 Loss -0.4608\n",
      "Epoch 7 Batch 350 Loss -0.4746\n",
      "Epoch 7 Loss -0.4850\n",
      "{'Epoch': 7}\n",
      "Epoch 8 Batch 0 Loss 0.1946\n",
      "Epoch 8 Batch 50 Loss -0.4582\n",
      "Epoch 8 Batch 100 Loss -0.4335\n",
      "Epoch 8 Batch 150 Loss -0.4387\n",
      "Epoch 8 Batch 200 Loss -0.4822\n",
      "Epoch 8 Batch 250 Loss -0.5015\n",
      "Epoch 8 Batch 300 Loss -0.5005\n",
      "Epoch 8 Batch 350 Loss -0.5135\n",
      "Epoch 8 Loss -0.5238\n",
      "{'Epoch': 8}\n",
      "Epoch 9 Batch 0 Loss 0.1550\n",
      "Epoch 9 Batch 50 Loss -0.4787\n",
      "Epoch 9 Batch 100 Loss -0.4526\n",
      "Epoch 9 Batch 150 Loss -0.4594\n",
      "Epoch 9 Batch 200 Loss -0.5031\n",
      "Epoch 9 Batch 250 Loss -0.5291\n",
      "Epoch 9 Batch 300 Loss -0.5290\n",
      "Epoch 9 Batch 350 Loss -0.5405\n",
      "Epoch 9 Loss -0.5500\n",
      "{'Epoch': 9}\n",
      "Epoch 10 Batch 0 Loss 0.0465\n",
      "Epoch 10 Batch 50 Loss -0.5221\n",
      "Epoch 10 Batch 100 Loss -0.4805\n",
      "Epoch 10 Batch 150 Loss -0.4936\n",
      "Epoch 10 Batch 200 Loss -0.5388\n",
      "Epoch 10 Batch 250 Loss -0.5566\n",
      "Epoch 10 Batch 300 Loss -0.5562\n",
      "Epoch 10 Batch 350 Loss -0.5689\n",
      "Epoch 10 Loss -0.5776\n",
      "{'Epoch': 10}\n",
      "Epoch 11 Batch 0 Loss 0.0240\n",
      "Epoch 11 Batch 50 Loss -0.5366\n",
      "Epoch 11 Batch 100 Loss -0.4988\n",
      "Epoch 11 Batch 150 Loss -0.5098\n",
      "Epoch 11 Batch 200 Loss -0.5534\n",
      "Epoch 11 Batch 250 Loss -0.5765\n",
      "Epoch 11 Batch 300 Loss -0.5755\n",
      "Epoch 11 Batch 350 Loss -0.5877\n",
      "Epoch 11 Loss -0.5961\n",
      "{'Epoch': 11}\n",
      "Epoch 12 Batch 0 Loss 0.0546\n",
      "Epoch 12 Batch 50 Loss -0.5551\n",
      "Epoch 12 Batch 100 Loss -0.5190\n",
      "Epoch 12 Batch 150 Loss -0.5291\n",
      "Epoch 12 Batch 200 Loss -0.5701\n",
      "Epoch 12 Batch 250 Loss -0.5826\n",
      "Epoch 12 Batch 300 Loss -0.5842\n",
      "Epoch 12 Batch 350 Loss -0.5976\n",
      "Epoch 12 Loss -0.6066\n",
      "{'Epoch': 12}\n",
      "Epoch 13 Batch 0 Loss -0.0429\n",
      "Epoch 13 Batch 50 Loss -0.5780\n",
      "Epoch 13 Batch 100 Loss -0.5419\n",
      "Epoch 13 Batch 150 Loss -0.5498\n",
      "Epoch 13 Batch 200 Loss -0.5926\n",
      "Epoch 13 Batch 250 Loss -0.6138\n",
      "Epoch 13 Batch 300 Loss -0.6147\n",
      "Epoch 13 Batch 350 Loss -0.6259\n",
      "Epoch 13 Loss -0.6340\n",
      "{'Epoch': 13}\n",
      "Epoch 14 Batch 0 Loss -0.0528\n",
      "Epoch 14 Batch 50 Loss -0.5913\n",
      "Epoch 14 Batch 100 Loss -0.5626\n",
      "Epoch 14 Batch 150 Loss -0.5701\n",
      "Epoch 14 Batch 200 Loss -0.6153\n",
      "Epoch 14 Batch 250 Loss -0.6332\n",
      "Epoch 14 Batch 300 Loss -0.6310\n",
      "Epoch 14 Batch 350 Loss -0.6411\n",
      "Epoch 14 Loss -0.6505\n",
      "{'Epoch': 14}\n",
      "Epoch 15 Batch 0 Loss -0.0751\n",
      "Epoch 15 Batch 50 Loss -0.6092\n",
      "Epoch 15 Batch 100 Loss -0.5777\n",
      "Epoch 15 Batch 150 Loss -0.5874\n",
      "Epoch 15 Batch 200 Loss -0.6255\n",
      "Epoch 15 Batch 250 Loss -0.6451\n",
      "Epoch 15 Batch 300 Loss -0.6456\n",
      "Epoch 15 Batch 350 Loss -0.6558\n",
      "Epoch 15 Loss -0.6651\n",
      "{'Epoch': 15}\n",
      "Epoch 16 Batch 0 Loss -0.0787\n",
      "Epoch 16 Batch 50 Loss -0.6268\n",
      "Epoch 16 Batch 100 Loss -0.5778\n",
      "Epoch 16 Batch 150 Loss -0.5902\n",
      "Epoch 16 Batch 200 Loss -0.6375\n",
      "Epoch 16 Batch 250 Loss -0.6589\n",
      "Epoch 16 Batch 300 Loss -0.6599\n",
      "Epoch 16 Batch 350 Loss -0.6733\n",
      "Epoch 16 Loss -0.6815\n",
      "{'Epoch': 16}\n",
      "Epoch 17 Batch 0 Loss -0.0773\n",
      "Epoch 17 Batch 50 Loss -0.6264\n",
      "Epoch 17 Batch 100 Loss -0.5973\n",
      "Epoch 17 Batch 150 Loss -0.6089\n",
      "Epoch 17 Batch 200 Loss -0.6481\n",
      "Epoch 17 Batch 250 Loss -0.6748\n",
      "Epoch 17 Batch 300 Loss -0.6753\n",
      "Epoch 17 Batch 350 Loss -0.6863\n",
      "Epoch 17 Loss -0.6954\n",
      "{'Epoch': 17}\n",
      "Epoch 18 Batch 0 Loss -0.1097\n",
      "Epoch 18 Batch 50 Loss -0.6450\n",
      "Epoch 18 Batch 100 Loss -0.6185\n",
      "Epoch 18 Batch 150 Loss -0.6302\n",
      "Epoch 18 Batch 200 Loss -0.6689\n",
      "Epoch 18 Batch 250 Loss -0.6830\n",
      "Epoch 18 Batch 300 Loss -0.6878\n",
      "Epoch 18 Batch 350 Loss -0.7015\n",
      "Epoch 18 Loss -0.7099\n",
      "{'Epoch': 18}\n",
      "Epoch 19 Batch 0 Loss -0.1072\n",
      "Epoch 19 Batch 50 Loss -0.6503\n",
      "Epoch 19 Batch 100 Loss -0.6277\n",
      "Epoch 19 Batch 150 Loss -0.6406\n",
      "Epoch 19 Batch 200 Loss -0.6829\n",
      "Epoch 19 Batch 250 Loss -0.7059\n",
      "Epoch 19 Batch 300 Loss -0.7086\n",
      "Epoch 19 Batch 350 Loss -0.7201\n",
      "Epoch 19 Loss -0.7283\n",
      "{'Epoch': 19}\n",
      "Epoch 20 Batch 0 Loss -0.1339\n",
      "Epoch 20 Batch 50 Loss -0.6806\n",
      "Epoch 20 Batch 100 Loss -0.6378\n",
      "Epoch 20 Batch 150 Loss -0.6499\n",
      "Epoch 20 Batch 200 Loss -0.6951\n",
      "Epoch 20 Batch 250 Loss -0.7190\n",
      "Epoch 20 Batch 300 Loss -0.7211\n",
      "Epoch 20 Batch 350 Loss -0.7298\n",
      "Epoch 20 Loss -0.7383\n",
      "{'Epoch': 20}\n",
      "Epoch 21 Batch 0 Loss -0.1128\n",
      "Epoch 21 Batch 50 Loss -0.6847\n",
      "Epoch 21 Batch 100 Loss -0.6592\n",
      "Epoch 21 Batch 150 Loss -0.6746\n",
      "Epoch 21 Batch 200 Loss -0.7156\n",
      "Epoch 21 Batch 250 Loss -0.7385\n",
      "Epoch 21 Batch 300 Loss -0.7392\n",
      "Epoch 21 Batch 350 Loss -0.7510\n",
      "Epoch 21 Loss -0.7596\n",
      "{'Epoch': 21}\n",
      "Epoch 22 Batch 0 Loss -0.1676\n",
      "Epoch 22 Batch 50 Loss -0.7010\n",
      "Epoch 22 Batch 100 Loss -0.6716\n",
      "Epoch 22 Batch 150 Loss -0.6893\n",
      "Epoch 22 Batch 200 Loss -0.7286\n",
      "Epoch 22 Batch 250 Loss -0.7552\n",
      "Epoch 22 Batch 300 Loss -0.7586\n",
      "Epoch 22 Batch 350 Loss -0.7656\n",
      "Epoch 22 Loss -0.7730\n",
      "{'Epoch': 22}\n",
      "Epoch 23 Batch 0 Loss -0.1795\n",
      "Epoch 23 Batch 50 Loss -0.7273\n",
      "Epoch 23 Batch 100 Loss -0.6975\n",
      "Epoch 23 Batch 150 Loss -0.7073\n",
      "Epoch 23 Batch 200 Loss -0.7495\n",
      "Epoch 23 Batch 250 Loss -0.7716\n",
      "Epoch 23 Batch 300 Loss -0.7732\n",
      "Epoch 23 Batch 350 Loss -0.7839\n",
      "Epoch 23 Loss -0.7918\n",
      "{'Epoch': 23}\n",
      "Epoch 24 Batch 0 Loss -0.1917\n",
      "Epoch 24 Batch 50 Loss -0.7314\n",
      "Epoch 24 Batch 100 Loss -0.6995\n",
      "Epoch 24 Batch 150 Loss -0.7126\n",
      "Epoch 24 Batch 200 Loss -0.7558\n",
      "Epoch 24 Batch 250 Loss -0.7795\n",
      "Epoch 24 Batch 300 Loss -0.7818\n",
      "Epoch 24 Batch 350 Loss -0.7923\n",
      "Epoch 24 Loss -0.8001\n",
      "{'Epoch': 24}\n",
      "Epoch 25 Batch 0 Loss -0.2218\n",
      "Epoch 25 Batch 50 Loss -0.7392\n",
      "Epoch 25 Batch 100 Loss -0.7149\n",
      "Epoch 25 Batch 150 Loss -0.7277\n",
      "Epoch 25 Batch 200 Loss -0.7690\n",
      "Epoch 25 Batch 250 Loss -0.7943\n",
      "Epoch 25 Batch 300 Loss -0.7960\n",
      "Epoch 25 Batch 350 Loss -0.8046\n",
      "Epoch 25 Loss -0.8122\n",
      "{'Epoch': 25}\n",
      "Epoch 26 Batch 0 Loss -0.1506\n",
      "Epoch 26 Batch 50 Loss -0.7442\n",
      "Epoch 26 Batch 100 Loss -0.7218\n",
      "Epoch 26 Batch 150 Loss -0.7338\n",
      "Epoch 26 Batch 200 Loss -0.7788\n",
      "Epoch 26 Batch 250 Loss -0.8079\n",
      "Epoch 26 Batch 300 Loss -0.8102\n",
      "Epoch 26 Batch 350 Loss -0.8196\n",
      "Epoch 26 Loss -0.8275\n",
      "{'Epoch': 26}\n",
      "Epoch 27 Batch 0 Loss -0.2417\n",
      "Epoch 27 Batch 50 Loss -0.7598\n",
      "Epoch 27 Batch 100 Loss -0.7343\n",
      "Epoch 27 Batch 150 Loss -0.7493\n",
      "Epoch 27 Batch 200 Loss -0.7899\n",
      "Epoch 27 Batch 250 Loss -0.8156\n",
      "Epoch 27 Batch 300 Loss -0.8194\n",
      "Epoch 27 Batch 350 Loss -0.8321\n",
      "Epoch 27 Loss -0.8399\n",
      "{'Epoch': 27}\n",
      "Epoch 28 Batch 0 Loss -0.2087\n",
      "Epoch 28 Batch 50 Loss -0.7635\n",
      "Epoch 28 Batch 100 Loss -0.7445\n",
      "Epoch 28 Batch 150 Loss -0.7581\n",
      "Epoch 28 Batch 200 Loss -0.7988\n",
      "Epoch 28 Batch 250 Loss -0.8221\n",
      "Epoch 28 Batch 300 Loss -0.8260\n",
      "Epoch 28 Batch 350 Loss -0.8349\n",
      "Epoch 28 Loss -0.8430\n",
      "{'Epoch': 28}\n",
      "Epoch 29 Batch 0 Loss -0.2306\n",
      "Epoch 29 Batch 50 Loss -0.7687\n",
      "Epoch 29 Batch 100 Loss -0.7490\n",
      "Epoch 29 Batch 150 Loss -0.7701\n",
      "Epoch 29 Batch 200 Loss -0.8070\n",
      "Epoch 29 Batch 250 Loss -0.8312\n",
      "Epoch 29 Batch 300 Loss -0.8357\n",
      "Epoch 29 Batch 350 Loss -0.8462\n",
      "Epoch 29 Loss -0.8543\n",
      "{'Epoch': 29}\n",
      "Epoch 30 Batch 0 Loss -0.2909\n",
      "Epoch 30 Batch 50 Loss -0.8107\n",
      "Epoch 30 Batch 100 Loss -0.7783\n",
      "Epoch 30 Batch 150 Loss -0.7891\n",
      "Epoch 30 Batch 200 Loss -0.8280\n",
      "Epoch 30 Batch 250 Loss -0.8513\n",
      "Epoch 30 Batch 300 Loss -0.8555\n",
      "Epoch 30 Batch 350 Loss -0.8663\n",
      "Epoch 30 Loss -0.8735\n",
      "{'Epoch': 30}\n",
      "Epoch 31 Batch 0 Loss -0.3002\n",
      "Epoch 31 Batch 50 Loss -0.8071\n",
      "Epoch 31 Batch 100 Loss -0.7877\n",
      "Epoch 31 Batch 150 Loss -0.7998\n",
      "Epoch 31 Batch 200 Loss -0.8423\n",
      "Epoch 31 Batch 250 Loss -0.8693\n",
      "Epoch 31 Batch 300 Loss -0.8719\n",
      "Epoch 31 Batch 350 Loss -0.8831\n",
      "Epoch 31 Loss -0.8915\n",
      "{'Epoch': 31}\n",
      "Epoch 32 Batch 0 Loss -0.2695\n",
      "Epoch 32 Batch 50 Loss -0.8030\n",
      "Epoch 32 Batch 100 Loss -0.7912\n",
      "Epoch 32 Batch 150 Loss -0.8041\n",
      "Epoch 32 Batch 200 Loss -0.8510\n",
      "Epoch 32 Batch 250 Loss -0.8817\n",
      "Epoch 32 Batch 300 Loss -0.8832\n",
      "Epoch 32 Batch 350 Loss -0.8936\n",
      "Epoch 32 Loss -0.8995\n",
      "{'Epoch': 32}\n",
      "Epoch 33 Batch 0 Loss -0.2917\n",
      "Epoch 33 Batch 50 Loss -0.8311\n",
      "Epoch 33 Batch 100 Loss -0.8127\n",
      "Epoch 33 Batch 150 Loss -0.8256\n",
      "Epoch 33 Batch 200 Loss -0.8648\n",
      "Epoch 33 Batch 250 Loss -0.8895\n",
      "Epoch 33 Batch 300 Loss -0.8935\n",
      "Epoch 33 Batch 350 Loss -0.9065\n",
      "Epoch 33 Loss -0.9145\n",
      "{'Epoch': 33}\n",
      "Epoch 34 Batch 0 Loss -0.3039\n",
      "Epoch 34 Batch 50 Loss -0.8159\n",
      "Epoch 34 Batch 100 Loss -0.8142\n",
      "Epoch 34 Batch 150 Loss -0.8290\n",
      "Epoch 34 Batch 200 Loss -0.8719\n",
      "Epoch 34 Batch 250 Loss -0.9021\n",
      "Epoch 34 Batch 300 Loss -0.9048\n",
      "Epoch 34 Batch 350 Loss -0.9110\n",
      "Epoch 34 Loss -0.9184\n",
      "{'Epoch': 34}\n",
      "Epoch 35 Batch 0 Loss -0.3194\n",
      "Epoch 35 Batch 50 Loss -0.8460\n",
      "Epoch 35 Batch 100 Loss -0.8311\n",
      "Epoch 35 Batch 150 Loss -0.8460\n",
      "Epoch 35 Batch 200 Loss -0.8854\n",
      "Epoch 35 Batch 250 Loss -0.9153\n",
      "Epoch 35 Batch 300 Loss -0.9148\n",
      "Epoch 35 Batch 350 Loss -0.9232\n",
      "Epoch 35 Loss -0.9306\n",
      "{'Epoch': 35}\n",
      "Epoch 36 Batch 0 Loss -0.3550\n",
      "Epoch 36 Batch 50 Loss -0.8516\n",
      "Epoch 36 Batch 100 Loss -0.8210\n",
      "Epoch 36 Batch 150 Loss -0.8407\n",
      "Epoch 36 Batch 200 Loss -0.8826\n",
      "Epoch 36 Batch 250 Loss -0.9095\n",
      "Epoch 36 Batch 300 Loss -0.9136\n",
      "Epoch 36 Batch 350 Loss -0.9227\n",
      "Epoch 36 Loss -0.9285\n",
      "{'Epoch': 36}\n",
      "Epoch 37 Batch 0 Loss -0.3477\n",
      "Epoch 37 Batch 50 Loss -0.8747\n",
      "Epoch 37 Batch 100 Loss -0.8549\n",
      "Epoch 37 Batch 150 Loss -0.8597\n",
      "Epoch 37 Batch 200 Loss -0.8993\n",
      "Epoch 37 Batch 250 Loss -0.9269\n",
      "Epoch 37 Batch 300 Loss -0.9272\n",
      "Epoch 37 Batch 350 Loss -0.9376\n",
      "Epoch 37 Loss -0.9448\n",
      "{'Epoch': 37}\n",
      "Epoch 38 Batch 0 Loss -0.3504\n",
      "Epoch 38 Batch 50 Loss -0.8733\n",
      "Epoch 38 Batch 100 Loss -0.8482\n",
      "Epoch 38 Batch 150 Loss -0.8647\n",
      "Epoch 38 Batch 200 Loss -0.9066\n",
      "Epoch 38 Batch 250 Loss -0.9354\n",
      "Epoch 38 Batch 300 Loss -0.9342\n",
      "Epoch 38 Batch 350 Loss -0.9431\n",
      "Epoch 38 Loss -0.9510\n",
      "{'Epoch': 38}\n",
      "Epoch 39 Batch 0 Loss -0.3751\n",
      "Epoch 39 Batch 50 Loss -0.8807\n",
      "Epoch 39 Batch 100 Loss -0.8607\n",
      "Epoch 39 Batch 150 Loss -0.8741\n",
      "Epoch 39 Batch 200 Loss -0.9153\n",
      "Epoch 39 Batch 250 Loss -0.9431\n",
      "Epoch 39 Batch 300 Loss -0.9464\n",
      "Epoch 39 Batch 350 Loss -0.9561\n",
      "Epoch 39 Loss -0.9631\n",
      "{'Epoch': 39}\n",
      "Epoch 40 Batch 0 Loss -0.3454\n",
      "Epoch 40 Batch 50 Loss -0.8802\n",
      "Epoch 40 Batch 100 Loss -0.8659\n",
      "Epoch 40 Batch 150 Loss -0.8787\n",
      "Epoch 40 Batch 200 Loss -0.9200\n",
      "Epoch 40 Batch 250 Loss -0.9480\n",
      "Epoch 40 Batch 300 Loss -0.9460\n",
      "Epoch 40 Batch 350 Loss -0.9551\n",
      "Epoch 40 Loss -0.9630\n",
      "{'Epoch': 40}\n",
      "Epoch 41 Batch 0 Loss -0.3933\n",
      "Epoch 41 Batch 50 Loss -0.8883\n",
      "Epoch 41 Batch 100 Loss -0.8762\n",
      "Epoch 41 Batch 150 Loss -0.8870\n",
      "Epoch 41 Batch 200 Loss -0.9272\n",
      "Epoch 41 Batch 250 Loss -0.9543\n",
      "Epoch 41 Batch 300 Loss -0.9580\n",
      "Epoch 41 Batch 350 Loss -0.9678\n",
      "Epoch 41 Loss -0.9759\n",
      "{'Epoch': 41}\n",
      "Epoch 42 Batch 0 Loss -0.3789\n",
      "Epoch 42 Batch 50 Loss -0.9032\n",
      "Epoch 42 Batch 100 Loss -0.8812\n",
      "Epoch 42 Batch 150 Loss -0.8917\n",
      "Epoch 42 Batch 200 Loss -0.9341\n",
      "Epoch 42 Batch 250 Loss -0.9620\n",
      "Epoch 42 Batch 300 Loss -0.9615\n",
      "Epoch 42 Batch 350 Loss -0.9720\n",
      "Epoch 42 Loss -0.9787\n",
      "{'Epoch': 42}\n",
      "Epoch 43 Batch 0 Loss -0.3977\n",
      "Epoch 43 Batch 50 Loss -0.8924\n",
      "Epoch 43 Batch 100 Loss -0.8838\n",
      "Epoch 43 Batch 150 Loss -0.8992\n",
      "Epoch 43 Batch 200 Loss -0.9420\n",
      "Epoch 43 Batch 250 Loss -0.9692\n",
      "Epoch 43 Batch 300 Loss -0.9679\n",
      "Epoch 43 Batch 350 Loss -0.9745\n",
      "Epoch 43 Loss -0.9816\n",
      "{'Epoch': 43}\n",
      "Epoch 44 Batch 0 Loss -0.3891\n",
      "Epoch 44 Batch 50 Loss -0.9076\n",
      "Epoch 44 Batch 100 Loss -0.8962\n",
      "Epoch 44 Batch 150 Loss -0.9047\n",
      "Epoch 44 Batch 200 Loss -0.9434\n",
      "Epoch 44 Batch 250 Loss -0.9722\n",
      "Epoch 44 Batch 300 Loss -0.9775\n",
      "Epoch 44 Batch 350 Loss -0.9864\n",
      "Epoch 44 Loss -0.9945\n",
      "{'Epoch': 44}\n",
      "Epoch 45 Batch 0 Loss -0.4362\n",
      "Epoch 45 Batch 50 Loss -0.8949\n",
      "Epoch 45 Batch 100 Loss -0.8914\n",
      "Epoch 45 Batch 150 Loss -0.9012\n",
      "Epoch 45 Batch 200 Loss -0.9500\n",
      "Epoch 45 Batch 250 Loss -0.9768\n",
      "Epoch 45 Batch 300 Loss -0.9783\n",
      "Epoch 45 Batch 350 Loss -0.9890\n",
      "Epoch 45 Loss -0.9975\n",
      "{'Epoch': 45}\n",
      "Epoch 46 Batch 0 Loss -0.4204\n",
      "Epoch 46 Batch 50 Loss -0.9151\n",
      "Epoch 46 Batch 100 Loss -0.8880\n",
      "Epoch 46 Batch 150 Loss -0.9025\n",
      "Epoch 46 Batch 200 Loss -0.9480\n",
      "Epoch 46 Batch 250 Loss -0.9767\n",
      "Epoch 46 Batch 300 Loss -0.9815\n",
      "Epoch 46 Batch 350 Loss -0.9930\n",
      "Epoch 46 Loss -1.0026\n",
      "{'Epoch': 46}\n",
      "Epoch 47 Batch 0 Loss -0.4680\n",
      "Epoch 47 Batch 50 Loss -0.9282\n",
      "Epoch 47 Batch 100 Loss -0.9175\n",
      "Epoch 47 Batch 150 Loss -0.9303\n",
      "Epoch 47 Batch 200 Loss -0.9681\n",
      "Epoch 47 Batch 250 Loss -0.9959\n",
      "Epoch 47 Batch 300 Loss -0.9982\n",
      "Epoch 47 Batch 350 Loss -1.0086\n",
      "Epoch 47 Loss -1.0155\n",
      "{'Epoch': 47}\n",
      "Epoch 48 Batch 0 Loss -0.4430\n",
      "Epoch 48 Batch 50 Loss -0.9307\n",
      "Epoch 48 Batch 100 Loss -0.8971\n",
      "Epoch 48 Batch 150 Loss -0.9133\n",
      "Epoch 48 Batch 200 Loss -0.9622\n",
      "Epoch 48 Batch 250 Loss -0.9877\n",
      "Epoch 48 Batch 300 Loss -0.9900\n",
      "Epoch 48 Batch 350 Loss -0.9978\n",
      "Epoch 48 Loss -1.0075\n",
      "{'Epoch': 48}\n",
      "Epoch 49 Batch 0 Loss -0.4305\n",
      "Epoch 49 Batch 50 Loss -0.9213\n",
      "Epoch 49 Batch 100 Loss -0.9183\n",
      "Epoch 49 Batch 150 Loss -0.9321\n",
      "Epoch 49 Batch 200 Loss -0.9751\n",
      "Epoch 49 Batch 250 Loss -1.0026\n",
      "Epoch 49 Batch 300 Loss -1.0006\n",
      "Epoch 49 Batch 350 Loss -1.0111\n",
      "Epoch 49 Loss -1.0198\n",
      "{'Epoch': 49}\n",
      "Epoch 50 Batch 0 Loss -0.4829\n",
      "Epoch 50 Batch 50 Loss -0.9442\n",
      "Epoch 50 Batch 100 Loss -0.9230\n",
      "Epoch 50 Batch 150 Loss -0.9372\n",
      "Epoch 50 Batch 200 Loss -0.9815\n",
      "Epoch 50 Batch 250 Loss -1.0075\n",
      "Epoch 50 Batch 300 Loss -1.0098\n",
      "Epoch 50 Batch 350 Loss -1.0206\n",
      "Epoch 50 Loss -1.0286\n",
      "{'Epoch': 50}\n",
      "Epoch 51 Batch 0 Loss -0.4811\n",
      "Epoch 51 Batch 50 Loss -0.9400\n",
      "Epoch 51 Batch 100 Loss -0.9294\n",
      "Epoch 51 Batch 150 Loss -0.9418\n",
      "Epoch 51 Batch 200 Loss -0.9847\n",
      "Epoch 51 Batch 250 Loss -1.0149\n",
      "Epoch 51 Batch 300 Loss -1.0178\n",
      "Epoch 51 Batch 350 Loss -1.0282\n",
      "Epoch 51 Loss -1.0351\n",
      "{'Epoch': 51}\n",
      "Epoch 52 Batch 0 Loss -0.4663\n",
      "Epoch 52 Batch 50 Loss -0.9433\n",
      "Epoch 52 Batch 100 Loss -0.9404\n",
      "Epoch 52 Batch 150 Loss -0.9509\n",
      "Epoch 52 Batch 200 Loss -0.9925\n",
      "Epoch 52 Batch 250 Loss -1.0189\n",
      "Epoch 52 Batch 300 Loss -1.0211\n",
      "Epoch 52 Batch 350 Loss -1.0311\n",
      "Epoch 52 Loss -1.0370\n",
      "{'Epoch': 52}\n",
      "Epoch 53 Batch 0 Loss -0.4388\n",
      "Epoch 53 Batch 50 Loss -0.9687\n",
      "Epoch 53 Batch 100 Loss -0.9447\n",
      "Epoch 53 Batch 150 Loss -0.9532\n",
      "Epoch 53 Batch 200 Loss -0.9926\n",
      "Epoch 53 Batch 250 Loss -1.0212\n",
      "Epoch 53 Batch 300 Loss -1.0240\n",
      "Epoch 53 Batch 350 Loss -1.0342\n",
      "Epoch 53 Loss -1.0419\n",
      "{'Epoch': 53}\n",
      "Epoch 54 Batch 0 Loss -0.4567\n",
      "Epoch 54 Batch 50 Loss -0.9591\n",
      "Epoch 54 Batch 100 Loss -0.9445\n",
      "Epoch 54 Batch 150 Loss -0.9586\n",
      "Epoch 54 Batch 200 Loss -1.0005\n",
      "Epoch 54 Batch 250 Loss -1.0197\n",
      "Epoch 54 Batch 300 Loss -1.0241\n",
      "Epoch 54 Batch 350 Loss -1.0345\n",
      "Epoch 54 Loss -1.0403\n",
      "{'Epoch': 54}\n",
      "Epoch 55 Batch 0 Loss -0.4287\n",
      "Epoch 55 Batch 50 Loss -0.9619\n",
      "Epoch 55 Batch 100 Loss -0.9476\n",
      "Epoch 55 Batch 150 Loss -0.9625\n",
      "Epoch 55 Batch 200 Loss -1.0013\n",
      "Epoch 55 Batch 250 Loss -1.0280\n",
      "Epoch 55 Batch 300 Loss -1.0318\n",
      "Epoch 55 Batch 350 Loss -1.0428\n",
      "Epoch 55 Loss -1.0508\n",
      "{'Epoch': 55}\n",
      "Epoch 56 Batch 0 Loss -0.4898\n",
      "Epoch 56 Batch 50 Loss -0.9815\n",
      "Epoch 56 Batch 100 Loss -0.9559\n",
      "Epoch 56 Batch 150 Loss -0.9635\n",
      "Epoch 56 Batch 200 Loss -1.0094\n",
      "Epoch 56 Batch 250 Loss -1.0369\n",
      "Epoch 56 Batch 300 Loss -1.0359\n",
      "Epoch 56 Batch 350 Loss -1.0466\n",
      "Epoch 56 Loss -1.0521\n",
      "{'Epoch': 56}\n",
      "Epoch 57 Batch 0 Loss -0.4706\n",
      "Epoch 57 Batch 50 Loss -0.9748\n",
      "Epoch 57 Batch 100 Loss -0.9608\n",
      "Epoch 57 Batch 150 Loss -0.9723\n",
      "Epoch 57 Batch 200 Loss -1.0161\n",
      "Epoch 57 Batch 250 Loss -1.0445\n",
      "Epoch 57 Batch 300 Loss -1.0460\n",
      "Epoch 57 Batch 350 Loss -1.0533\n",
      "Epoch 57 Loss -1.0601\n",
      "{'Epoch': 57}\n",
      "Epoch 58 Batch 0 Loss -0.4710\n",
      "Epoch 58 Batch 50 Loss -1.0015\n",
      "Epoch 58 Batch 100 Loss -0.9661\n",
      "Epoch 58 Batch 150 Loss -0.9780\n",
      "Epoch 58 Batch 200 Loss -1.0240\n",
      "Epoch 58 Batch 250 Loss -1.0551\n",
      "Epoch 58 Batch 300 Loss -1.0586\n",
      "Epoch 58 Batch 350 Loss -1.0602\n",
      "Epoch 58 Loss -1.0682\n",
      "{'Epoch': 58}\n",
      "Epoch 59 Batch 0 Loss -0.4779\n",
      "Epoch 59 Batch 50 Loss -0.9803\n",
      "Epoch 59 Batch 100 Loss -0.9663\n",
      "Epoch 59 Batch 150 Loss -0.9798\n",
      "Epoch 59 Batch 200 Loss -1.0211\n",
      "Epoch 59 Batch 250 Loss -1.0488\n",
      "Epoch 59 Batch 300 Loss -1.0506\n",
      "Epoch 59 Batch 350 Loss -1.0620\n",
      "Epoch 59 Loss -1.0681\n",
      "{'Epoch': 59}\n",
      "Epoch 60 Batch 0 Loss -0.4910\n",
      "Epoch 60 Batch 50 Loss -0.9994\n",
      "Epoch 60 Batch 100 Loss -0.9788\n",
      "Epoch 60 Batch 150 Loss -0.9919\n",
      "Epoch 60 Batch 200 Loss -1.0349\n",
      "Epoch 60 Batch 250 Loss -1.0615\n",
      "Epoch 60 Batch 300 Loss -1.0663\n",
      "Epoch 60 Batch 350 Loss -1.0747\n",
      "Epoch 60 Loss -1.0818\n",
      "{'Epoch': 60}\n",
      "Epoch 61 Batch 0 Loss -0.5336\n",
      "Epoch 61 Batch 50 Loss -0.9964\n",
      "Epoch 61 Batch 100 Loss -0.9787\n",
      "Epoch 61 Batch 150 Loss -0.9885\n",
      "Epoch 61 Batch 200 Loss -1.0324\n",
      "Epoch 61 Batch 250 Loss -1.0606\n",
      "Epoch 61 Batch 300 Loss -1.0626\n",
      "Epoch 61 Batch 350 Loss -1.0694\n",
      "Epoch 61 Loss -1.0775\n",
      "{'Epoch': 61}\n",
      "Epoch 62 Batch 0 Loss -0.4759\n",
      "Epoch 62 Batch 50 Loss -0.9965\n",
      "Epoch 62 Batch 100 Loss -0.9879\n",
      "Epoch 62 Batch 150 Loss -0.9972\n",
      "Epoch 62 Batch 200 Loss -1.0435\n",
      "Epoch 62 Batch 250 Loss -1.0707\n",
      "Epoch 62 Batch 300 Loss -1.0712\n",
      "Epoch 62 Batch 350 Loss -1.0778\n",
      "Epoch 62 Loss -1.0868\n",
      "{'Epoch': 62}\n",
      "Epoch 63 Batch 0 Loss -0.5349\n",
      "Epoch 63 Batch 50 Loss -0.9950\n",
      "Epoch 63 Batch 100 Loss -0.9816\n",
      "Epoch 63 Batch 150 Loss -0.9953\n",
      "Epoch 63 Batch 200 Loss -1.0437\n",
      "Epoch 63 Batch 250 Loss -1.0708\n",
      "Epoch 63 Batch 300 Loss -1.0709\n",
      "Epoch 63 Batch 350 Loss -1.0808\n",
      "Epoch 63 Loss -1.0888\n",
      "{'Epoch': 63}\n",
      "Epoch 64 Batch 0 Loss -0.5487\n",
      "Epoch 64 Batch 50 Loss -1.0215\n",
      "Epoch 64 Batch 100 Loss -1.0050\n",
      "Epoch 64 Batch 150 Loss -1.0127\n",
      "Epoch 64 Batch 200 Loss -1.0543\n",
      "Epoch 64 Batch 250 Loss -1.0822\n",
      "Epoch 64 Batch 300 Loss -1.0834\n",
      "Epoch 64 Batch 350 Loss -1.0929\n",
      "Epoch 64 Loss -1.1008\n",
      "{'Epoch': 64}\n",
      "Epoch 65 Batch 0 Loss -0.5138\n",
      "Epoch 65 Batch 50 Loss -1.0231\n",
      "Epoch 65 Batch 100 Loss -1.0021\n",
      "Epoch 65 Batch 150 Loss -1.0137\n",
      "Epoch 65 Batch 200 Loss -1.0552\n",
      "Epoch 65 Batch 250 Loss -1.0795\n",
      "Epoch 65 Batch 300 Loss -1.0811\n",
      "Epoch 65 Batch 350 Loss -1.0913\n",
      "Epoch 65 Loss -1.0985\n",
      "{'Epoch': 65}\n",
      "Epoch 66 Batch 0 Loss -0.5384\n",
      "Epoch 66 Batch 50 Loss -1.0305\n",
      "Epoch 66 Batch 100 Loss -1.0089\n",
      "Epoch 66 Batch 150 Loss -1.0213\n",
      "Epoch 66 Batch 200 Loss -1.0637\n",
      "Epoch 66 Batch 250 Loss -1.0917\n",
      "Epoch 66 Batch 300 Loss -1.0932\n",
      "Epoch 66 Batch 350 Loss -1.1003\n",
      "Epoch 66 Loss -1.1082\n",
      "{'Epoch': 66}\n",
      "Epoch 67 Batch 0 Loss -0.5215\n",
      "Epoch 67 Batch 50 Loss -1.0333\n",
      "Epoch 67 Batch 100 Loss -1.0033\n",
      "Epoch 67 Batch 150 Loss -1.0176\n",
      "Epoch 67 Batch 200 Loss -1.0655\n",
      "Epoch 67 Batch 250 Loss -1.0887\n",
      "Epoch 67 Batch 300 Loss -1.0908\n",
      "Epoch 67 Batch 350 Loss -1.1007\n",
      "Epoch 67 Loss -1.1098\n",
      "{'Epoch': 67}\n",
      "Epoch 68 Batch 0 Loss -0.5680\n",
      "Epoch 68 Batch 50 Loss -1.0306\n",
      "Epoch 68 Batch 100 Loss -1.0114\n",
      "Epoch 68 Batch 150 Loss -1.0244\n",
      "Epoch 68 Batch 200 Loss -1.0685\n",
      "Epoch 68 Batch 250 Loss -1.0960\n",
      "Epoch 68 Batch 300 Loss -1.1017\n",
      "Epoch 68 Batch 350 Loss -1.1104\n",
      "Epoch 68 Loss -1.1163\n",
      "{'Epoch': 68}\n",
      "Epoch 69 Batch 0 Loss -0.5509\n",
      "Epoch 69 Batch 50 Loss -1.0456\n",
      "Epoch 69 Batch 100 Loss -1.0147\n",
      "Epoch 69 Batch 150 Loss -1.0286\n",
      "Epoch 69 Batch 200 Loss -1.0735\n",
      "Epoch 69 Batch 250 Loss -1.1007\n",
      "Epoch 69 Batch 300 Loss -1.1053\n",
      "Epoch 69 Batch 350 Loss -1.1149\n",
      "Epoch 69 Loss -1.1224\n",
      "{'Epoch': 69}\n",
      "Epoch 70 Batch 0 Loss -0.5640\n",
      "Epoch 70 Batch 50 Loss -1.0333\n",
      "Epoch 70 Batch 100 Loss -1.0200\n",
      "Epoch 70 Batch 150 Loss -1.0306\n",
      "Epoch 70 Batch 200 Loss -1.0797\n",
      "Epoch 70 Batch 250 Loss -1.1080\n",
      "Epoch 70 Batch 300 Loss -1.1092\n",
      "Epoch 70 Batch 350 Loss -1.1188\n",
      "Epoch 70 Loss -1.1270\n",
      "{'Epoch': 70}\n",
      "Epoch 71 Batch 0 Loss -0.5718\n",
      "Epoch 71 Batch 50 Loss -1.0477\n",
      "Epoch 71 Batch 100 Loss -1.0303\n",
      "Epoch 71 Batch 150 Loss -1.0419\n",
      "Epoch 71 Batch 200 Loss -1.0872\n",
      "Epoch 71 Batch 250 Loss -1.1146\n",
      "Epoch 71 Batch 300 Loss -1.1173\n",
      "Epoch 71 Batch 350 Loss -1.1241\n",
      "Epoch 71 Loss -1.1290\n",
      "{'Epoch': 71}\n",
      "Epoch 72 Batch 0 Loss -0.5776\n",
      "Epoch 72 Batch 50 Loss -1.0574\n",
      "Epoch 72 Batch 100 Loss -1.0353\n",
      "Epoch 72 Batch 150 Loss -1.0473\n",
      "Epoch 72 Batch 200 Loss -1.0910\n",
      "Epoch 72 Batch 250 Loss -1.1091\n",
      "Epoch 72 Batch 300 Loss -1.1123\n",
      "Epoch 72 Batch 350 Loss -1.1225\n",
      "Epoch 72 Loss -1.1286\n",
      "{'Epoch': 72}\n",
      "Epoch 73 Batch 0 Loss -0.5806\n",
      "Epoch 73 Batch 50 Loss -1.0658\n",
      "Epoch 73 Batch 100 Loss -1.0327\n",
      "Epoch 73 Batch 150 Loss -1.0476\n",
      "Epoch 73 Batch 200 Loss -1.0932\n",
      "Epoch 73 Batch 250 Loss -1.1181\n",
      "Epoch 73 Batch 300 Loss -1.1196\n",
      "Epoch 73 Batch 350 Loss -1.1276\n",
      "Epoch 73 Loss -1.1344\n",
      "{'Epoch': 73}\n",
      "Epoch 74 Batch 0 Loss -0.5340\n",
      "Epoch 74 Batch 50 Loss -1.0555\n",
      "Epoch 74 Batch 100 Loss -1.0397\n",
      "Epoch 74 Batch 150 Loss -1.0499\n",
      "Epoch 74 Batch 200 Loss -1.0964\n",
      "Epoch 74 Batch 250 Loss -1.1228\n",
      "Epoch 74 Batch 300 Loss -1.1243\n",
      "Epoch 74 Batch 350 Loss -1.1319\n",
      "Epoch 74 Loss -1.1389\n",
      "{'Epoch': 74}\n",
      "Epoch 75 Batch 0 Loss -0.5774\n",
      "Epoch 75 Batch 50 Loss -1.0613\n",
      "Epoch 75 Batch 100 Loss -1.0444\n",
      "Epoch 75 Batch 150 Loss -1.0564\n",
      "Epoch 75 Batch 200 Loss -1.0996\n",
      "Epoch 75 Batch 250 Loss -1.1250\n",
      "Epoch 75 Batch 300 Loss -1.1284\n",
      "Epoch 75 Batch 350 Loss -1.1383\n",
      "Epoch 75 Loss -1.1464\n",
      "{'Epoch': 75}\n",
      "Epoch 76 Batch 0 Loss -0.6164\n",
      "Epoch 76 Batch 50 Loss -1.0735\n",
      "Epoch 76 Batch 100 Loss -1.0423\n",
      "Epoch 76 Batch 150 Loss -1.0567\n",
      "Epoch 76 Batch 200 Loss -1.1028\n",
      "Epoch 76 Batch 250 Loss -1.1310\n",
      "Epoch 76 Batch 300 Loss -1.1338\n",
      "Epoch 76 Batch 350 Loss -1.1432\n",
      "Epoch 76 Loss -1.1506\n",
      "{'Epoch': 76}\n",
      "Epoch 77 Batch 0 Loss -0.5728\n",
      "Epoch 77 Batch 50 Loss -1.0788\n",
      "Epoch 77 Batch 100 Loss -1.0528\n",
      "Epoch 77 Batch 150 Loss -1.0667\n",
      "Epoch 77 Batch 200 Loss -1.1123\n",
      "Epoch 77 Batch 250 Loss -1.1398\n",
      "Epoch 77 Batch 300 Loss -1.1420\n",
      "Epoch 77 Batch 350 Loss -1.1442\n",
      "Epoch 77 Loss -1.1509\n",
      "{'Epoch': 77}\n",
      "Epoch 78 Batch 0 Loss -0.6381\n",
      "Epoch 78 Batch 50 Loss -1.0753\n",
      "Epoch 78 Batch 100 Loss -1.0599\n",
      "Epoch 78 Batch 150 Loss -1.0706\n",
      "Epoch 78 Batch 200 Loss -1.1090\n",
      "Epoch 78 Batch 250 Loss -1.1379\n",
      "Epoch 78 Batch 300 Loss -1.1416\n",
      "Epoch 78 Batch 350 Loss -1.1506\n",
      "Epoch 78 Loss -1.1570\n",
      "{'Epoch': 78}\n",
      "Epoch 79 Batch 0 Loss -0.6327\n",
      "Epoch 79 Batch 50 Loss -1.0898\n",
      "Epoch 79 Batch 100 Loss -1.0563\n",
      "Epoch 79 Batch 150 Loss -1.0701\n",
      "Epoch 79 Batch 200 Loss -1.1147\n",
      "Epoch 79 Batch 250 Loss -1.1421\n",
      "Epoch 79 Batch 300 Loss -1.1447\n",
      "Epoch 79 Batch 350 Loss -1.1545\n",
      "Epoch 79 Loss -1.1616\n",
      "{'Epoch': 79}\n",
      "Epoch 80 Batch 0 Loss -0.6388\n",
      "Epoch 80 Batch 50 Loss -1.0912\n",
      "Epoch 80 Batch 100 Loss -1.0646\n",
      "Epoch 80 Batch 150 Loss -1.0761\n",
      "Epoch 80 Batch 200 Loss -1.1203\n",
      "Epoch 80 Batch 250 Loss -1.1453\n",
      "Epoch 80 Batch 300 Loss -1.1491\n",
      "Epoch 80 Batch 350 Loss -1.1577\n",
      "Epoch 80 Loss -1.1646\n",
      "{'Epoch': 80}\n",
      "Epoch 81 Batch 0 Loss -0.6350\n",
      "Epoch 81 Batch 50 Loss -1.0993\n",
      "Epoch 81 Batch 100 Loss -1.0707\n",
      "Epoch 81 Batch 150 Loss -1.0818\n",
      "Epoch 81 Batch 200 Loss -1.1248\n",
      "Epoch 81 Batch 250 Loss -1.1485\n",
      "Epoch 81 Batch 300 Loss -1.1503\n",
      "Epoch 81 Batch 350 Loss -1.1582\n",
      "Epoch 81 Loss -1.1655\n",
      "{'Epoch': 81}\n",
      "Epoch 82 Batch 0 Loss -0.6055\n",
      "Epoch 82 Batch 50 Loss -1.0936\n",
      "Epoch 82 Batch 100 Loss -1.0752\n",
      "Epoch 82 Batch 150 Loss -1.0865\n",
      "Epoch 82 Batch 200 Loss -1.1327\n",
      "Epoch 82 Batch 250 Loss -1.1595\n",
      "Epoch 82 Batch 300 Loss -1.1599\n",
      "Epoch 82 Batch 350 Loss -1.1665\n",
      "Epoch 82 Loss -1.1739\n",
      "{'Epoch': 82}\n",
      "Epoch 83 Batch 0 Loss -0.6284\n",
      "Epoch 83 Batch 50 Loss -1.0984\n",
      "Epoch 83 Batch 100 Loss -1.0763\n",
      "Epoch 83 Batch 150 Loss -1.0883\n",
      "Epoch 83 Batch 200 Loss -1.1294\n",
      "Epoch 83 Batch 250 Loss -1.1510\n",
      "Epoch 83 Batch 300 Loss -1.1549\n",
      "Epoch 83 Batch 350 Loss -1.1545\n",
      "Epoch 83 Loss -1.1628\n",
      "{'Epoch': 83}\n",
      "Epoch 84 Batch 0 Loss -0.6390\n",
      "Epoch 84 Batch 50 Loss -1.0975\n",
      "Epoch 84 Batch 100 Loss -1.0696\n",
      "Epoch 84 Batch 150 Loss -1.0838\n",
      "Epoch 84 Batch 200 Loss -1.1289\n",
      "Epoch 84 Batch 250 Loss -1.1535\n",
      "Epoch 84 Batch 300 Loss -1.1570\n",
      "Epoch 84 Batch 350 Loss -1.1642\n",
      "Epoch 84 Loss -1.1712\n",
      "{'Epoch': 84}\n",
      "Epoch 85 Batch 0 Loss -0.6404\n",
      "Epoch 85 Batch 50 Loss -1.1048\n",
      "Epoch 85 Batch 100 Loss -1.0783\n",
      "Epoch 85 Batch 150 Loss -1.0893\n",
      "Epoch 85 Batch 200 Loss -1.1317\n",
      "Epoch 85 Batch 250 Loss -1.1543\n",
      "Epoch 85 Batch 300 Loss -1.1589\n",
      "Epoch 85 Batch 350 Loss -1.1604\n",
      "Epoch 85 Loss -1.1687\n",
      "{'Epoch': 85}\n",
      "Epoch 86 Batch 0 Loss -0.6350\n",
      "Epoch 86 Batch 50 Loss -1.1106\n",
      "Epoch 86 Batch 100 Loss -1.0825\n",
      "Epoch 86 Batch 150 Loss -1.0930\n",
      "Epoch 86 Batch 200 Loss -1.1374\n",
      "Epoch 86 Batch 250 Loss -1.1578\n",
      "Epoch 86 Batch 300 Loss -1.1604\n",
      "Epoch 86 Batch 350 Loss -1.1696\n",
      "Epoch 86 Loss -1.1773\n",
      "{'Epoch': 86}\n",
      "Epoch 87 Batch 0 Loss -0.6371\n",
      "Epoch 87 Batch 50 Loss -1.1122\n",
      "Epoch 87 Batch 100 Loss -1.0839\n",
      "Epoch 87 Batch 150 Loss -1.0929\n",
      "Epoch 87 Batch 200 Loss -1.1363\n",
      "Epoch 87 Batch 250 Loss -1.1586\n",
      "Epoch 87 Batch 300 Loss -1.1617\n",
      "Epoch 87 Batch 350 Loss -1.1679\n",
      "Epoch 87 Loss -1.1766\n",
      "{'Epoch': 87}\n",
      "Epoch 88 Batch 0 Loss -0.6556\n",
      "Epoch 88 Batch 50 Loss -1.1099\n",
      "Epoch 88 Batch 100 Loss -1.0922\n",
      "Epoch 88 Batch 150 Loss -1.1011\n",
      "Epoch 88 Batch 200 Loss -1.1432\n",
      "Epoch 88 Batch 250 Loss -1.1650\n",
      "Epoch 88 Batch 300 Loss -1.1683\n",
      "Epoch 88 Batch 350 Loss -1.1763\n",
      "Epoch 88 Loss -1.1843\n",
      "{'Epoch': 88}\n",
      "Epoch 89 Batch 0 Loss -0.6434\n",
      "Epoch 89 Batch 50 Loss -1.1217\n",
      "Epoch 89 Batch 100 Loss -1.0952\n",
      "Epoch 89 Batch 150 Loss -1.1028\n",
      "Epoch 89 Batch 200 Loss -1.1492\n",
      "Epoch 89 Batch 250 Loss -1.1747\n",
      "Epoch 89 Batch 300 Loss -1.1776\n",
      "Epoch 89 Batch 350 Loss -1.1827\n",
      "Epoch 89 Loss -1.1900\n",
      "{'Epoch': 89}\n",
      "Epoch 90 Batch 0 Loss -0.6624\n",
      "Epoch 90 Batch 50 Loss -1.1163\n",
      "Epoch 90 Batch 100 Loss -1.0885\n",
      "Epoch 90 Batch 150 Loss -1.1004\n",
      "Epoch 90 Batch 200 Loss -1.1460\n",
      "Epoch 90 Batch 250 Loss -1.1666\n",
      "Epoch 90 Batch 300 Loss -1.1709\n",
      "Epoch 90 Batch 350 Loss -1.1797\n",
      "Epoch 90 Loss -1.1876\n",
      "{'Epoch': 90}\n",
      "Epoch 91 Batch 0 Loss -0.6453\n",
      "Epoch 91 Batch 50 Loss -1.1263\n",
      "Epoch 91 Batch 100 Loss -1.1003\n",
      "Epoch 91 Batch 150 Loss -1.1081\n",
      "Epoch 91 Batch 200 Loss -1.1547\n",
      "Epoch 91 Batch 250 Loss -1.1794\n",
      "Epoch 91 Batch 300 Loss -1.1825\n",
      "Epoch 91 Batch 350 Loss -1.1905\n",
      "Epoch 91 Loss -1.1971\n",
      "{'Epoch': 91}\n",
      "Epoch 92 Batch 0 Loss -0.6692\n",
      "Epoch 92 Batch 50 Loss -1.1270\n",
      "Epoch 92 Batch 100 Loss -1.0949\n",
      "Epoch 92 Batch 150 Loss -1.1067\n",
      "Epoch 92 Batch 200 Loss -1.1538\n",
      "Epoch 92 Batch 250 Loss -1.1805\n",
      "Epoch 92 Batch 300 Loss -1.1830\n",
      "Epoch 92 Batch 350 Loss -1.1894\n",
      "Epoch 92 Loss -1.1963\n",
      "{'Epoch': 92}\n",
      "Epoch 93 Batch 0 Loss -0.6577\n",
      "Epoch 93 Batch 50 Loss -1.1202\n",
      "Epoch 93 Batch 100 Loss -1.0974\n",
      "Epoch 93 Batch 150 Loss -1.1096\n",
      "Epoch 93 Batch 200 Loss -1.1563\n",
      "Epoch 93 Batch 250 Loss -1.1817\n",
      "Epoch 93 Batch 300 Loss -1.1858\n",
      "Epoch 93 Batch 350 Loss -1.1936\n",
      "Epoch 93 Loss -1.2000\n",
      "{'Epoch': 93}\n",
      "Epoch 94 Batch 0 Loss -0.6653\n",
      "Epoch 94 Batch 50 Loss -1.1325\n",
      "Epoch 94 Batch 100 Loss -1.1049\n",
      "Epoch 94 Batch 150 Loss -1.1140\n",
      "Epoch 94 Batch 200 Loss -1.1602\n",
      "Epoch 94 Batch 250 Loss -1.1819\n",
      "Epoch 94 Batch 300 Loss -1.1855\n",
      "Epoch 94 Batch 350 Loss -1.1902\n",
      "Epoch 94 Loss -1.1979\n",
      "{'Epoch': 94}\n",
      "Epoch 95 Batch 0 Loss -0.6623\n",
      "Epoch 95 Batch 50 Loss -1.1347\n",
      "Epoch 95 Batch 100 Loss -1.1057\n",
      "Epoch 95 Batch 150 Loss -1.1160\n",
      "Epoch 95 Batch 200 Loss -1.1606\n",
      "Epoch 95 Batch 250 Loss -1.1792\n",
      "Epoch 95 Batch 300 Loss -1.1835\n",
      "Epoch 95 Batch 350 Loss -1.1935\n",
      "Epoch 95 Loss -1.2013\n",
      "{'Epoch': 95}\n",
      "Epoch 96 Batch 0 Loss -0.6850\n",
      "Epoch 96 Batch 50 Loss -1.1446\n",
      "Epoch 96 Batch 100 Loss -1.1161\n",
      "Epoch 96 Batch 150 Loss -1.1221\n",
      "Epoch 96 Batch 200 Loss -1.1649\n",
      "Epoch 96 Batch 250 Loss -1.1867\n",
      "Epoch 96 Batch 300 Loss -1.1892\n",
      "Epoch 96 Batch 350 Loss -1.1902\n",
      "Epoch 96 Loss -1.1994\n",
      "{'Epoch': 96}\n",
      "Epoch 97 Batch 0 Loss -0.6962\n",
      "Epoch 97 Batch 50 Loss -1.1460\n",
      "Epoch 97 Batch 100 Loss -1.1172\n",
      "Epoch 97 Batch 150 Loss -1.1250\n",
      "Epoch 97 Batch 200 Loss -1.1702\n",
      "Epoch 97 Batch 250 Loss -1.1935\n",
      "Epoch 97 Batch 300 Loss -1.1954\n",
      "Epoch 97 Batch 350 Loss -1.2037\n",
      "Epoch 97 Loss -1.2107\n",
      "{'Epoch': 97}\n",
      "Epoch 98 Batch 0 Loss -0.7002\n",
      "Epoch 98 Batch 50 Loss -1.1483\n",
      "Epoch 98 Batch 100 Loss -1.1187\n",
      "Epoch 98 Batch 150 Loss -1.1239\n",
      "Epoch 98 Batch 200 Loss -1.1682\n",
      "Epoch 98 Batch 250 Loss -1.1926\n",
      "Epoch 98 Batch 300 Loss -1.1945\n",
      "Epoch 98 Batch 350 Loss -1.2017\n",
      "Epoch 98 Loss -1.2091\n",
      "{'Epoch': 98}\n",
      "Epoch 99 Batch 0 Loss -0.6995\n",
      "Epoch 99 Batch 50 Loss -1.1432\n",
      "Epoch 99 Batch 100 Loss -1.1182\n",
      "Epoch 99 Batch 150 Loss -1.1274\n",
      "Epoch 99 Batch 200 Loss -1.1706\n",
      "Epoch 99 Batch 250 Loss -1.1888\n",
      "Epoch 99 Batch 300 Loss -1.1929\n",
      "Epoch 99 Batch 350 Loss -1.2014\n",
      "Epoch 99 Loss -1.2092\n",
      "{'Epoch': 99}\n",
      "Epoch 100 Batch 0 Loss -0.7139\n",
      "Epoch 100 Batch 50 Loss -1.1510\n",
      "Epoch 100 Batch 100 Loss -1.1214\n",
      "Epoch 100 Batch 150 Loss -1.1299\n",
      "Epoch 100 Batch 200 Loss -1.1743\n",
      "Epoch 100 Batch 250 Loss -1.1948\n",
      "Epoch 100 Batch 300 Loss -1.1996\n",
      "Epoch 100 Batch 350 Loss -1.2070\n",
      "Epoch 100 Loss -1.2143\n",
      "{'Epoch': 100}\n",
      "Epoch 101 Batch 0 Loss -0.7252\n",
      "Epoch 101 Batch 50 Loss -1.1383\n",
      "Epoch 101 Batch 100 Loss -1.1159\n",
      "Epoch 101 Batch 150 Loss -1.1265\n",
      "Epoch 101 Batch 200 Loss -1.1729\n",
      "Epoch 101 Batch 250 Loss -1.1967\n",
      "Epoch 101 Batch 300 Loss -1.2012\n",
      "Epoch 101 Batch 350 Loss -1.2108\n",
      "Epoch 101 Loss -1.2183\n",
      "{'Epoch': 101}\n",
      "Epoch 102 Batch 0 Loss -0.7189\n",
      "Epoch 102 Batch 50 Loss -1.1587\n",
      "Epoch 102 Batch 100 Loss -1.1306\n",
      "Epoch 102 Batch 150 Loss -1.1368\n",
      "Epoch 102 Batch 200 Loss -1.1811\n",
      "Epoch 102 Batch 250 Loss -1.2056\n",
      "Epoch 102 Batch 300 Loss -1.2076\n",
      "Epoch 102 Batch 350 Loss -1.2126\n",
      "Epoch 102 Loss -1.2206\n",
      "{'Epoch': 102}\n",
      "Epoch 103 Batch 0 Loss -0.7172\n",
      "Epoch 103 Batch 50 Loss -1.1531\n",
      "Epoch 103 Batch 100 Loss -1.1239\n",
      "Epoch 103 Batch 150 Loss -1.1320\n",
      "Epoch 103 Batch 200 Loss -1.1787\n",
      "Epoch 103 Batch 250 Loss -1.2056\n",
      "Epoch 103 Batch 300 Loss -1.2099\n",
      "Epoch 103 Batch 350 Loss -1.2170\n",
      "Epoch 103 Loss -1.2248\n",
      "{'Epoch': 103}\n",
      "Epoch 104 Batch 0 Loss -0.7299\n",
      "Epoch 104 Batch 50 Loss -1.1449\n",
      "Epoch 104 Batch 100 Loss -1.1250\n",
      "Epoch 104 Batch 150 Loss -1.1355\n",
      "Epoch 104 Batch 200 Loss -1.1816\n",
      "Epoch 104 Batch 250 Loss -1.2031\n",
      "Epoch 104 Batch 300 Loss -1.2049\n",
      "Epoch 104 Batch 350 Loss -1.2124\n",
      "Epoch 104 Loss -1.2208\n",
      "{'Epoch': 104}\n",
      "Epoch 105 Batch 0 Loss -0.7308\n",
      "Epoch 105 Batch 50 Loss -1.1562\n",
      "Epoch 105 Batch 100 Loss -1.1341\n",
      "Epoch 105 Batch 150 Loss -1.1405\n",
      "Epoch 105 Batch 200 Loss -1.1845\n",
      "Epoch 105 Batch 250 Loss -1.2072\n",
      "Epoch 105 Batch 300 Loss -1.2109\n",
      "Epoch 105 Batch 350 Loss -1.2176\n",
      "Epoch 105 Loss -1.2245\n",
      "{'Epoch': 105}\n",
      "Epoch 106 Batch 0 Loss -0.6871\n",
      "Epoch 106 Batch 50 Loss -1.1636\n",
      "Epoch 106 Batch 100 Loss -1.1351\n",
      "Epoch 106 Batch 150 Loss -1.1436\n",
      "Epoch 106 Batch 200 Loss -1.1872\n",
      "Epoch 106 Batch 250 Loss -1.2126\n",
      "Epoch 106 Batch 300 Loss -1.2151\n",
      "Epoch 106 Batch 350 Loss -1.2207\n",
      "Epoch 106 Loss -1.2281\n",
      "{'Epoch': 106}\n",
      "Epoch 107 Batch 0 Loss -0.7089\n",
      "Epoch 107 Batch 50 Loss -1.1665\n",
      "Epoch 107 Batch 100 Loss -1.1340\n",
      "Epoch 107 Batch 150 Loss -1.1420\n",
      "Epoch 107 Batch 200 Loss -1.1873\n",
      "Epoch 107 Batch 250 Loss -1.2117\n",
      "Epoch 107 Batch 300 Loss -1.2164\n",
      "Epoch 107 Batch 350 Loss -1.2219\n",
      "Epoch 107 Loss -1.2306\n",
      "{'Epoch': 107}\n",
      "Epoch 108 Batch 0 Loss -0.7497\n",
      "Epoch 108 Batch 50 Loss -1.1699\n",
      "Epoch 108 Batch 100 Loss -1.1421\n",
      "Epoch 108 Batch 150 Loss -1.1513\n",
      "Epoch 108 Batch 200 Loss -1.1944\n",
      "Epoch 108 Batch 250 Loss -1.2168\n",
      "Epoch 108 Batch 300 Loss -1.2197\n",
      "Epoch 108 Batch 350 Loss -1.2264\n",
      "Epoch 108 Loss -1.2344\n",
      "{'Epoch': 108}\n",
      "Epoch 109 Batch 0 Loss -0.7264\n",
      "Epoch 109 Batch 50 Loss -1.1687\n",
      "Epoch 109 Batch 100 Loss -1.1417\n",
      "Epoch 109 Batch 150 Loss -1.1524\n",
      "Epoch 109 Batch 200 Loss -1.1988\n",
      "Epoch 109 Batch 250 Loss -1.2177\n",
      "Epoch 109 Batch 300 Loss -1.2221\n",
      "Epoch 109 Batch 350 Loss -1.2300\n",
      "Epoch 109 Loss -1.2384\n",
      "{'Epoch': 109}\n",
      "Epoch 110 Batch 0 Loss -0.7321\n",
      "Epoch 110 Batch 50 Loss -1.1735\n",
      "Epoch 110 Batch 100 Loss -1.1502\n",
      "Epoch 110 Batch 150 Loss -1.1565\n",
      "Epoch 110 Batch 200 Loss -1.1989\n",
      "Epoch 110 Batch 250 Loss -1.2226\n",
      "Epoch 110 Batch 300 Loss -1.2265\n",
      "Epoch 110 Batch 350 Loss -1.2337\n",
      "Epoch 110 Loss -1.2416\n",
      "{'Epoch': 110}\n",
      "Epoch 111 Batch 0 Loss -0.7396\n",
      "Epoch 111 Batch 50 Loss -1.1737\n",
      "Epoch 111 Batch 100 Loss -1.1430\n",
      "Epoch 111 Batch 150 Loss -1.1511\n",
      "Epoch 111 Batch 200 Loss -1.1971\n",
      "Epoch 111 Batch 250 Loss -1.2210\n",
      "Epoch 111 Batch 300 Loss -1.2258\n",
      "Epoch 111 Batch 350 Loss -1.2304\n",
      "Epoch 111 Loss -1.2385\n",
      "{'Epoch': 111}\n",
      "Epoch 112 Batch 0 Loss -0.7315\n",
      "Epoch 112 Batch 50 Loss -1.1661\n",
      "Epoch 112 Batch 100 Loss -1.1422\n",
      "Epoch 112 Batch 150 Loss -1.1495\n",
      "Epoch 112 Batch 200 Loss -1.1965\n",
      "Epoch 112 Batch 250 Loss -1.2204\n",
      "Epoch 112 Batch 300 Loss -1.2249\n",
      "Epoch 112 Batch 350 Loss -1.2333\n",
      "Epoch 112 Loss -1.2409\n",
      "{'Epoch': 112}\n",
      "Epoch 113 Batch 0 Loss -0.7464\n",
      "Epoch 113 Batch 50 Loss -1.1621\n",
      "Epoch 113 Batch 100 Loss -1.1432\n",
      "Epoch 113 Batch 150 Loss -1.1532\n",
      "Epoch 113 Batch 200 Loss -1.1973\n",
      "Epoch 113 Batch 250 Loss -1.2228\n",
      "Epoch 113 Batch 300 Loss -1.2271\n",
      "Epoch 113 Batch 350 Loss -1.2355\n",
      "Epoch 113 Loss -1.2430\n",
      "{'Epoch': 113}\n",
      "Epoch 114 Batch 0 Loss -0.7109\n",
      "Epoch 114 Batch 50 Loss -1.1705\n",
      "Epoch 114 Batch 100 Loss -1.1503\n",
      "Epoch 114 Batch 150 Loss -1.1601\n",
      "Epoch 114 Batch 200 Loss -1.2041\n",
      "Epoch 114 Batch 250 Loss -1.2271\n",
      "Epoch 114 Batch 300 Loss -1.2322\n",
      "Epoch 114 Batch 350 Loss -1.2409\n",
      "Epoch 114 Loss -1.2488\n",
      "{'Epoch': 114}\n",
      "Epoch 115 Batch 0 Loss -0.7392\n",
      "Epoch 115 Batch 50 Loss -1.1812\n",
      "Epoch 115 Batch 100 Loss -1.1544\n",
      "Epoch 115 Batch 150 Loss -1.1612\n",
      "Epoch 115 Batch 200 Loss -1.2058\n",
      "Epoch 115 Batch 250 Loss -1.2303\n",
      "Epoch 115 Batch 300 Loss -1.2350\n",
      "Epoch 115 Batch 350 Loss -1.2421\n",
      "Epoch 115 Loss -1.2507\n",
      "{'Epoch': 115}\n",
      "Epoch 116 Batch 0 Loss -0.7615\n",
      "Epoch 116 Batch 50 Loss -1.1837\n",
      "Epoch 116 Batch 100 Loss -1.1569\n",
      "Epoch 116 Batch 150 Loss -1.1637\n",
      "Epoch 116 Batch 200 Loss -1.2090\n",
      "Epoch 116 Batch 250 Loss -1.2315\n",
      "Epoch 116 Batch 300 Loss -1.2362\n",
      "Epoch 116 Batch 350 Loss -1.2421\n",
      "Epoch 116 Loss -1.2511\n",
      "{'Epoch': 116}\n",
      "Epoch 117 Batch 0 Loss -0.7486\n",
      "Epoch 117 Batch 50 Loss -1.1882\n",
      "Epoch 117 Batch 100 Loss -1.1623\n",
      "Epoch 117 Batch 150 Loss -1.1701\n",
      "Epoch 117 Batch 200 Loss -1.2143\n",
      "Epoch 117 Batch 250 Loss -1.2374\n",
      "Epoch 117 Batch 300 Loss -1.2419\n",
      "Epoch 117 Batch 350 Loss -1.2492\n",
      "Epoch 117 Loss -1.2572\n",
      "{'Epoch': 117}\n",
      "Epoch 118 Batch 0 Loss -0.7562\n",
      "Epoch 118 Batch 50 Loss -1.1798\n",
      "Epoch 118 Batch 100 Loss -1.1591\n",
      "Epoch 118 Batch 150 Loss -1.1680\n",
      "Epoch 118 Batch 200 Loss -1.2117\n",
      "Epoch 118 Batch 250 Loss -1.2361\n",
      "Epoch 118 Batch 300 Loss -1.2412\n",
      "Epoch 118 Batch 350 Loss -1.2482\n",
      "Epoch 118 Loss -1.2566\n",
      "{'Epoch': 118}\n",
      "Epoch 119 Batch 0 Loss -0.7659\n",
      "Epoch 119 Batch 50 Loss -1.1897\n",
      "Epoch 119 Batch 100 Loss -1.1610\n",
      "Epoch 119 Batch 150 Loss -1.1675\n",
      "Epoch 119 Batch 200 Loss -1.2107\n",
      "Epoch 119 Batch 250 Loss -1.2361\n",
      "Epoch 119 Batch 300 Loss -1.2417\n",
      "Epoch 119 Batch 350 Loss -1.2500\n",
      "Epoch 119 Loss -1.2586\n",
      "{'Epoch': 119}\n",
      "Epoch 120 Batch 0 Loss -0.7820\n",
      "Epoch 120 Batch 50 Loss -1.1928\n",
      "Epoch 120 Batch 100 Loss -1.1651\n",
      "Epoch 120 Batch 150 Loss -1.1744\n",
      "Epoch 120 Batch 200 Loss -1.2196\n",
      "Epoch 120 Batch 250 Loss -1.2426\n",
      "Epoch 120 Batch 300 Loss -1.2461\n",
      "Epoch 120 Batch 350 Loss -1.2529\n",
      "Epoch 120 Loss -1.2612\n",
      "{'Epoch': 120}\n",
      "Epoch 121 Batch 0 Loss -0.7827\n",
      "Epoch 121 Batch 50 Loss -1.1874\n",
      "Epoch 121 Batch 100 Loss -1.1640\n",
      "Epoch 121 Batch 150 Loss -1.1712\n",
      "Epoch 121 Batch 200 Loss -1.2163\n",
      "Epoch 121 Batch 250 Loss -1.2404\n",
      "Epoch 121 Batch 300 Loss -1.2452\n",
      "Epoch 121 Batch 350 Loss -1.2513\n",
      "Epoch 121 Loss -1.2598\n",
      "{'Epoch': 121}\n",
      "Epoch 122 Batch 0 Loss -0.7836\n",
      "Epoch 122 Batch 50 Loss -1.1984\n",
      "Epoch 122 Batch 100 Loss -1.1696\n",
      "Epoch 122 Batch 150 Loss -1.1762\n",
      "Epoch 122 Batch 200 Loss -1.2203\n",
      "Epoch 122 Batch 250 Loss -1.2440\n",
      "Epoch 122 Batch 300 Loss -1.2481\n",
      "Epoch 122 Batch 350 Loss -1.2555\n",
      "Epoch 122 Loss -1.2647\n",
      "{'Epoch': 122}\n",
      "Epoch 123 Batch 0 Loss -0.7810\n",
      "Epoch 123 Batch 50 Loss -1.2016\n",
      "Epoch 123 Batch 100 Loss -1.1741\n",
      "Epoch 123 Batch 150 Loss -1.1808\n",
      "Epoch 123 Batch 200 Loss -1.2244\n",
      "Epoch 123 Batch 250 Loss -1.2493\n",
      "Epoch 123 Batch 300 Loss -1.2543\n",
      "Epoch 123 Batch 350 Loss -1.2604\n",
      "Epoch 123 Loss -1.2688\n",
      "{'Epoch': 123}\n",
      "Epoch 124 Batch 0 Loss -0.7784\n",
      "Epoch 124 Batch 50 Loss -1.2028\n",
      "Epoch 124 Batch 100 Loss -1.1752\n",
      "Epoch 124 Batch 150 Loss -1.1835\n",
      "Epoch 124 Batch 200 Loss -1.2277\n",
      "Epoch 124 Batch 250 Loss -1.2501\n",
      "Epoch 124 Batch 300 Loss -1.2553\n",
      "Epoch 124 Batch 350 Loss -1.2610\n",
      "Epoch 124 Loss -1.2702\n",
      "{'Epoch': 124}\n",
      "Epoch 125 Batch 0 Loss -0.7924\n",
      "Epoch 125 Batch 50 Loss -1.2020\n",
      "Epoch 125 Batch 100 Loss -1.1773\n",
      "Epoch 125 Batch 150 Loss -1.1835\n",
      "Epoch 125 Batch 200 Loss -1.2300\n",
      "Epoch 125 Batch 250 Loss -1.2518\n",
      "Epoch 125 Batch 300 Loss -1.2567\n",
      "Epoch 125 Batch 350 Loss -1.2621\n",
      "Epoch 125 Loss -1.2714\n",
      "{'Epoch': 125}\n",
      "Epoch 126 Batch 0 Loss -0.8044\n",
      "Epoch 126 Batch 50 Loss -1.2093\n",
      "Epoch 126 Batch 100 Loss -1.1812\n",
      "Epoch 126 Batch 150 Loss -1.1873\n",
      "Epoch 126 Batch 200 Loss -1.2328\n",
      "Epoch 126 Batch 250 Loss -1.2553\n",
      "Epoch 126 Batch 300 Loss -1.2594\n",
      "Epoch 126 Batch 350 Loss -1.2642\n",
      "Epoch 126 Loss -1.2732\n",
      "{'Epoch': 126}\n",
      "Epoch 127 Batch 0 Loss -0.7933\n",
      "Epoch 127 Batch 50 Loss -1.2064\n",
      "Epoch 127 Batch 100 Loss -1.1781\n",
      "Epoch 127 Batch 150 Loss -1.1861\n",
      "Epoch 127 Batch 200 Loss -1.2312\n",
      "Epoch 127 Batch 250 Loss -1.2555\n",
      "Epoch 127 Batch 300 Loss -1.2607\n",
      "Epoch 127 Batch 350 Loss -1.2675\n",
      "Epoch 127 Loss -1.2763\n",
      "{'Epoch': 127}\n",
      "Epoch 128 Batch 0 Loss -0.7955\n",
      "Epoch 128 Batch 50 Loss -1.2086\n",
      "Epoch 128 Batch 100 Loss -1.1830\n",
      "Epoch 128 Batch 150 Loss -1.1897\n",
      "Epoch 128 Batch 200 Loss -1.2358\n",
      "Epoch 128 Batch 250 Loss -1.2586\n",
      "Epoch 128 Batch 300 Loss -1.2635\n",
      "Epoch 128 Batch 350 Loss -1.2702\n",
      "Epoch 128 Loss -1.2790\n",
      "{'Epoch': 128}\n",
      "Epoch 129 Batch 0 Loss -0.7959\n",
      "Epoch 129 Batch 50 Loss -1.2148\n",
      "Epoch 129 Batch 100 Loss -1.1858\n",
      "Epoch 129 Batch 150 Loss -1.1926\n",
      "Epoch 129 Batch 200 Loss -1.2368\n",
      "Epoch 129 Batch 250 Loss -1.2599\n",
      "Epoch 129 Batch 300 Loss -1.2647\n",
      "Epoch 129 Batch 350 Loss -1.2716\n",
      "Epoch 129 Loss -1.2803\n",
      "{'Epoch': 129}\n",
      "Epoch 130 Batch 0 Loss -0.7672\n",
      "Epoch 130 Batch 50 Loss -1.2162\n",
      "Epoch 130 Batch 100 Loss -1.1882\n",
      "Epoch 130 Batch 150 Loss -1.1952\n",
      "Epoch 130 Batch 200 Loss -1.2367\n",
      "Epoch 130 Batch 250 Loss -1.2603\n",
      "Epoch 130 Batch 300 Loss -1.2654\n",
      "Epoch 130 Batch 350 Loss -1.2723\n",
      "Epoch 130 Loss -1.2814\n",
      "{'Epoch': 130}\n",
      "Epoch 131 Batch 0 Loss -0.8000\n",
      "Epoch 131 Batch 50 Loss -1.2157\n",
      "Epoch 131 Batch 100 Loss -1.1869\n",
      "Epoch 131 Batch 150 Loss -1.1951\n",
      "Epoch 131 Batch 200 Loss -1.2378\n",
      "Epoch 131 Batch 250 Loss -1.2629\n",
      "Epoch 131 Batch 300 Loss -1.2675\n",
      "Epoch 131 Batch 350 Loss -1.2732\n",
      "Epoch 131 Loss -1.2820\n",
      "{'Epoch': 131}\n",
      "Epoch 132 Batch 0 Loss -0.8175\n",
      "Epoch 132 Batch 50 Loss -1.2143\n",
      "Epoch 132 Batch 100 Loss -1.1911\n",
      "Epoch 132 Batch 150 Loss -1.1989\n",
      "Epoch 132 Batch 200 Loss -1.2424\n",
      "Epoch 132 Batch 250 Loss -1.2670\n",
      "Epoch 132 Batch 300 Loss -1.2721\n",
      "Epoch 132 Batch 350 Loss -1.2789\n",
      "Epoch 132 Loss -1.2876\n",
      "{'Epoch': 132}\n",
      "Epoch 133 Batch 0 Loss -0.8145\n",
      "Epoch 133 Batch 50 Loss -1.2140\n",
      "Epoch 133 Batch 100 Loss -1.1891\n",
      "Epoch 133 Batch 150 Loss -1.1967\n",
      "Epoch 133 Batch 200 Loss -1.2427\n",
      "Epoch 133 Batch 250 Loss -1.2658\n",
      "Epoch 133 Batch 300 Loss -1.2710\n",
      "Epoch 133 Batch 350 Loss -1.2771\n",
      "Epoch 133 Loss -1.2863\n",
      "{'Epoch': 133}\n",
      "Epoch 134 Batch 0 Loss -0.8187\n",
      "Epoch 134 Batch 50 Loss -1.2214\n",
      "Epoch 134 Batch 100 Loss -1.1945\n",
      "Epoch 134 Batch 150 Loss -1.2021\n",
      "Epoch 134 Batch 200 Loss -1.2450\n",
      "Epoch 134 Batch 250 Loss -1.2702\n",
      "Epoch 134 Batch 300 Loss -1.2749\n",
      "Epoch 134 Batch 350 Loss -1.2818\n",
      "Epoch 134 Loss -1.2909\n",
      "{'Epoch': 134}\n",
      "Epoch 135 Batch 0 Loss -0.8146\n",
      "Epoch 135 Batch 50 Loss -1.2229\n",
      "Epoch 135 Batch 100 Loss -1.1958\n",
      "Epoch 135 Batch 150 Loss -1.2020\n",
      "Epoch 135 Batch 200 Loss -1.2468\n",
      "Epoch 135 Batch 250 Loss -1.2702\n",
      "Epoch 135 Batch 300 Loss -1.2750\n",
      "Epoch 135 Batch 350 Loss -1.2815\n",
      "Epoch 135 Loss -1.2906\n",
      "{'Epoch': 135}\n",
      "Epoch 136 Batch 0 Loss -0.8065\n",
      "Epoch 136 Batch 50 Loss -1.2208\n",
      "Epoch 136 Batch 100 Loss -1.1988\n",
      "Epoch 136 Batch 150 Loss -1.2067\n",
      "Epoch 136 Batch 200 Loss -1.2505\n",
      "Epoch 136 Batch 250 Loss -1.2761\n",
      "Epoch 136 Batch 300 Loss -1.2809\n",
      "Epoch 136 Batch 350 Loss -1.2876\n",
      "Epoch 136 Loss -1.2963\n",
      "{'Epoch': 136}\n",
      "Epoch 137 Batch 0 Loss -0.8209\n",
      "Epoch 137 Batch 50 Loss -1.2247\n",
      "Epoch 137 Batch 100 Loss -1.1993\n",
      "Epoch 137 Batch 150 Loss -1.2077\n",
      "Epoch 137 Batch 200 Loss -1.2521\n",
      "Epoch 137 Batch 250 Loss -1.2762\n",
      "Epoch 137 Batch 300 Loss -1.2811\n",
      "Epoch 137 Batch 350 Loss -1.2877\n",
      "Epoch 137 Loss -1.2968\n",
      "{'Epoch': 137}\n",
      "Epoch 138 Batch 0 Loss -0.8349\n",
      "Epoch 138 Batch 50 Loss -1.2208\n",
      "Epoch 138 Batch 100 Loss -1.1967\n",
      "Epoch 138 Batch 150 Loss -1.2061\n",
      "Epoch 138 Batch 200 Loss -1.2488\n",
      "Epoch 138 Batch 250 Loss -1.2738\n",
      "Epoch 138 Batch 300 Loss -1.2789\n",
      "Epoch 138 Batch 350 Loss -1.2869\n",
      "Epoch 138 Loss -1.2961\n",
      "{'Epoch': 138}\n",
      "Epoch 139 Batch 0 Loss -0.8478\n",
      "Epoch 139 Batch 50 Loss -1.2266\n",
      "Epoch 139 Batch 100 Loss -1.2043\n",
      "Epoch 139 Batch 150 Loss -1.2121\n",
      "Epoch 139 Batch 200 Loss -1.2565\n",
      "Epoch 139 Batch 250 Loss -1.2815\n",
      "Epoch 139 Batch 300 Loss -1.2864\n",
      "Epoch 139 Batch 350 Loss -1.2931\n",
      "Epoch 139 Loss -1.3016\n",
      "{'Epoch': 139}\n",
      "Epoch 140 Batch 0 Loss -0.8441\n",
      "Epoch 140 Batch 50 Loss -1.2256\n",
      "Epoch 140 Batch 100 Loss -1.2017\n",
      "Epoch 140 Batch 150 Loss -1.2101\n",
      "Epoch 140 Batch 200 Loss -1.2551\n",
      "Epoch 140 Batch 250 Loss -1.2815\n",
      "Epoch 140 Batch 300 Loss -1.2868\n",
      "Epoch 140 Batch 350 Loss -1.2960\n",
      "Epoch 140 Loss -1.3056\n",
      "{'Epoch': 140}\n",
      "Epoch 141 Batch 0 Loss -0.8312\n",
      "Epoch 141 Batch 50 Loss -1.2376\n",
      "Epoch 141 Batch 100 Loss -1.2133\n",
      "Epoch 141 Batch 150 Loss -1.2283\n",
      "Epoch 141 Batch 200 Loss -1.2726\n",
      "Epoch 141 Batch 250 Loss -1.2973\n",
      "Epoch 141 Batch 300 Loss -1.2953\n",
      "Epoch 141 Batch 350 Loss -1.3040\n",
      "Epoch 141 Loss -1.3141\n",
      "{'Epoch': 141}\n",
      "Epoch 142 Batch 0 Loss -0.8746\n",
      "Epoch 142 Batch 50 Loss -1.2413\n",
      "Epoch 142 Batch 100 Loss -1.2149\n",
      "Epoch 142 Batch 150 Loss -1.2200\n",
      "Epoch 142 Batch 200 Loss -1.2703\n",
      "Epoch 142 Batch 250 Loss -1.2922\n",
      "Epoch 142 Batch 300 Loss -1.2984\n",
      "Epoch 142 Batch 350 Loss -1.3077\n",
      "Epoch 142 Loss -1.3073\n",
      "{'Epoch': 142}\n",
      "Epoch 143 Batch 0 Loss -0.8516\n",
      "Epoch 143 Batch 50 Loss -1.2260\n",
      "Epoch 143 Batch 100 Loss -1.2079\n",
      "Epoch 143 Batch 150 Loss -1.2186\n",
      "Epoch 143 Batch 200 Loss -1.2744\n",
      "Epoch 143 Batch 250 Loss -1.2957\n",
      "Epoch 143 Batch 300 Loss -1.2887\n",
      "Epoch 143 Batch 350 Loss -1.2978\n",
      "Epoch 143 Loss -1.3084\n",
      "{'Epoch': 143}\n",
      "Epoch 144 Batch 0 Loss -0.8485\n",
      "Epoch 144 Batch 50 Loss -1.2587\n",
      "Epoch 144 Batch 100 Loss -1.2292\n",
      "Epoch 144 Batch 150 Loss -1.2370\n",
      "Epoch 144 Batch 200 Loss -1.2846\n",
      "Epoch 144 Batch 250 Loss -1.3140\n",
      "Epoch 144 Batch 300 Loss -1.3229\n",
      "Epoch 144 Batch 350 Loss -1.3191\n",
      "Epoch 144 Loss -1.3276\n",
      "{'Epoch': 144}\n",
      "Epoch 145 Batch 0 Loss -0.8048\n",
      "Epoch 145 Batch 50 Loss -1.2472\n",
      "Epoch 145 Batch 100 Loss -1.2079\n",
      "Epoch 145 Batch 150 Loss -1.2122\n",
      "Epoch 145 Batch 200 Loss -1.2613\n",
      "Epoch 145 Batch 250 Loss -1.2911\n",
      "Epoch 145 Batch 300 Loss -1.2994\n",
      "Epoch 145 Batch 350 Loss -1.3094\n",
      "Epoch 145 Loss -1.3149\n",
      "{'Epoch': 145}\n",
      "Epoch 146 Batch 0 Loss -0.8178\n",
      "Epoch 146 Batch 50 Loss -1.2673\n",
      "Epoch 146 Batch 100 Loss -1.2567\n",
      "Epoch 146 Batch 150 Loss -1.2719\n",
      "Epoch 146 Batch 200 Loss -1.3145\n",
      "Epoch 146 Batch 250 Loss -1.3354\n",
      "Epoch 146 Batch 300 Loss -1.3163\n",
      "Epoch 146 Batch 350 Loss -1.3263\n",
      "Epoch 146 Loss -1.3378\n",
      "{'Epoch': 146}\n",
      "Epoch 147 Batch 0 Loss -0.8893\n",
      "Epoch 147 Batch 50 Loss -1.2885\n",
      "Epoch 147 Batch 100 Loss -1.2601\n",
      "Epoch 147 Batch 150 Loss -1.2641\n",
      "Epoch 147 Batch 200 Loss -1.3054\n",
      "Epoch 147 Batch 250 Loss -1.3276\n",
      "Epoch 147 Batch 300 Loss -1.3223\n",
      "Epoch 147 Batch 350 Loss -1.3372\n",
      "Epoch 147 Loss -1.3489\n",
      "{'Epoch': 147}\n",
      "Epoch 148 Batch 0 Loss -0.8690\n",
      "Epoch 148 Batch 50 Loss -1.2996\n",
      "Epoch 148 Batch 100 Loss -1.2551\n",
      "Epoch 148 Batch 150 Loss -1.2608\n",
      "Epoch 148 Batch 200 Loss -1.3037\n",
      "Epoch 148 Batch 250 Loss -1.3165\n",
      "Epoch 148 Batch 300 Loss -1.3181\n",
      "Epoch 148 Batch 350 Loss -1.3304\n",
      "Epoch 148 Loss -1.3409\n",
      "{'Epoch': 148}\n",
      "Epoch 149 Batch 0 Loss -0.8061\n",
      "Epoch 149 Batch 50 Loss -1.2249\n",
      "Epoch 149 Batch 100 Loss -1.2109\n",
      "Epoch 149 Batch 150 Loss -1.2285\n",
      "Epoch 149 Batch 200 Loss -1.2753\n",
      "Epoch 149 Batch 250 Loss -1.2996\n",
      "Epoch 149 Batch 300 Loss -1.3117\n",
      "Epoch 149 Batch 350 Loss -1.3208\n",
      "Epoch 149 Loss -1.3248\n",
      "{'Epoch': 149}\n",
      "Epoch 150 Batch 0 Loss -0.7630\n",
      "Epoch 150 Batch 50 Loss -1.2281\n",
      "Epoch 150 Batch 100 Loss -1.2130\n",
      "Epoch 150 Batch 150 Loss -1.2370\n",
      "Epoch 150 Batch 200 Loss -1.2831\n",
      "Epoch 150 Batch 250 Loss -1.3136\n",
      "Epoch 150 Batch 300 Loss -1.3183\n",
      "Epoch 150 Batch 350 Loss -1.3310\n",
      "Epoch 150 Loss -1.3396\n",
      "{'Epoch': 150}\n",
      "Epoch 151 Batch 0 Loss -0.8481\n",
      "Epoch 151 Batch 50 Loss -1.2349\n",
      "Epoch 151 Batch 100 Loss -1.2302\n",
      "Epoch 151 Batch 150 Loss -1.2520\n",
      "Epoch 151 Batch 200 Loss -1.3076\n",
      "Epoch 151 Batch 250 Loss -1.3309\n",
      "Epoch 151 Batch 300 Loss -1.3296\n",
      "Epoch 151 Batch 350 Loss -1.3373\n",
      "Epoch 151 Loss -1.3435\n",
      "{'Epoch': 151}\n",
      "Epoch 152 Batch 0 Loss -0.8869\n",
      "Epoch 152 Batch 50 Loss -1.2736\n",
      "Epoch 152 Batch 100 Loss -1.2456\n",
      "Epoch 152 Batch 150 Loss -1.2547\n",
      "Epoch 152 Batch 200 Loss -1.3100\n",
      "Epoch 152 Batch 250 Loss -1.3443\n",
      "Epoch 152 Batch 300 Loss -1.3567\n",
      "Epoch 152 Batch 350 Loss -1.3715\n",
      "Epoch 152 Loss -1.3825\n",
      "{'Epoch': 152}\n",
      "Epoch 153 Batch 0 Loss -0.9497\n",
      "Epoch 153 Batch 50 Loss -1.3463\n",
      "Epoch 153 Batch 100 Loss -1.2863\n",
      "Epoch 153 Batch 150 Loss -1.2830\n",
      "Epoch 153 Batch 200 Loss -1.3108\n",
      "Epoch 153 Batch 250 Loss -1.3426\n",
      "Epoch 153 Batch 300 Loss -1.3527\n",
      "Epoch 153 Batch 350 Loss -1.3559\n",
      "Epoch 153 Loss -1.3649\n",
      "{'Epoch': 153}\n",
      "Epoch 154 Batch 0 Loss -0.8825\n",
      "Epoch 154 Batch 50 Loss -1.2708\n",
      "Epoch 154 Batch 100 Loss -1.2380\n",
      "Epoch 154 Batch 150 Loss -1.2516\n",
      "Epoch 154 Batch 200 Loss -1.3012\n",
      "Epoch 154 Batch 250 Loss -1.3198\n",
      "Epoch 154 Batch 300 Loss -1.3148\n",
      "Epoch 154 Batch 350 Loss -1.3251\n",
      "Epoch 154 Loss -1.3374\n",
      "{'Epoch': 154}\n",
      "Epoch 155 Batch 0 Loss -0.8583\n",
      "Epoch 155 Batch 50 Loss -1.2950\n",
      "Epoch 155 Batch 100 Loss -1.2546\n",
      "Epoch 155 Batch 150 Loss -1.2694\n",
      "Epoch 155 Batch 200 Loss -1.3069\n",
      "Epoch 155 Batch 250 Loss -1.3302\n",
      "Epoch 155 Batch 300 Loss -1.3219\n",
      "Epoch 155 Batch 350 Loss -1.3339\n",
      "Epoch 155 Loss -1.3459\n",
      "{'Epoch': 155}\n",
      "Epoch 156 Batch 0 Loss -0.8914\n",
      "Epoch 156 Batch 50 Loss -1.3150\n",
      "Epoch 156 Batch 100 Loss -1.2731\n",
      "Epoch 156 Batch 150 Loss -1.2798\n",
      "Epoch 156 Batch 200 Loss -1.3242\n",
      "Epoch 156 Batch 250 Loss -1.3516\n",
      "Epoch 156 Batch 300 Loss -1.3444\n",
      "Epoch 156 Batch 350 Loss -1.3527\n",
      "Epoch 156 Loss -1.3631\n",
      "{'Epoch': 156}\n",
      "Epoch 157 Batch 0 Loss -0.8874\n",
      "Epoch 157 Batch 50 Loss -1.3311\n",
      "Epoch 157 Batch 100 Loss -1.2969\n",
      "Epoch 157 Batch 150 Loss -1.2944\n",
      "Epoch 157 Batch 200 Loss -1.3212\n",
      "Epoch 157 Batch 250 Loss -1.3355\n",
      "Epoch 157 Batch 300 Loss -1.3436\n",
      "Epoch 157 Batch 350 Loss -1.3567\n",
      "Epoch 157 Loss -1.3655\n",
      "{'Epoch': 157}\n",
      "Epoch 158 Batch 0 Loss -0.8963\n",
      "Epoch 158 Batch 50 Loss -1.3062\n",
      "Epoch 158 Batch 100 Loss -1.2921\n",
      "Epoch 158 Batch 150 Loss -1.3032\n",
      "Epoch 158 Batch 200 Loss -1.3496\n",
      "Epoch 158 Batch 250 Loss -1.3777\n",
      "Epoch 158 Batch 300 Loss -1.3769\n",
      "Epoch 158 Batch 350 Loss -1.3746\n",
      "Epoch 158 Loss -1.3768\n",
      "{'Epoch': 158}\n",
      "Epoch 159 Batch 0 Loss -0.8505\n",
      "Epoch 159 Batch 50 Loss -1.2792\n",
      "Epoch 159 Batch 100 Loss -1.2616\n",
      "Epoch 159 Batch 150 Loss -1.2702\n",
      "Epoch 159 Batch 200 Loss -1.3210\n",
      "Epoch 159 Batch 250 Loss -1.3470\n",
      "Epoch 159 Batch 300 Loss -1.3515\n",
      "Epoch 159 Batch 350 Loss -1.3624\n",
      "Epoch 159 Loss -1.3732\n",
      "{'Epoch': 159}\n",
      "Epoch 160 Batch 0 Loss -0.9142\n",
      "Epoch 160 Batch 50 Loss -1.3036\n",
      "Epoch 160 Batch 100 Loss -1.2509\n",
      "Epoch 160 Batch 150 Loss -1.2493\n",
      "Epoch 160 Batch 200 Loss -1.2976\n",
      "Epoch 160 Batch 250 Loss -1.3362\n",
      "Epoch 160 Batch 300 Loss -1.3403\n",
      "Epoch 160 Batch 350 Loss -1.3498\n",
      "Epoch 160 Loss -1.3575\n",
      "{'Epoch': 160}\n",
      "Epoch 161 Batch 0 Loss -0.7747\n",
      "Epoch 161 Batch 50 Loss -1.2817\n",
      "Epoch 161 Batch 100 Loss -1.2616\n",
      "Epoch 161 Batch 150 Loss -1.2775\n",
      "Epoch 161 Batch 200 Loss -1.3329\n",
      "Epoch 161 Batch 250 Loss -1.3569\n",
      "Epoch 161 Batch 300 Loss -1.3561\n",
      "Epoch 161 Batch 350 Loss -1.3637\n",
      "Epoch 161 Loss -1.3716\n",
      "{'Epoch': 161}\n",
      "Epoch 162 Batch 0 Loss -0.8679\n",
      "Epoch 162 Batch 50 Loss -1.3284\n",
      "Epoch 162 Batch 100 Loss -1.3088\n",
      "Epoch 162 Batch 150 Loss -1.3177\n",
      "Epoch 162 Batch 200 Loss -1.3618\n",
      "Epoch 162 Batch 250 Loss -1.3816\n",
      "Epoch 162 Batch 300 Loss -1.3779\n",
      "Epoch 162 Batch 350 Loss -1.3785\n",
      "Epoch 162 Loss -1.3836\n",
      "{'Epoch': 162}\n",
      "Epoch 163 Batch 0 Loss -0.8903\n",
      "Epoch 163 Batch 50 Loss -1.3232\n",
      "Epoch 163 Batch 100 Loss -1.3021\n",
      "Epoch 163 Batch 150 Loss -1.3108\n",
      "Epoch 163 Batch 200 Loss -1.3427\n",
      "Epoch 163 Batch 250 Loss -1.3599\n",
      "Epoch 163 Batch 300 Loss -1.3703\n",
      "Epoch 163 Batch 350 Loss -1.3883\n",
      "Epoch 163 Loss -1.4002\n",
      "{'Epoch': 163}\n",
      "Epoch 164 Batch 0 Loss -0.8774\n",
      "Epoch 164 Batch 50 Loss -1.3641\n",
      "Epoch 164 Batch 100 Loss -1.3486\n",
      "Epoch 164 Batch 150 Loss -1.3594\n",
      "Epoch 164 Batch 200 Loss -1.4033\n",
      "Epoch 164 Batch 250 Loss -1.4151\n",
      "Epoch 164 Batch 300 Loss -1.3944\n",
      "Epoch 164 Batch 350 Loss -1.4026\n",
      "Epoch 164 Loss -1.4111\n",
      "{'Epoch': 164}\n",
      "Epoch 165 Batch 0 Loss -0.9329\n",
      "Epoch 165 Batch 50 Loss -1.3497\n",
      "Epoch 165 Batch 100 Loss -1.3182\n",
      "Epoch 165 Batch 150 Loss -1.3120\n",
      "Epoch 165 Batch 200 Loss -1.3515\n",
      "Epoch 165 Batch 250 Loss -1.3678\n",
      "Epoch 165 Batch 300 Loss -1.3578\n",
      "Epoch 165 Batch 350 Loss -1.3652\n",
      "Epoch 165 Loss -1.3749\n",
      "{'Epoch': 165}\n",
      "Epoch 166 Batch 0 Loss -0.8782\n",
      "Epoch 166 Batch 50 Loss -1.3285\n",
      "Epoch 166 Batch 100 Loss -1.3131\n",
      "Epoch 166 Batch 150 Loss -1.3203\n",
      "Epoch 166 Batch 200 Loss -1.3604\n",
      "Epoch 166 Batch 250 Loss -1.3868\n",
      "Epoch 166 Batch 300 Loss -1.3890\n",
      "Epoch 166 Batch 350 Loss -1.3946\n",
      "Epoch 166 Loss -1.4001\n",
      "{'Epoch': 166}\n",
      "Epoch 167 Batch 0 Loss -0.7907\n",
      "Epoch 167 Batch 50 Loss -1.2815\n",
      "Epoch 167 Batch 100 Loss -1.2758\n",
      "Epoch 167 Batch 150 Loss -1.2953\n",
      "Epoch 167 Batch 200 Loss -1.3450\n",
      "Epoch 167 Batch 250 Loss -1.3670\n",
      "Epoch 167 Batch 300 Loss -1.3728\n",
      "Epoch 167 Batch 350 Loss -1.3775\n",
      "Epoch 167 Loss -1.3824\n",
      "{'Epoch': 167}\n",
      "Epoch 168 Batch 0 Loss -0.8536\n",
      "Epoch 168 Batch 50 Loss -1.2437\n",
      "Epoch 168 Batch 100 Loss -1.2339\n",
      "Epoch 168 Batch 150 Loss -1.2721\n",
      "Epoch 168 Batch 200 Loss -1.3219\n",
      "Epoch 168 Batch 250 Loss -1.3420\n",
      "Epoch 168 Batch 300 Loss -1.3485\n",
      "Epoch 168 Batch 350 Loss -1.3623\n",
      "Epoch 168 Loss -1.3724\n",
      "{'Epoch': 168}\n",
      "Epoch 169 Batch 0 Loss -0.9047\n",
      "Epoch 169 Batch 50 Loss -1.2849\n",
      "Epoch 169 Batch 100 Loss -1.2654\n",
      "Epoch 169 Batch 150 Loss -1.2820\n",
      "Epoch 169 Batch 200 Loss -1.3382\n",
      "Epoch 169 Batch 250 Loss -1.3622\n",
      "Epoch 169 Batch 300 Loss -1.3598\n",
      "Epoch 169 Batch 350 Loss -1.3708\n",
      "Epoch 169 Loss -1.3814\n",
      "{'Epoch': 169}\n",
      "Epoch 170 Batch 0 Loss -0.9009\n",
      "Epoch 170 Batch 50 Loss -1.2883\n",
      "Epoch 170 Batch 100 Loss -1.2764\n",
      "Epoch 170 Batch 150 Loss -1.2810\n",
      "Epoch 170 Batch 200 Loss -1.3343\n",
      "Epoch 170 Batch 250 Loss -1.3746\n",
      "Epoch 170 Batch 300 Loss -1.3764\n",
      "Epoch 170 Batch 350 Loss -1.3857\n",
      "Epoch 170 Loss -1.3861\n",
      "{'Epoch': 170}\n",
      "Epoch 171 Batch 0 Loss -0.8918\n",
      "Epoch 171 Batch 50 Loss -1.2922\n",
      "Epoch 171 Batch 100 Loss -1.2771\n",
      "Epoch 171 Batch 150 Loss -1.2924\n",
      "Epoch 171 Batch 200 Loss -1.3475\n",
      "Epoch 171 Batch 250 Loss -1.3767\n",
      "Epoch 171 Batch 300 Loss -1.3817\n",
      "Epoch 171 Batch 350 Loss -1.3877\n",
      "Epoch 171 Loss -1.3977\n",
      "{'Epoch': 171}\n",
      "Epoch 172 Batch 0 Loss -0.9446\n",
      "Epoch 172 Batch 50 Loss -1.3084\n",
      "Epoch 172 Batch 100 Loss -1.2902\n",
      "Epoch 172 Batch 150 Loss -1.3071\n",
      "Epoch 172 Batch 200 Loss -1.3578\n",
      "Epoch 172 Batch 250 Loss -1.3879\n",
      "Epoch 172 Batch 300 Loss -1.3941\n",
      "Epoch 172 Batch 350 Loss -1.3990\n",
      "Epoch 172 Loss -1.4052\n",
      "{'Epoch': 172}\n",
      "Epoch 173 Batch 0 Loss -0.9559\n",
      "Epoch 173 Batch 50 Loss -1.3250\n",
      "Epoch 173 Batch 100 Loss -1.2886\n",
      "Epoch 173 Batch 150 Loss -1.3063\n",
      "Epoch 173 Batch 200 Loss -1.3577\n",
      "Epoch 173 Batch 250 Loss -1.3874\n",
      "Epoch 173 Batch 300 Loss -1.3940\n",
      "Epoch 173 Batch 350 Loss -1.4056\n",
      "Epoch 173 Loss -1.4138\n",
      "{'Epoch': 173}\n",
      "Epoch 174 Batch 0 Loss -0.8033\n",
      "Epoch 174 Batch 50 Loss -1.3152\n",
      "Epoch 174 Batch 100 Loss -1.2784\n",
      "Epoch 174 Batch 150 Loss -1.2771\n",
      "Epoch 174 Batch 200 Loss -1.3388\n",
      "Epoch 174 Batch 250 Loss -1.3716\n",
      "Epoch 174 Batch 300 Loss -1.3771\n",
      "Epoch 174 Batch 350 Loss -1.3879\n",
      "Epoch 174 Loss -1.3963\n",
      "{'Epoch': 174}\n",
      "Epoch 175 Batch 0 Loss -0.9184\n",
      "Epoch 175 Batch 50 Loss -1.3416\n",
      "Epoch 175 Batch 100 Loss -1.3294\n",
      "Epoch 175 Batch 150 Loss -1.3321\n",
      "Epoch 175 Batch 200 Loss -1.3800\n",
      "Epoch 175 Batch 250 Loss -1.4036\n",
      "Epoch 175 Batch 300 Loss -1.3949\n",
      "Epoch 175 Batch 350 Loss -1.4055\n",
      "Epoch 175 Loss -1.4166\n",
      "{'Epoch': 175}\n",
      "Epoch 176 Batch 0 Loss -0.9266\n",
      "Epoch 176 Batch 50 Loss -1.3089\n",
      "Epoch 176 Batch 100 Loss -1.2911\n",
      "Epoch 176 Batch 150 Loss -1.3132\n",
      "Epoch 176 Batch 200 Loss -1.3633\n",
      "Epoch 176 Batch 250 Loss -1.3832\n",
      "Epoch 176 Batch 300 Loss -1.3796\n",
      "Epoch 176 Batch 350 Loss -1.3935\n",
      "Epoch 176 Loss -1.4020\n",
      "{'Epoch': 176}\n",
      "Epoch 177 Batch 0 Loss -0.9390\n",
      "Epoch 177 Batch 50 Loss -1.3555\n",
      "Epoch 177 Batch 100 Loss -1.3280\n",
      "Epoch 177 Batch 150 Loss -1.3287\n",
      "Epoch 177 Batch 200 Loss -1.3702\n",
      "Epoch 177 Batch 250 Loss -1.3926\n",
      "Epoch 177 Batch 300 Loss -1.3983\n",
      "Epoch 177 Batch 350 Loss -1.4113\n",
      "Epoch 177 Loss -1.4203\n",
      "{'Epoch': 177}\n",
      "Epoch 178 Batch 0 Loss -0.9181\n",
      "Epoch 178 Batch 50 Loss -1.3052\n",
      "Epoch 178 Batch 100 Loss -1.2845\n",
      "Epoch 178 Batch 150 Loss -1.3043\n",
      "Epoch 178 Batch 200 Loss -1.3513\n",
      "Epoch 178 Batch 250 Loss -1.3880\n",
      "Epoch 178 Batch 300 Loss -1.4012\n",
      "Epoch 178 Batch 350 Loss -1.4173\n",
      "Epoch 178 Loss -1.4294\n",
      "{'Epoch': 178}\n",
      "Epoch 179 Batch 0 Loss -0.9448\n",
      "Epoch 179 Batch 50 Loss -1.3903\n",
      "Epoch 179 Batch 100 Loss -1.3728\n",
      "Epoch 179 Batch 150 Loss -1.3607\n",
      "Epoch 179 Batch 200 Loss -1.3978\n",
      "Epoch 179 Batch 250 Loss -1.4123\n",
      "Epoch 179 Batch 300 Loss -1.4109\n",
      "Epoch 179 Batch 350 Loss -1.4138\n",
      "Epoch 179 Loss -1.4189\n",
      "{'Epoch': 179}\n",
      "Epoch 180 Batch 0 Loss -0.9309\n",
      "Epoch 180 Batch 50 Loss -1.3361\n",
      "Epoch 180 Batch 100 Loss -1.3145\n",
      "Epoch 180 Batch 150 Loss -1.3269\n",
      "Epoch 180 Batch 200 Loss -1.3789\n",
      "Epoch 180 Batch 250 Loss -1.4123\n",
      "Epoch 180 Batch 300 Loss -1.4160\n",
      "Epoch 180 Batch 350 Loss -1.4173\n",
      "Epoch 180 Loss -1.4241\n",
      "{'Epoch': 180}\n",
      "Epoch 181 Batch 0 Loss -0.9264\n",
      "Epoch 181 Batch 50 Loss -1.3298\n",
      "Epoch 181 Batch 100 Loss -1.3061\n",
      "Epoch 181 Batch 150 Loss -1.3188\n",
      "Epoch 181 Batch 200 Loss -1.3738\n",
      "Epoch 181 Batch 250 Loss -1.4071\n",
      "Epoch 181 Batch 300 Loss -1.4170\n",
      "Epoch 181 Batch 350 Loss -1.4186\n",
      "Epoch 181 Loss -1.4251\n",
      "{'Epoch': 181}\n",
      "Epoch 182 Batch 0 Loss -0.9470\n",
      "Epoch 182 Batch 50 Loss -1.3614\n",
      "Epoch 182 Batch 100 Loss -1.3337\n",
      "Epoch 182 Batch 150 Loss -1.3431\n",
      "Epoch 182 Batch 200 Loss -1.3861\n",
      "Epoch 182 Batch 250 Loss -1.4066\n",
      "Epoch 182 Batch 300 Loss -1.4069\n",
      "Epoch 182 Batch 350 Loss -1.4166\n",
      "Epoch 182 Loss -1.4246\n",
      "{'Epoch': 182}\n",
      "Epoch 183 Batch 0 Loss -0.9665\n",
      "Epoch 183 Batch 50 Loss -1.3683\n",
      "Epoch 183 Batch 100 Loss -1.3395\n",
      "Epoch 183 Batch 150 Loss -1.3491\n",
      "Epoch 183 Batch 200 Loss -1.3949\n",
      "Epoch 183 Batch 250 Loss -1.4236\n",
      "Epoch 183 Batch 300 Loss -1.4230\n",
      "Epoch 183 Batch 350 Loss -1.4257\n",
      "Epoch 183 Loss -1.4289\n",
      "{'Epoch': 183}\n",
      "Epoch 184 Batch 0 Loss -0.8890\n",
      "Epoch 184 Batch 50 Loss -1.2887\n",
      "Epoch 184 Batch 100 Loss -1.2856\n",
      "Epoch 184 Batch 150 Loss -1.3110\n",
      "Epoch 184 Batch 200 Loss -1.3637\n",
      "Epoch 184 Batch 250 Loss -1.3962\n",
      "Epoch 184 Batch 300 Loss -1.4056\n",
      "Epoch 184 Batch 350 Loss -1.4170\n",
      "Epoch 184 Loss -1.4286\n",
      "{'Epoch': 184}\n",
      "Epoch 185 Batch 0 Loss -0.9767\n",
      "Epoch 185 Batch 50 Loss -1.3943\n",
      "Epoch 185 Batch 100 Loss -1.3320\n",
      "Epoch 185 Batch 150 Loss -1.3308\n",
      "Epoch 185 Batch 200 Loss -1.3800\n",
      "Epoch 185 Batch 250 Loss -1.4009\n",
      "Epoch 185 Batch 300 Loss -1.4075\n",
      "Epoch 185 Batch 350 Loss -1.4112\n",
      "Epoch 185 Loss -1.4190\n",
      "{'Epoch': 185}\n",
      "Epoch 186 Batch 0 Loss -0.9112\n",
      "Epoch 186 Batch 50 Loss -1.3610\n",
      "Epoch 186 Batch 100 Loss -1.3517\n",
      "Epoch 186 Batch 150 Loss -1.3462\n",
      "Epoch 186 Batch 200 Loss -1.3936\n",
      "Epoch 186 Batch 250 Loss -1.4178\n",
      "Epoch 186 Batch 300 Loss -1.4127\n",
      "Epoch 186 Batch 350 Loss -1.4253\n",
      "Epoch 186 Loss -1.4363\n",
      "{'Epoch': 186}\n",
      "Epoch 187 Batch 0 Loss -0.9770\n",
      "Epoch 187 Batch 50 Loss -1.3940\n",
      "Epoch 187 Batch 100 Loss -1.3373\n",
      "Epoch 187 Batch 150 Loss -1.3541\n",
      "Epoch 187 Batch 200 Loss -1.4108\n",
      "Epoch 187 Batch 250 Loss -1.4480\n",
      "Epoch 187 Batch 300 Loss -1.4526\n",
      "Epoch 187 Batch 350 Loss -1.4596\n",
      "Epoch 187 Loss -1.4678\n",
      "{'Epoch': 187}\n",
      "Epoch 188 Batch 0 Loss -0.9779\n",
      "Epoch 188 Batch 50 Loss -1.3769\n",
      "Epoch 188 Batch 100 Loss -1.3420\n",
      "Epoch 188 Batch 150 Loss -1.3399\n",
      "Epoch 188 Batch 200 Loss -1.3892\n",
      "Epoch 188 Batch 250 Loss -1.4169\n",
      "Epoch 188 Batch 300 Loss -1.4222\n",
      "Epoch 188 Batch 350 Loss -1.4321\n",
      "Epoch 188 Loss -1.4370\n",
      "{'Epoch': 188}\n",
      "Epoch 189 Batch 0 Loss -0.9464\n",
      "Epoch 189 Batch 50 Loss -1.3481\n",
      "Epoch 189 Batch 100 Loss -1.3365\n",
      "Epoch 189 Batch 150 Loss -1.3344\n",
      "Epoch 189 Batch 200 Loss -1.3657\n",
      "Epoch 189 Batch 250 Loss -1.3941\n",
      "Epoch 189 Batch 300 Loss -1.4008\n",
      "Epoch 189 Batch 350 Loss -1.4061\n",
      "Epoch 189 Loss -1.4136\n",
      "{'Epoch': 189}\n",
      "Epoch 190 Batch 0 Loss -0.8479\n",
      "Epoch 190 Batch 50 Loss -1.2860\n",
      "Epoch 190 Batch 100 Loss -1.3026\n",
      "Epoch 190 Batch 150 Loss -1.3110\n",
      "Epoch 190 Batch 200 Loss -1.3643\n",
      "Epoch 190 Batch 250 Loss -1.3970\n",
      "Epoch 190 Batch 300 Loss -1.4004\n",
      "Epoch 190 Batch 350 Loss -1.4111\n",
      "Epoch 190 Loss -1.4157\n",
      "{'Epoch': 190}\n",
      "Epoch 191 Batch 0 Loss -0.8871\n",
      "Epoch 191 Batch 50 Loss -1.3274\n",
      "Epoch 191 Batch 100 Loss -1.3174\n",
      "Epoch 191 Batch 150 Loss -1.3157\n",
      "Epoch 191 Batch 200 Loss -1.3615\n",
      "Epoch 191 Batch 250 Loss -1.3896\n",
      "Epoch 191 Batch 300 Loss -1.4002\n",
      "Epoch 191 Batch 350 Loss -1.4148\n",
      "Epoch 191 Loss -1.4258\n",
      "{'Epoch': 191}\n",
      "Epoch 192 Batch 0 Loss -0.9612\n",
      "Epoch 192 Batch 50 Loss -1.3461\n",
      "Epoch 192 Batch 100 Loss -1.3148\n",
      "Epoch 192 Batch 150 Loss -1.3307\n",
      "Epoch 192 Batch 200 Loss -1.3840\n",
      "Epoch 192 Batch 250 Loss -1.4078\n",
      "Epoch 192 Batch 300 Loss -1.4119\n",
      "Epoch 192 Batch 350 Loss -1.4212\n",
      "Epoch 192 Loss -1.4296\n",
      "{'Epoch': 192}\n",
      "Epoch 193 Batch 0 Loss -0.9639\n",
      "Epoch 193 Batch 50 Loss -1.3594\n",
      "Epoch 193 Batch 100 Loss -1.3454\n",
      "Epoch 193 Batch 150 Loss -1.3571\n",
      "Epoch 193 Batch 200 Loss -1.4043\n",
      "Epoch 193 Batch 250 Loss -1.4363\n",
      "Epoch 193 Batch 300 Loss -1.4402\n",
      "Epoch 193 Batch 350 Loss -1.4440\n",
      "Epoch 193 Loss -1.4528\n",
      "{'Epoch': 193}\n",
      "Epoch 194 Batch 0 Loss -0.9633\n",
      "Epoch 194 Batch 50 Loss -1.3618\n",
      "Epoch 194 Batch 100 Loss -1.3322\n",
      "Epoch 194 Batch 150 Loss -1.3237\n",
      "Epoch 194 Batch 200 Loss -1.3724\n",
      "Epoch 194 Batch 250 Loss -1.4085\n",
      "Epoch 194 Batch 300 Loss -1.4242\n",
      "Epoch 194 Batch 350 Loss -1.4391\n",
      "Epoch 194 Loss -1.4499\n",
      "{'Epoch': 194}\n",
      "Epoch 195 Batch 0 Loss -1.0063\n",
      "Epoch 195 Batch 50 Loss -1.4052\n",
      "Epoch 195 Batch 100 Loss -1.3789\n",
      "Epoch 195 Batch 150 Loss -1.3718\n",
      "Epoch 195 Batch 200 Loss -1.4159\n",
      "Epoch 195 Batch 250 Loss -1.4432\n",
      "Epoch 195 Batch 300 Loss -1.4511\n",
      "Epoch 195 Batch 350 Loss -1.4598\n",
      "Epoch 195 Loss -1.4687\n",
      "{'Epoch': 195}\n",
      "Epoch 196 Batch 0 Loss -1.0161\n",
      "Epoch 196 Batch 50 Loss -1.3834\n",
      "Epoch 196 Batch 100 Loss -1.3418\n",
      "Epoch 196 Batch 150 Loss -1.3476\n",
      "Epoch 196 Batch 200 Loss -1.4029\n",
      "Epoch 196 Batch 250 Loss -1.4233\n",
      "Epoch 196 Batch 300 Loss -1.4310\n",
      "Epoch 196 Batch 350 Loss -1.4382\n",
      "Epoch 196 Loss -1.4447\n",
      "{'Epoch': 196}\n",
      "Epoch 197 Batch 0 Loss -0.9686\n",
      "Epoch 197 Batch 50 Loss -1.3721\n",
      "Epoch 197 Batch 100 Loss -1.3188\n",
      "Epoch 197 Batch 150 Loss -1.3378\n",
      "Epoch 197 Batch 200 Loss -1.3930\n",
      "Epoch 197 Batch 250 Loss -1.4283\n",
      "Epoch 197 Batch 300 Loss -1.4363\n",
      "Epoch 197 Batch 350 Loss -1.4484\n",
      "Epoch 197 Loss -1.4566\n",
      "{'Epoch': 197}\n",
      "Epoch 198 Batch 0 Loss -1.0060\n",
      "Epoch 198 Batch 50 Loss -1.3859\n",
      "Epoch 198 Batch 100 Loss -1.3603\n",
      "Epoch 198 Batch 150 Loss -1.3650\n",
      "Epoch 198 Batch 200 Loss -1.4012\n",
      "Epoch 198 Batch 250 Loss -1.4258\n",
      "Epoch 198 Batch 300 Loss -1.4348\n",
      "Epoch 198 Batch 350 Loss -1.4426\n",
      "Epoch 198 Loss -1.4491\n",
      "{'Epoch': 198}\n",
      "Epoch 199 Batch 0 Loss -0.9060\n",
      "Epoch 199 Batch 50 Loss -1.3607\n",
      "Epoch 199 Batch 100 Loss -1.3434\n",
      "Epoch 199 Batch 150 Loss -1.3455\n",
      "Epoch 199 Batch 200 Loss -1.4017\n",
      "Epoch 199 Batch 250 Loss -1.4373\n",
      "Epoch 199 Batch 300 Loss -1.4408\n",
      "Epoch 199 Batch 350 Loss -1.4506\n",
      "Epoch 199 Loss -1.4594\n",
      "{'Epoch': 199}\n",
      "Epoch 200 Batch 0 Loss -0.9810\n",
      "Epoch 200 Batch 50 Loss -1.4021\n",
      "Epoch 200 Batch 100 Loss -1.3541\n",
      "Epoch 200 Batch 150 Loss -1.3696\n",
      "Epoch 200 Batch 200 Loss -1.4241\n",
      "Epoch 200 Batch 250 Loss -1.4574\n",
      "Epoch 200 Batch 300 Loss -1.4644\n",
      "Epoch 200 Batch 350 Loss -1.4737\n",
      "Epoch 200 Loss -1.4827\n",
      "{'Epoch': 200}\n",
      "Epoch 201 Batch 0 Loss -1.0080\n",
      "Epoch 201 Batch 50 Loss -1.4130\n",
      "Epoch 201 Batch 100 Loss -1.3868\n",
      "Epoch 201 Batch 150 Loss -1.3940\n",
      "Epoch 201 Batch 200 Loss -1.4320\n",
      "Epoch 201 Batch 250 Loss -1.4491\n",
      "Epoch 201 Batch 300 Loss -1.4500\n",
      "Epoch 201 Batch 350 Loss -1.4582\n",
      "Epoch 201 Loss -1.4661\n",
      "{'Epoch': 201}\n",
      "Epoch 202 Batch 0 Loss -0.9718\n",
      "Epoch 202 Batch 50 Loss -1.3921\n",
      "Epoch 202 Batch 100 Loss -1.3691\n",
      "Epoch 202 Batch 150 Loss -1.3753\n",
      "Epoch 202 Batch 200 Loss -1.4130\n",
      "Epoch 202 Batch 250 Loss -1.4406\n",
      "Epoch 202 Batch 300 Loss -1.4435\n",
      "Epoch 202 Batch 350 Loss -1.4527\n",
      "Epoch 202 Loss -1.4602\n",
      "{'Epoch': 202}\n",
      "Epoch 203 Batch 0 Loss -0.9879\n",
      "Epoch 203 Batch 50 Loss -1.3735\n",
      "Epoch 203 Batch 100 Loss -1.3714\n",
      "Epoch 203 Batch 150 Loss -1.3731\n",
      "Epoch 203 Batch 200 Loss -1.4159\n",
      "Epoch 203 Batch 250 Loss -1.4409\n",
      "Epoch 203 Batch 300 Loss -1.4467\n",
      "Epoch 203 Batch 350 Loss -1.4524\n",
      "Epoch 203 Loss -1.4595\n",
      "{'Epoch': 203}\n",
      "Epoch 204 Batch 0 Loss -0.9801\n",
      "Epoch 204 Batch 50 Loss -1.3925\n",
      "Epoch 204 Batch 100 Loss -1.3697\n",
      "Epoch 204 Batch 150 Loss -1.3802\n",
      "Epoch 204 Batch 200 Loss -1.4335\n",
      "Epoch 204 Batch 250 Loss -1.4601\n",
      "Epoch 204 Batch 300 Loss -1.4596\n",
      "Epoch 204 Batch 350 Loss -1.4653\n",
      "Epoch 204 Loss -1.4693\n",
      "{'Epoch': 204}\n",
      "Epoch 205 Batch 0 Loss -1.0156\n",
      "Epoch 205 Batch 50 Loss -1.3894\n",
      "Epoch 205 Batch 100 Loss -1.3738\n",
      "Epoch 205 Batch 150 Loss -1.3839\n",
      "Epoch 205 Batch 200 Loss -1.4300\n",
      "Epoch 205 Batch 250 Loss -1.4616\n",
      "Epoch 205 Batch 300 Loss -1.4694\n",
      "Epoch 205 Batch 350 Loss -1.4723\n",
      "Epoch 205 Loss -1.4804\n",
      "{'Epoch': 205}\n",
      "Epoch 206 Batch 0 Loss -0.8926\n",
      "Epoch 206 Batch 50 Loss -1.3910\n",
      "Epoch 206 Batch 100 Loss -1.3551\n",
      "Epoch 206 Batch 150 Loss -1.3654\n",
      "Epoch 206 Batch 200 Loss -1.4216\n",
      "Epoch 206 Batch 250 Loss -1.4401\n",
      "Epoch 206 Batch 300 Loss -1.4495\n",
      "Epoch 206 Batch 350 Loss -1.4617\n",
      "Epoch 206 Loss -1.4734\n",
      "{'Epoch': 206}\n",
      "Epoch 207 Batch 0 Loss -1.0157\n",
      "Epoch 207 Batch 50 Loss -1.4293\n",
      "Epoch 207 Batch 100 Loss -1.4160\n",
      "Epoch 207 Batch 150 Loss -1.4281\n",
      "Epoch 207 Batch 200 Loss -1.4755\n",
      "Epoch 207 Batch 250 Loss -1.5070\n",
      "Epoch 207 Batch 300 Loss -1.5026\n",
      "Epoch 207 Batch 350 Loss -1.5027\n",
      "Epoch 207 Loss -1.5081\n",
      "{'Epoch': 207}\n",
      "Epoch 208 Batch 0 Loss -0.9968\n",
      "Epoch 208 Batch 50 Loss -1.4039\n",
      "Epoch 208 Batch 100 Loss -1.3754\n",
      "Epoch 208 Batch 150 Loss -1.3759\n",
      "Epoch 208 Batch 200 Loss -1.4187\n",
      "Epoch 208 Batch 250 Loss -1.4513\n",
      "Epoch 208 Batch 300 Loss -1.4591\n",
      "Epoch 208 Batch 350 Loss -1.4681\n",
      "Epoch 208 Loss -1.4761\n",
      "{'Epoch': 208}\n",
      "Epoch 209 Batch 0 Loss -0.9642\n",
      "Epoch 209 Batch 50 Loss -1.4033\n",
      "Epoch 209 Batch 100 Loss -1.3839\n",
      "Epoch 209 Batch 150 Loss -1.3926\n",
      "Epoch 209 Batch 200 Loss -1.4367\n",
      "Epoch 209 Batch 250 Loss -1.4586\n",
      "Epoch 209 Batch 300 Loss -1.4595\n",
      "Epoch 209 Batch 350 Loss -1.4655\n",
      "Epoch 209 Loss -1.4735\n",
      "{'Epoch': 209}\n",
      "Epoch 210 Batch 0 Loss -0.9749\n",
      "Epoch 210 Batch 50 Loss -1.4010\n",
      "Epoch 210 Batch 100 Loss -1.3792\n",
      "Epoch 210 Batch 150 Loss -1.3745\n",
      "Epoch 210 Batch 200 Loss -1.4186\n",
      "Epoch 210 Batch 250 Loss -1.4485\n",
      "Epoch 210 Batch 300 Loss -1.4555\n",
      "Epoch 210 Batch 350 Loss -1.4657\n",
      "Epoch 210 Loss -1.4742\n",
      "{'Epoch': 210}\n",
      "Epoch 211 Batch 0 Loss -0.9787\n",
      "Epoch 211 Batch 50 Loss -1.3870\n",
      "Epoch 211 Batch 100 Loss -1.3760\n",
      "Epoch 211 Batch 150 Loss -1.3821\n",
      "Epoch 211 Batch 200 Loss -1.4268\n",
      "Epoch 211 Batch 250 Loss -1.4514\n",
      "Epoch 211 Batch 300 Loss -1.4580\n",
      "Epoch 211 Batch 350 Loss -1.4709\n",
      "Epoch 211 Loss -1.4810\n",
      "{'Epoch': 211}\n",
      "Epoch 212 Batch 0 Loss -1.0254\n",
      "Epoch 212 Batch 50 Loss -1.4258\n",
      "Epoch 212 Batch 100 Loss -1.3928\n",
      "Epoch 212 Batch 150 Loss -1.3908\n",
      "Epoch 212 Batch 200 Loss -1.4278\n",
      "Epoch 212 Batch 250 Loss -1.4577\n",
      "Epoch 212 Batch 300 Loss -1.4579\n",
      "Epoch 212 Batch 350 Loss -1.4683\n",
      "Epoch 212 Loss -1.4778\n",
      "{'Epoch': 212}\n",
      "Epoch 213 Batch 0 Loss -0.9941\n",
      "Epoch 213 Batch 50 Loss -1.3890\n",
      "Epoch 213 Batch 100 Loss -1.3768\n",
      "Epoch 213 Batch 150 Loss -1.3826\n",
      "Epoch 213 Batch 200 Loss -1.4312\n",
      "Epoch 213 Batch 250 Loss -1.4657\n",
      "Epoch 213 Batch 300 Loss -1.4769\n",
      "Epoch 213 Batch 350 Loss -1.4813\n",
      "Epoch 213 Loss -1.4892\n",
      "{'Epoch': 213}\n",
      "Epoch 214 Batch 0 Loss -0.9373\n",
      "Epoch 214 Batch 50 Loss -1.4011\n",
      "Epoch 214 Batch 100 Loss -1.3797\n",
      "Epoch 214 Batch 150 Loss -1.3959\n",
      "Epoch 214 Batch 200 Loss -1.4220\n",
      "Epoch 214 Batch 250 Loss -1.4546\n",
      "Epoch 214 Batch 300 Loss -1.4622\n",
      "Epoch 214 Batch 350 Loss -1.4697\n",
      "Epoch 214 Loss -1.4786\n",
      "{'Epoch': 214}\n",
      "Epoch 215 Batch 0 Loss -0.9907\n",
      "Epoch 215 Batch 50 Loss -1.3835\n",
      "Epoch 215 Batch 100 Loss -1.3642\n",
      "Epoch 215 Batch 150 Loss -1.3838\n",
      "Epoch 215 Batch 200 Loss -1.4314\n",
      "Epoch 215 Batch 250 Loss -1.4639\n",
      "Epoch 215 Batch 300 Loss -1.4747\n",
      "Epoch 215 Batch 350 Loss -1.4810\n",
      "Epoch 215 Loss -1.4906\n",
      "{'Epoch': 215}\n",
      "Epoch 216 Batch 0 Loss -1.0405\n",
      "Epoch 216 Batch 50 Loss -1.4522\n",
      "Epoch 216 Batch 100 Loss -1.4336\n",
      "Epoch 216 Batch 150 Loss -1.4451\n",
      "Epoch 216 Batch 200 Loss -1.4927\n",
      "Epoch 216 Batch 250 Loss -1.5210\n",
      "Epoch 216 Batch 300 Loss -1.5288\n",
      "Epoch 216 Batch 350 Loss -1.5366\n",
      "Epoch 216 Loss -1.5371\n",
      "{'Epoch': 216}\n",
      "Epoch 217 Batch 0 Loss -0.9786\n",
      "Epoch 217 Batch 50 Loss -1.4094\n",
      "Epoch 217 Batch 100 Loss -1.3984\n",
      "Epoch 217 Batch 150 Loss -1.4138\n",
      "Epoch 217 Batch 200 Loss -1.4455\n",
      "Epoch 217 Batch 250 Loss -1.4731\n",
      "Epoch 217 Batch 300 Loss -1.4852\n",
      "Epoch 217 Batch 350 Loss -1.4954\n",
      "Epoch 217 Loss -1.5052\n",
      "{'Epoch': 217}\n",
      "Epoch 218 Batch 0 Loss -1.0576\n",
      "Epoch 218 Batch 50 Loss -1.4371\n",
      "Epoch 218 Batch 100 Loss -1.4147\n",
      "Epoch 218 Batch 150 Loss -1.3945\n",
      "Epoch 218 Batch 200 Loss -1.4408\n",
      "Epoch 218 Batch 250 Loss -1.4707\n",
      "Epoch 218 Batch 300 Loss -1.4753\n",
      "Epoch 218 Batch 350 Loss -1.4877\n",
      "Epoch 218 Loss -1.4978\n",
      "{'Epoch': 218}\n",
      "Epoch 219 Batch 0 Loss -1.0451\n",
      "Epoch 219 Batch 50 Loss -1.4442\n",
      "Epoch 219 Batch 100 Loss -1.4198\n",
      "Epoch 219 Batch 150 Loss -1.4271\n",
      "Epoch 219 Batch 200 Loss -1.4752\n",
      "Epoch 219 Batch 250 Loss -1.4996\n",
      "Epoch 219 Batch 300 Loss -1.4903\n",
      "Epoch 219 Batch 350 Loss -1.4969\n",
      "Epoch 219 Loss -1.5022\n",
      "{'Epoch': 219}\n",
      "Epoch 220 Batch 0 Loss -0.9112\n",
      "Epoch 220 Batch 50 Loss -1.3699\n",
      "Epoch 220 Batch 100 Loss -1.3763\n",
      "Epoch 220 Batch 150 Loss -1.3930\n",
      "Epoch 220 Batch 200 Loss -1.4430\n",
      "Epoch 220 Batch 250 Loss -1.4769\n",
      "Epoch 220 Batch 300 Loss -1.4901\n",
      "Epoch 220 Batch 350 Loss -1.5017\n",
      "Epoch 220 Loss -1.5114\n",
      "{'Epoch': 220}\n",
      "Epoch 221 Batch 0 Loss -1.0565\n",
      "Epoch 221 Batch 50 Loss -1.4407\n",
      "Epoch 221 Batch 100 Loss -1.4074\n",
      "Epoch 221 Batch 150 Loss -1.4187\n",
      "Epoch 221 Batch 200 Loss -1.4664\n",
      "Epoch 221 Batch 250 Loss -1.4991\n",
      "Epoch 221 Batch 300 Loss -1.5072\n",
      "Epoch 221 Batch 350 Loss -1.5145\n",
      "Epoch 221 Loss -1.5228\n",
      "{'Epoch': 221}\n",
      "Epoch 222 Batch 0 Loss -1.0527\n",
      "Epoch 222 Batch 50 Loss -1.4402\n",
      "Epoch 222 Batch 100 Loss -1.3826\n",
      "Epoch 222 Batch 150 Loss -1.3846\n",
      "Epoch 222 Batch 200 Loss -1.4337\n",
      "Epoch 222 Batch 250 Loss -1.4699\n",
      "Epoch 222 Batch 300 Loss -1.4806\n",
      "Epoch 222 Batch 350 Loss -1.4888\n",
      "Epoch 222 Loss -1.4973\n",
      "{'Epoch': 222}\n",
      "Epoch 223 Batch 0 Loss -1.0405\n",
      "Epoch 223 Batch 50 Loss -1.4109\n",
      "Epoch 223 Batch 100 Loss -1.3823\n",
      "Epoch 223 Batch 150 Loss -1.4001\n",
      "Epoch 223 Batch 200 Loss -1.4486\n",
      "Epoch 223 Batch 250 Loss -1.4802\n",
      "Epoch 223 Batch 300 Loss -1.4830\n",
      "Epoch 223 Batch 350 Loss -1.4896\n",
      "Epoch 223 Loss -1.4994\n",
      "{'Epoch': 223}\n",
      "Epoch 224 Batch 0 Loss -1.0332\n",
      "Epoch 224 Batch 50 Loss -1.4240\n",
      "Epoch 224 Batch 100 Loss -1.4061\n",
      "Epoch 224 Batch 150 Loss -1.4099\n",
      "Epoch 224 Batch 200 Loss -1.4579\n",
      "Epoch 224 Batch 250 Loss -1.4884\n",
      "Epoch 224 Batch 300 Loss -1.4974\n",
      "Epoch 224 Batch 350 Loss -1.5069\n",
      "Epoch 224 Loss -1.5165\n",
      "{'Epoch': 224}\n",
      "Epoch 225 Batch 0 Loss -1.0676\n",
      "Epoch 225 Batch 50 Loss -1.4526\n",
      "Epoch 225 Batch 100 Loss -1.4287\n",
      "Epoch 225 Batch 150 Loss -1.4354\n",
      "Epoch 225 Batch 200 Loss -1.4841\n",
      "Epoch 225 Batch 250 Loss -1.5153\n",
      "Epoch 225 Batch 300 Loss -1.5228\n",
      "Epoch 225 Batch 350 Loss -1.5299\n",
      "Epoch 225 Loss -1.5379\n",
      "{'Epoch': 225}\n",
      "Epoch 226 Batch 0 Loss -1.0860\n",
      "Epoch 226 Batch 50 Loss -1.4522\n",
      "Epoch 226 Batch 100 Loss -1.4189\n",
      "Epoch 226 Batch 150 Loss -1.4114\n",
      "Epoch 226 Batch 200 Loss -1.4649\n",
      "Epoch 226 Batch 250 Loss -1.4918\n",
      "Epoch 226 Batch 300 Loss -1.5033\n",
      "Epoch 226 Batch 350 Loss -1.5177\n",
      "Epoch 226 Loss -1.5292\n",
      "{'Epoch': 226}\n",
      "Epoch 227 Batch 0 Loss -1.1002\n",
      "Epoch 227 Batch 50 Loss -1.4872\n",
      "Epoch 227 Batch 100 Loss -1.4246\n",
      "Epoch 227 Batch 150 Loss -1.4255\n",
      "Epoch 227 Batch 200 Loss -1.4649\n",
      "Epoch 227 Batch 250 Loss -1.4873\n",
      "Epoch 227 Batch 300 Loss -1.4928\n",
      "Epoch 227 Batch 350 Loss -1.4965\n",
      "Epoch 227 Loss -1.5030\n",
      "{'Epoch': 227}\n",
      "Epoch 228 Batch 0 Loss -1.0739\n",
      "Epoch 228 Batch 50 Loss -1.4045\n",
      "Epoch 228 Batch 100 Loss -1.3691\n",
      "Epoch 228 Batch 150 Loss -1.3880\n",
      "Epoch 228 Batch 200 Loss -1.4448\n",
      "Epoch 228 Batch 250 Loss -1.4810\n",
      "Epoch 228 Batch 300 Loss -1.4909\n",
      "Epoch 228 Batch 350 Loss -1.5010\n",
      "Epoch 228 Loss -1.5106\n",
      "{'Epoch': 228}\n",
      "Epoch 229 Batch 0 Loss -1.0672\n",
      "Epoch 229 Batch 50 Loss -1.4305\n",
      "Epoch 229 Batch 100 Loss -1.4026\n",
      "Epoch 229 Batch 150 Loss -1.4187\n",
      "Epoch 229 Batch 200 Loss -1.4739\n",
      "Epoch 229 Batch 250 Loss -1.5015\n",
      "Epoch 229 Batch 300 Loss -1.5013\n",
      "Epoch 229 Batch 350 Loss -1.5083\n",
      "Epoch 229 Loss -1.5169\n",
      "{'Epoch': 229}\n",
      "Epoch 230 Batch 0 Loss -1.0615\n",
      "Epoch 230 Batch 50 Loss -1.4273\n",
      "Epoch 230 Batch 100 Loss -1.4181\n",
      "Epoch 230 Batch 150 Loss -1.4178\n",
      "Epoch 230 Batch 200 Loss -1.4646\n",
      "Epoch 230 Batch 250 Loss -1.5046\n",
      "Epoch 230 Batch 300 Loss -1.5149\n",
      "Epoch 230 Batch 350 Loss -1.5155\n",
      "Epoch 230 Loss -1.5202\n",
      "{'Epoch': 230}\n",
      "Epoch 231 Batch 0 Loss -1.0779\n",
      "Epoch 231 Batch 50 Loss -1.4310\n",
      "Epoch 231 Batch 100 Loss -1.4056\n",
      "Epoch 231 Batch 150 Loss -1.4055\n",
      "Epoch 231 Batch 200 Loss -1.4511\n",
      "Epoch 231 Batch 250 Loss -1.4870\n",
      "Epoch 231 Batch 300 Loss -1.5036\n",
      "Epoch 231 Batch 350 Loss -1.5181\n",
      "Epoch 231 Loss -1.5292\n",
      "{'Epoch': 231}\n",
      "Epoch 232 Batch 0 Loss -1.0862\n",
      "Epoch 232 Batch 50 Loss -1.4785\n",
      "Epoch 232 Batch 100 Loss -1.4616\n",
      "Epoch 232 Batch 150 Loss -1.4717\n",
      "Epoch 232 Batch 200 Loss -1.5055\n",
      "Epoch 232 Batch 250 Loss -1.5237\n",
      "Epoch 232 Batch 300 Loss -1.5199\n",
      "Epoch 232 Batch 350 Loss -1.5225\n",
      "Epoch 232 Loss -1.5292\n",
      "{'Epoch': 232}\n",
      "Epoch 233 Batch 0 Loss -1.0955\n",
      "Epoch 233 Batch 50 Loss -1.4647\n",
      "Epoch 233 Batch 100 Loss -1.4452\n",
      "Epoch 233 Batch 150 Loss -1.4544\n",
      "Epoch 233 Batch 200 Loss -1.5021\n",
      "Epoch 233 Batch 250 Loss -1.5306\n",
      "Epoch 233 Batch 300 Loss -1.5290\n",
      "Epoch 233 Batch 350 Loss -1.5317\n",
      "Epoch 233 Loss -1.5358\n",
      "{'Epoch': 233}\n",
      "Epoch 234 Batch 0 Loss -0.9774\n",
      "Epoch 234 Batch 50 Loss -1.4413\n",
      "Epoch 234 Batch 100 Loss -1.4171\n",
      "Epoch 234 Batch 150 Loss -1.4096\n",
      "Epoch 234 Batch 200 Loss -1.4584\n",
      "Epoch 234 Batch 250 Loss -1.4905\n",
      "Epoch 234 Batch 300 Loss -1.5058\n",
      "Epoch 234 Batch 350 Loss -1.5201\n",
      "Epoch 234 Loss -1.5313\n",
      "{'Epoch': 234}\n",
      "Epoch 235 Batch 0 Loss -1.0880\n",
      "Epoch 235 Batch 50 Loss -1.4834\n",
      "Epoch 235 Batch 100 Loss -1.4658\n",
      "Epoch 235 Batch 150 Loss -1.4768\n",
      "Epoch 235 Batch 200 Loss -1.5049\n",
      "Epoch 235 Batch 250 Loss -1.5327\n",
      "Epoch 235 Batch 300 Loss -1.5412\n",
      "Epoch 235 Batch 350 Loss -1.5455\n",
      "Epoch 235 Loss -1.5487\n",
      "{'Epoch': 235}\n",
      "Epoch 236 Batch 0 Loss -1.0928\n",
      "Epoch 236 Batch 50 Loss -1.4225\n",
      "Epoch 236 Batch 100 Loss -1.4218\n",
      "Epoch 236 Batch 150 Loss -1.4370\n",
      "Epoch 236 Batch 200 Loss -1.4896\n",
      "Epoch 236 Batch 250 Loss -1.5234\n",
      "Epoch 236 Batch 300 Loss -1.5233\n",
      "Epoch 236 Batch 350 Loss -1.5208\n",
      "Epoch 236 Loss -1.5248\n",
      "{'Epoch': 236}\n",
      "Epoch 237 Batch 0 Loss -1.0602\n",
      "Epoch 237 Batch 50 Loss -1.4654\n",
      "Epoch 237 Batch 100 Loss -1.4433\n",
      "Epoch 237 Batch 150 Loss -1.4514\n",
      "Epoch 237 Batch 200 Loss -1.5014\n",
      "Epoch 237 Batch 250 Loss -1.5307\n",
      "Epoch 237 Batch 300 Loss -1.5288\n",
      "Epoch 237 Batch 350 Loss -1.5344\n",
      "Epoch 237 Loss -1.5390\n",
      "{'Epoch': 237}\n",
      "Epoch 238 Batch 0 Loss -0.9833\n",
      "Epoch 238 Batch 50 Loss -1.4787\n",
      "Epoch 238 Batch 100 Loss -1.4643\n",
      "Epoch 238 Batch 150 Loss -1.4775\n",
      "Epoch 238 Batch 200 Loss -1.5265\n",
      "Epoch 238 Batch 250 Loss -1.5593\n",
      "Epoch 238 Batch 300 Loss -1.5599\n",
      "Epoch 238 Batch 350 Loss -1.5593\n",
      "Epoch 238 Loss -1.5642\n",
      "{'Epoch': 238}\n",
      "Epoch 239 Batch 0 Loss -1.0052\n",
      "Epoch 239 Batch 50 Loss -1.4136\n",
      "Epoch 239 Batch 100 Loss -1.4183\n",
      "Epoch 239 Batch 150 Loss -1.4395\n",
      "Epoch 239 Batch 200 Loss -1.4920\n",
      "Epoch 239 Batch 250 Loss -1.5275\n",
      "Epoch 239 Batch 300 Loss -1.5388\n",
      "Epoch 239 Batch 350 Loss -1.5484\n",
      "Epoch 239 Loss -1.5570\n",
      "{'Epoch': 239}\n",
      "Epoch 240 Batch 0 Loss -1.0976\n",
      "Epoch 240 Batch 50 Loss -1.4765\n",
      "Epoch 240 Batch 100 Loss -1.4294\n",
      "Epoch 240 Batch 150 Loss -1.4293\n",
      "Epoch 240 Batch 200 Loss -1.4795\n",
      "Epoch 240 Batch 250 Loss -1.5106\n",
      "Epoch 240 Batch 300 Loss -1.5260\n",
      "Epoch 240 Batch 350 Loss -1.5399\n",
      "Epoch 240 Loss -1.5479\n",
      "{'Epoch': 240}\n",
      "Epoch 241 Batch 0 Loss -1.0027\n",
      "Epoch 241 Batch 50 Loss -1.3881\n",
      "Epoch 241 Batch 100 Loss -1.3769\n",
      "Epoch 241 Batch 150 Loss -1.3850\n",
      "Epoch 241 Batch 200 Loss -1.4326\n",
      "Epoch 241 Batch 250 Loss -1.4674\n",
      "Epoch 241 Batch 300 Loss -1.4789\n",
      "Epoch 241 Batch 350 Loss -1.4966\n",
      "Epoch 241 Loss -1.5094\n",
      "{'Epoch': 241}\n",
      "Epoch 242 Batch 0 Loss -1.0991\n",
      "Epoch 242 Batch 50 Loss -1.4765\n",
      "Epoch 242 Batch 100 Loss -1.4350\n",
      "Epoch 242 Batch 150 Loss -1.4432\n",
      "Epoch 242 Batch 200 Loss -1.4880\n",
      "Epoch 242 Batch 250 Loss -1.5186\n",
      "Epoch 242 Batch 300 Loss -1.5289\n",
      "Epoch 242 Batch 350 Loss -1.5379\n",
      "Epoch 242 Loss -1.5461\n",
      "{'Epoch': 242}\n",
      "Epoch 243 Batch 0 Loss -1.0495\n",
      "Epoch 243 Batch 50 Loss -1.4593\n",
      "Epoch 243 Batch 100 Loss -1.4414\n",
      "Epoch 243 Batch 150 Loss -1.4509\n",
      "Epoch 243 Batch 200 Loss -1.4965\n",
      "Epoch 243 Batch 250 Loss -1.5296\n",
      "Epoch 243 Batch 300 Loss -1.5346\n",
      "Epoch 243 Batch 350 Loss -1.5414\n",
      "Epoch 243 Loss -1.5494\n",
      "{'Epoch': 243}\n",
      "Epoch 244 Batch 0 Loss -1.0380\n",
      "Epoch 244 Batch 50 Loss -1.4602\n",
      "Epoch 244 Batch 100 Loss -1.4414\n",
      "Epoch 244 Batch 150 Loss -1.4507\n",
      "Epoch 244 Batch 200 Loss -1.4987\n",
      "Epoch 244 Batch 250 Loss -1.5300\n",
      "Epoch 244 Batch 300 Loss -1.5405\n",
      "Epoch 244 Batch 350 Loss -1.5491\n",
      "Epoch 244 Loss -1.5582\n",
      "{'Epoch': 244}\n",
      "Epoch 245 Batch 0 Loss -1.0971\n",
      "Epoch 245 Batch 50 Loss -1.4853\n",
      "Epoch 245 Batch 100 Loss -1.4612\n",
      "Epoch 245 Batch 150 Loss -1.4468\n",
      "Epoch 245 Batch 200 Loss -1.4953\n",
      "Epoch 245 Batch 250 Loss -1.5219\n",
      "Epoch 245 Batch 300 Loss -1.5287\n",
      "Epoch 245 Batch 350 Loss -1.5392\n",
      "Epoch 245 Loss -1.5482\n",
      "{'Epoch': 245}\n",
      "Epoch 246 Batch 0 Loss -1.0859\n",
      "Epoch 246 Batch 50 Loss -1.4808\n",
      "Epoch 246 Batch 100 Loss -1.4548\n",
      "Epoch 246 Batch 150 Loss -1.4584\n",
      "Epoch 246 Batch 200 Loss -1.5009\n",
      "Epoch 246 Batch 250 Loss -1.5212\n",
      "Epoch 246 Batch 300 Loss -1.5328\n",
      "Epoch 246 Batch 350 Loss -1.5411\n",
      "Epoch 246 Loss -1.5499\n",
      "{'Epoch': 246}\n",
      "Epoch 247 Batch 0 Loss -1.0982\n",
      "Epoch 247 Batch 50 Loss -1.4721\n",
      "Epoch 247 Batch 100 Loss -1.4196\n",
      "Epoch 247 Batch 150 Loss -1.4224\n",
      "Epoch 247 Batch 200 Loss -1.4819\n",
      "Epoch 247 Batch 250 Loss -1.5174\n",
      "Epoch 247 Batch 300 Loss -1.5271\n",
      "Epoch 247 Batch 350 Loss -1.5377\n",
      "Epoch 247 Loss -1.5455\n",
      "{'Epoch': 247}\n",
      "Epoch 248 Batch 0 Loss -1.0542\n",
      "Epoch 248 Batch 50 Loss -1.4446\n",
      "Epoch 248 Batch 100 Loss -1.4426\n",
      "Epoch 248 Batch 150 Loss -1.4490\n",
      "Epoch 248 Batch 200 Loss -1.5049\n",
      "Epoch 248 Batch 250 Loss -1.5236\n",
      "Epoch 248 Batch 300 Loss -1.5343\n",
      "Epoch 248 Batch 350 Loss -1.5411\n",
      "Epoch 248 Loss -1.5485\n",
      "{'Epoch': 248}\n",
      "Epoch 249 Batch 0 Loss -1.0847\n",
      "Epoch 249 Batch 50 Loss -1.4500\n",
      "Epoch 249 Batch 100 Loss -1.4463\n",
      "Epoch 249 Batch 150 Loss -1.4655\n",
      "Epoch 249 Batch 200 Loss -1.5229\n",
      "Epoch 249 Batch 250 Loss -1.5596\n",
      "Epoch 249 Batch 300 Loss -1.5636\n",
      "Epoch 249 Batch 350 Loss -1.5646\n",
      "Epoch 249 Loss -1.5644\n",
      "{'Epoch': 249}\n",
      "Epoch 250 Batch 0 Loss -1.0801\n",
      "Epoch 250 Batch 50 Loss -1.4575\n",
      "Epoch 250 Batch 100 Loss -1.4294\n",
      "Epoch 250 Batch 150 Loss -1.4458\n",
      "Epoch 250 Batch 200 Loss -1.4970\n",
      "Epoch 250 Batch 250 Loss -1.5311\n",
      "Epoch 250 Batch 300 Loss -1.5434\n",
      "Epoch 250 Batch 350 Loss -1.5554\n",
      "Epoch 250 Loss -1.5649\n",
      "{'Epoch': 250}\n",
      "Epoch 251 Batch 0 Loss -1.1065\n",
      "Epoch 251 Batch 50 Loss -1.4873\n",
      "Epoch 251 Batch 100 Loss -1.4536\n",
      "Epoch 251 Batch 150 Loss -1.4578\n",
      "Epoch 251 Batch 200 Loss -1.5087\n",
      "Epoch 251 Batch 250 Loss -1.5441\n",
      "Epoch 251 Batch 300 Loss -1.5601\n",
      "Epoch 251 Batch 350 Loss -1.5730\n",
      "Epoch 251 Loss -1.5829\n",
      "{'Epoch': 251}\n",
      "Epoch 252 Batch 0 Loss -1.1264\n",
      "Epoch 252 Batch 50 Loss -1.5153\n",
      "Epoch 252 Batch 100 Loss -1.4974\n",
      "Epoch 252 Batch 150 Loss -1.4608\n",
      "Epoch 252 Batch 200 Loss -1.4895\n",
      "Epoch 252 Batch 250 Loss -1.5183\n",
      "Epoch 252 Batch 300 Loss -1.5207\n",
      "Epoch 252 Batch 350 Loss -1.5315\n",
      "Epoch 252 Loss -1.5406\n",
      "{'Epoch': 252}\n",
      "Epoch 253 Batch 0 Loss -1.0325\n",
      "Epoch 253 Batch 50 Loss -1.4251\n",
      "Epoch 253 Batch 100 Loss -1.4152\n",
      "Epoch 253 Batch 150 Loss -1.4252\n",
      "Epoch 253 Batch 200 Loss -1.4765\n",
      "Epoch 253 Batch 250 Loss -1.5157\n",
      "Epoch 253 Batch 300 Loss -1.5286\n",
      "Epoch 253 Batch 350 Loss -1.5404\n",
      "Epoch 253 Loss -1.5498\n",
      "{'Epoch': 253}\n",
      "Epoch 254 Batch 0 Loss -1.1197\n",
      "Epoch 254 Batch 50 Loss -1.4832\n",
      "Epoch 254 Batch 100 Loss -1.4607\n",
      "Epoch 254 Batch 150 Loss -1.4690\n",
      "Epoch 254 Batch 200 Loss -1.5179\n",
      "Epoch 254 Batch 250 Loss -1.5536\n",
      "Epoch 254 Batch 300 Loss -1.5580\n",
      "Epoch 254 Batch 350 Loss -1.5658\n",
      "Epoch 254 Loss -1.5741\n",
      "{'Epoch': 254}\n",
      "Epoch 255 Batch 0 Loss -1.1362\n",
      "Epoch 255 Batch 50 Loss -1.4955\n",
      "Epoch 255 Batch 100 Loss -1.4799\n",
      "Epoch 255 Batch 150 Loss -1.4917\n",
      "Epoch 255 Batch 200 Loss -1.5405\n",
      "Epoch 255 Batch 250 Loss -1.5725\n",
      "Epoch 255 Batch 300 Loss -1.5814\n",
      "Epoch 255 Batch 350 Loss -1.5879\n",
      "Epoch 255 Loss -1.5927\n",
      "{'Epoch': 255}\n",
      "Epoch 256 Batch 0 Loss -1.0635\n",
      "Epoch 256 Batch 50 Loss -1.4764\n",
      "Epoch 256 Batch 100 Loss -1.4594\n",
      "Epoch 256 Batch 150 Loss -1.4692\n",
      "Epoch 256 Batch 200 Loss -1.5176\n",
      "Epoch 256 Batch 250 Loss -1.5514\n",
      "Epoch 256 Batch 300 Loss -1.5600\n",
      "Epoch 256 Batch 350 Loss -1.5674\n",
      "Epoch 256 Loss -1.5753\n",
      "{'Epoch': 256}\n",
      "Epoch 257 Batch 0 Loss -1.1261\n",
      "Epoch 257 Batch 50 Loss -1.4786\n",
      "Epoch 257 Batch 100 Loss -1.4614\n",
      "Epoch 257 Batch 150 Loss -1.4737\n",
      "Epoch 257 Batch 200 Loss -1.5220\n",
      "Epoch 257 Batch 250 Loss -1.5550\n",
      "Epoch 257 Batch 300 Loss -1.5618\n",
      "Epoch 257 Batch 350 Loss -1.5690\n",
      "Epoch 257 Loss -1.5728\n",
      "{'Epoch': 257}\n",
      "Epoch 258 Batch 0 Loss -1.1179\n",
      "Epoch 258 Batch 50 Loss -1.5080\n",
      "Epoch 258 Batch 100 Loss -1.4938\n",
      "Epoch 258 Batch 150 Loss -1.5069\n",
      "Epoch 258 Batch 200 Loss -1.5568\n",
      "Epoch 258 Batch 250 Loss -1.5902\n",
      "Epoch 258 Batch 300 Loss -1.6010\n",
      "Epoch 258 Batch 350 Loss -1.6098\n",
      "Epoch 258 Loss -1.6175\n",
      "{'Epoch': 258}\n",
      "Epoch 259 Batch 0 Loss -1.1277\n",
      "Epoch 259 Batch 50 Loss -1.5246\n",
      "Epoch 259 Batch 100 Loss -1.5080\n",
      "Epoch 259 Batch 150 Loss -1.5139\n",
      "Epoch 259 Batch 200 Loss -1.5468\n",
      "Epoch 259 Batch 250 Loss -1.5735\n",
      "Epoch 259 Batch 300 Loss -1.5818\n",
      "Epoch 259 Batch 350 Loss -1.5852\n",
      "Epoch 259 Loss -1.5910\n",
      "{'Epoch': 259}\n",
      "Epoch 260 Batch 0 Loss -0.9783\n",
      "Epoch 260 Batch 50 Loss -1.4575\n",
      "Epoch 260 Batch 100 Loss -1.4662\n",
      "Epoch 260 Batch 150 Loss -1.4862\n",
      "Epoch 260 Batch 200 Loss -1.5380\n",
      "Epoch 260 Batch 250 Loss -1.5686\n",
      "Epoch 260 Batch 300 Loss -1.5764\n",
      "Epoch 260 Batch 350 Loss -1.5806\n",
      "Epoch 260 Loss -1.5866\n",
      "{'Epoch': 260}\n",
      "Epoch 261 Batch 0 Loss -1.1161\n",
      "Epoch 261 Batch 50 Loss -1.4810\n",
      "Epoch 261 Batch 100 Loss -1.4658\n",
      "Epoch 261 Batch 150 Loss -1.4772\n",
      "Epoch 261 Batch 200 Loss -1.5237\n",
      "Epoch 261 Batch 250 Loss -1.5544\n",
      "Epoch 261 Batch 300 Loss -1.5616\n",
      "Epoch 261 Batch 350 Loss -1.5717\n",
      "Epoch 261 Loss -1.5785\n",
      "{'Epoch': 261}\n",
      "Epoch 262 Batch 0 Loss -1.1142\n",
      "Epoch 262 Batch 50 Loss -1.4640\n",
      "Epoch 262 Batch 100 Loss -1.4566\n",
      "Epoch 262 Batch 150 Loss -1.4723\n",
      "Epoch 262 Batch 200 Loss -1.5224\n",
      "Epoch 262 Batch 250 Loss -1.5555\n",
      "Epoch 262 Batch 300 Loss -1.5680\n",
      "Epoch 262 Batch 350 Loss -1.5780\n",
      "Epoch 262 Loss -1.5856\n",
      "{'Epoch': 262}\n",
      "Epoch 263 Batch 0 Loss -1.1150\n",
      "Epoch 263 Batch 50 Loss -1.4822\n",
      "Epoch 263 Batch 100 Loss -1.4600\n",
      "Epoch 263 Batch 150 Loss -1.4779\n",
      "Epoch 263 Batch 200 Loss -1.5284\n",
      "Epoch 263 Batch 250 Loss -1.5623\n",
      "Epoch 263 Batch 300 Loss -1.5741\n",
      "Epoch 263 Batch 350 Loss -1.5831\n",
      "Epoch 263 Loss -1.5910\n",
      "{'Epoch': 263}\n",
      "Epoch 264 Batch 0 Loss -1.1450\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [376], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: step})\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (batch, (b, _)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_iter):\n\u001b[0;32m---> 14\u001b[0m   params, opt_state, loss, rng \u001b[38;5;241m=\u001b[39m \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m   train_loss(loss)\n\u001b[1;32m     18\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m__new__\u001b[0;34m(_cls)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#EPOCHS = 200\n",
    "\n",
    "#opt_state = optimiser.init(params)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "  data_iter = train.batched_onehot_set.prefetch(tf.data.experimental.AUTOTUNE).as_numpy_iterator()\n",
    "\n",
    "  train_loss.reset_states()\n",
    "\n",
    "  print({\"Epoch\": step})\n",
    "  for (batch, (b, _)) in enumerate(data_iter):\n",
    "    params, opt_state, loss, rng = update(params, rng, opt_state, b)\n",
    "\n",
    "    train_loss(loss)\n",
    "\n",
    "    if batch % 50 == 0:\n",
    "      print(f'Epoch {step + 1} Batch {batch} Loss {train_loss.result():.4f}')\n",
    "\n",
    "  print(f'Epoch {step + 1} Loss {train_loss.result():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need our own sampling function in this case\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "\n",
    "def fill_diagonal(a, val):\n",
    "  assert a.ndim >= 2\n",
    "  i, j = jnp.diag_indices(min(a.shape[-2:]))\n",
    "  return a.at[..., i, j].set(val)\n",
    "\n",
    "# sample the mixture model\n",
    "# input: res (mixture_components)\n",
    "#        b (temperature)\n",
    "# output: sample, pis, mean, variance\n",
    "def sample_mix_model(res, rng, b):\n",
    "      #print(res.shape)\n",
    "      \n",
    "      pis, mu, sig, rho, eos = jnp.array_split(res, [NUM_MIX_COM, NUM_MIX_COM*3, NUM_MIX_COM*5, NUM_MIX_COM*6], axis=-1)\n",
    "\n",
    "      # weights - must be a probability distribution so softmax over all components\n",
    "      pis = jax.nn.softmax(pis * (1.+b))\n",
    "\n",
    "\n",
    "      # means - no transformation needed\n",
    "      mu_x1, mu_x2 = jnp.array_split(mu, 2, axis=-1)\n",
    "      \n",
    "      # standard deviations - must be strictly positive so exponent\n",
    "      sig = jnp.exp(sig - b)\n",
    "\n",
    "      sig_x1, sig_x2 = jnp.array_split(sig, 2, axis=-1)\n",
    "            \n",
    "      # correlations - squish to -1 to 1 with tanh activation\n",
    "      rho = jnp.tanh(rho)\n",
    "\n",
    "      a = jnp.zeros((NUM_MIX_COM, 2, 2))\n",
    "\n",
    "      S = fill_diagonal(a, jnp.stack([sig_x1, sig_x2], axis=-1))\n",
    "\n",
    "      #print(S)\n",
    "\n",
    "      #print(S.shape)\n",
    "\n",
    "      #E = jnp.eye(2, batch_shape=[NUM_MIX_COM])\n",
    "      E = jnp.repeat(jnp.eye(2)[None, :], NUM_MIX_COM, axis=0)\n",
    "\n",
    "      rho_exp = jnp.reshape(jnp.repeat(rho, 4), [NUM_MIX_COM, 2, 2])\n",
    "    \n",
    "      corr_mat = jnp.where(jnp.equal(E, 1.), E, rho_exp)\n",
    "      \n",
    "      cov_mat = jnp.matmul(S, corr_mat)\n",
    "      cov_mat = jnp.matmul(cov_mat, S)\n",
    "\n",
    "      #print(cov_mat)\n",
    "\n",
    "      #print(cov_mat.shape)\n",
    "\n",
    "      # The distribution is a mixture of gaussians\n",
    "      gm = tfp.distributions.MixtureSameFamily(mixture_distribution=tfp.distributions.Categorical(probs=pis),\n",
    "            components_distribution=tfp.distributions.MultivariateNormalTriL(loc=jnp.stack([mu_x1, mu_x2], axis=-1),\n",
    "                                                                    scale_tril=jax.lax.linalg.cholesky(cov_mat)))\n",
    "\n",
    "      # End of stroke\n",
    "      eos = jax.nn.sigmoid(eos)\n",
    "      \n",
    "      bd = tfp.distributions.Bernoulli(probs=eos, dtype=float)\n",
    "\n",
    "      #print(bd.sample(seed=jax.random.PRNGKey(seed=1)))\n",
    "      \n",
    "      rng, new_rng = jax.random.split(rng)\n",
    "\n",
    "      bd_samp = bd.sample(seed=new_rng)\n",
    "      \n",
    "      #print(tf.concat([gm.sample(), eos], axis=-1))\n",
    "\n",
    "      rng, new_rng = jax.random.split(rng)\n",
    "      \n",
    "      gm_samp = gm.sample(seed=new_rng)\n",
    "\n",
    "      return jnp.hstack((gm_samp, bd_samp, gm.mean(), gm.covariance().ravel(), pis)), rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-1.8900048e-02, -3.3042753e-01,  0.0000000e+00,\n",
       "             -1.8898826e-02, -3.2954726e-01,  2.7309941e-05,\n",
       "             -6.3517196e-05, -6.3517196e-05,  3.7825466e-04,\n",
       "              4.3236162e-04,  1.2162946e-10,  1.2430813e-13,\n",
       "              5.7854582e-20,  4.3210040e-11,  9.7065145e-10,\n",
       "              1.6838317e-27,  2.7926050e-09,  9.9632746e-01,\n",
       "              1.1438686e-16,  2.2631118e-04,  6.9324026e-12,\n",
       "              1.0700577e-06,  5.8040485e-05,  1.0115902e-20,\n",
       "              2.9548528e-03,  5.8673964e-15,  8.0648543e-21,\n",
       "              8.7738051e-14,  5.4580181e-20], dtype=float32)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=10\n",
    "\n",
    "rng = jax.random.PRNGKey(5)\n",
    "rng, new_rng = jax.random.split(rng)\n",
    "dec_input = jnp.zeros((1, 1, 3))\n",
    "predictions_all = writing_transformer.apply(params, new_rng, one_hot_sentence, dec_input, False)\n",
    "\n",
    "predictions_all[0, -1, :]\n",
    "\n",
    "predictions, rng = sample_mix_model(predictions_all[0, -1, :], rng, b)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sample_mix_model() missing 1 required positional argument: 'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [257], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m42\u001b[39m) \n\u001b[1;32m      6\u001b[0m predictions_all \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mapply(params, key, one_hot_sentence, dec_input)\n\u001b[0;32m----> 8\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43msample_mix_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions_all\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m pred_strokes \u001b[38;5;241m=\u001b[39m predictions[:\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     11\u001b[0m pred_strokes \u001b[38;5;241m=\u001b[39m pred_strokes[jnp\u001b[38;5;241m.\u001b[39mnewaxis, jnp\u001b[38;5;241m.\u001b[39mnewaxis, :]\n",
      "\u001b[0;31mTypeError\u001b[0m: sample_mix_model() missing 1 required positional argument: 'b'"
     ]
    }
   ],
   "source": [
    "# Test the sample function\n",
    "dec_input = jnp.zeros((1, 1, 3))\n",
    "one_hot_sentence = convert_sentence(encoding_sent)\n",
    "\n",
    "key = jax.random.PRNGKey(42) \n",
    "predictions_all = network.apply(params, key, one_hot_sentence, dec_input)\n",
    "\n",
    "predictions = sample_mix_model(predictions_all[0, -1, :], 1)\n",
    "\n",
    "pred_strokes = predictions[:3]\n",
    "pred_strokes = pred_strokes[jnp.newaxis, jnp.newaxis, :]\n",
    "\n",
    "pred_strokes.shape\n",
    "\n",
    "dec_input = jax.lax.concatenate([dec_input, pred_strokes], 1)\n",
    "#dec_input = jnp.stack([dec_input, pred_strokes])\n",
    "\n",
    "dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def fill_diagonal(a, val):\n",
    "  assert a.ndim >= 2\n",
    "  i, j = jnp.diag_indices(min(a.shape[-2:]))\n",
    "  return a.at[..., i, j].set(val)\n",
    "\n",
    "a = jnp.zeros((2, 3, 4, 4))\n",
    "\n",
    "# works for scalars\n",
    "a1 = fill_diagonal(a, 2)\n",
    "\n",
    "# or for batched vectors\n",
    "a2 = fill_diagonal(a, jnp.arange(24).reshape(2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4, 4)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need an evaluate function that will take in a character sequence and \n",
    "# generate some writing\n",
    "\n",
    "# Convert a sentence to a one-hot-encoded vector\n",
    "def convert_sentence(sentence):\n",
    "  # Convert it to a one-hot encoded vector for the encoder\n",
    "  U_conv = tf.keras.backend.one_hot(train.text_to_int(sentence), len(train.vocab)+1)\n",
    "  #U_conv = train.text_to_int(sentence)\n",
    "  # Pad it to match the original data that was input into the encoder\n",
    "  U_conv = tf.keras.preprocessing.sequence.pad_sequences([U_conv],\n",
    "                                                         maxlen=train.MAX_CHAR_SEQ_LEN,\n",
    "                                                         padding='post',\n",
    "                                                         value=train.char_padding_value);\n",
    "  #U_conv = tf.convert_to_tensor(U_conv, dtype='float32')\n",
    "\n",
    "  return jnp.asarray(U_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20, 101)"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_sent = 'Hello World!'\n",
    "\n",
    "one_hot_sentence = convert_sentence(encoding_sent)\n",
    "\n",
    "one_hot_sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "@hk.transform\n",
    "def writing_transformer(inp: jnp.ndarray, tar: jnp.ndarray, training: bool) -> jnp.ndarray:\n",
    "    tra = Writing_Transformer(num_layers, key_size, d_model, num_heads, dff, pe_encoding=250, pe_target=1000, dropout_rate=dropout_rate)\n",
    "\n",
    "    return tra(inp, tar, training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "output_name = \"test_attention.mp4\"\n",
    "\n",
    "b = 10.0\n",
    "\n",
    "def evaluate(U_conv):\n",
    "  #gen_sequence = np.zeros((1, 3))\n",
    "\n",
    "  dec_input = jnp.zeros((1, 1, 3))\n",
    "\n",
    "  MAX_LEN = 250\n",
    "\n",
    " # transformer.reset_states()\n",
    "\n",
    "  #fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    " # ims = []\n",
    "\n",
    "  rng = jax.random.PRNGKey(5) \n",
    "\n",
    "#  for t in range(int(train.MAX_STROKE_LEN)):\n",
    "  for t in range(int(MAX_LEN)):\n",
    "    # Create masks.  Even in the inference stage we may create input that is \n",
    "    # padded, such as the one-hot_sentence, and we always need a look-ahead \n",
    "    # mask\n",
    "    #enc_padding_mask, combined_mask, dec_padding_mask = create_masks(U_conv, dec_input)\n",
    "\n",
    "    rng, new_rng = jax.random.split(rng)\n",
    "\n",
    "    predictions_all = writing_transformer.apply(params, new_rng, U_conv, dec_input, False)\n",
    "\n",
    "    #rng = new_rng\n",
    "\n",
    "    #predictions_all = transformer(U_conv,\n",
    "    #                                                     dec_input,\n",
    "     #                                                    False,\n",
    "     #                                                    enc_padding_mask,\n",
    "     #                                                    combined_mask,\n",
    "     #                                                    dec_padding_mask)\n",
    "\n",
    "    #data = tf.squeeze(attention_weights['decoder_layer1_block2'], 0)[0]\n",
    "  \n",
    "    #data_all = np.zeros((MAX_LEN, 20))\n",
    "\n",
    "    #data_all[:data.shape[0], :] = data\n",
    "\n",
    "    #ax = fig.add_subplot(1, 1, 1)\n",
    "    #im = plt.imshow(data_all, cmap='viridis', interpolation='nearest', aspect='auto', animated=True)\n",
    "\n",
    "    #ax = plt.gca()\n",
    "\n",
    "    #labels = 'Eye tracking....'\n",
    "\n",
    "    #ax.set_xticks(range(0, train.MAX_CHAR_SEQ_LEN-1))\n",
    "    #ax.set_xticklabels(labels)\n",
    "\n",
    "    #ax.set_xlabel('Characters to be Written')\n",
    "    #ax.set_ylabel('Stroke Number')\n",
    "\n",
    "    #ims.append([im])\n",
    "\n",
    "    predictions, rng = sample_mix_model(predictions_all[0, -1, :], rng, b)\n",
    "\n",
    "    pred_strokes = predictions[:3]\n",
    "    pred_strokes = pred_strokes[jnp.newaxis, jnp.newaxis, :]\n",
    "\n",
    "    #pred_strokes.shape\n",
    "\n",
    "    dec_input = jax.lax.concatenate([dec_input, pred_strokes], 1)\n",
    "\n",
    "    #print(dec_input.shape)\n",
    "\n",
    "    #print(dec_input.shape)\n",
    "\n",
    "  #ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True,\n",
    "   #                             repeat_delay=1000)\n",
    "  \n",
    "  #ani.save(output_name)\n",
    "\n",
    "  #plt.show()\n",
    "\n",
    "  #return dec_input.numpy(), attention_weights\n",
    "  return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(one_hot_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "             [-6.58484995e-02, -4.53935087e-01,  0.00000000e+00],\n",
       "             [-2.72910185e-02, -7.08441794e-01,  0.00000000e+00],\n",
       "             [-7.82406926e-02, -9.20342207e-01,  0.00000000e+00],\n",
       "             [-1.43423662e-01, -1.13145125e+00,  0.00000000e+00],\n",
       "             [-2.18539059e-01, -1.28711855e+00,  0.00000000e+00],\n",
       "             [-2.84574896e-01, -1.36015594e+00,  0.00000000e+00],\n",
       "             [-3.22692603e-01, -1.36078620e+00,  0.00000000e+00],\n",
       "             [-3.22469383e-01, -1.28471673e+00,  0.00000000e+00],\n",
       "             [-2.84034610e-01, -1.11360776e+00,  0.00000000e+00],\n",
       "             [-2.26841703e-01, -8.54882360e-01,  0.00000000e+00],\n",
       "             [-1.56656519e-01, -4.94713783e-01,  0.00000000e+00],\n",
       "             [-8.71679112e-02, -9.84423980e-02,  1.00000000e+00],\n",
       "             [ 3.48062658e+00,  8.74321651e+00,  0.00000000e+00],\n",
       "             [ 1.58640459e-01, -4.16771352e-01,  0.00000000e+00],\n",
       "             [ 6.18601777e-02, -7.79793382e-01,  0.00000000e+00],\n",
       "             [-6.91035688e-02, -1.06263530e+00,  0.00000000e+00],\n",
       "             [-1.92758173e-01, -1.35249269e+00,  0.00000000e+00],\n",
       "             [-2.77012944e-01, -1.53538740e+00,  0.00000000e+00],\n",
       "             [-3.16689819e-01, -1.57310784e+00,  0.00000000e+00],\n",
       "             [-2.94123113e-01, -1.46786213e+00,  0.00000000e+00],\n",
       "             [-2.46062219e-01, -1.24580491e+00,  0.00000000e+00],\n",
       "             [-1.96150705e-01, -9.30650055e-01,  0.00000000e+00],\n",
       "             [-1.51301354e-01, -5.25408268e-01,  0.00000000e+00],\n",
       "             [-1.09066792e-01, -1.34128109e-01,  0.00000000e+00],\n",
       "             [-1.20929495e-01,  1.97677374e-01,  0.00000000e+00],\n",
       "             [-1.69148758e-01,  5.81651747e-01,  0.00000000e+00],\n",
       "             [-1.12774983e-01,  9.55862999e-01,  0.00000000e+00],\n",
       "             [ 1.31484484e-02,  1.16345775e+00,  0.00000000e+00],\n",
       "             [ 1.63437709e-01,  1.31721842e+00,  0.00000000e+00],\n",
       "             [ 3.52888465e-01,  1.39123869e+00,  0.00000000e+00],\n",
       "             [ 5.65348148e-01,  1.35070431e+00,  0.00000000e+00],\n",
       "             [ 7.64869511e-01,  1.23233640e+00,  0.00000000e+00],\n",
       "             [ 9.47253585e-01,  1.05071628e+00,  0.00000000e+00],\n",
       "             [ 1.07757008e+00,  8.11397314e-01,  0.00000000e+00],\n",
       "             [ 1.15725958e+00,  4.89488125e-01,  0.00000000e+00],\n",
       "             [ 1.16578436e+00,  1.49026960e-01,  0.00000000e+00],\n",
       "             [ 1.11179066e+00, -1.30393833e-01,  0.00000000e+00],\n",
       "             [ 1.00032663e+00, -3.96706134e-01,  0.00000000e+00],\n",
       "             [ 8.64501297e-01, -6.61447167e-01,  0.00000000e+00],\n",
       "             [ 6.34081841e-01, -8.42932105e-01,  0.00000000e+00],\n",
       "             [ 3.31600785e-01, -1.01713228e+00,  0.00000000e+00],\n",
       "             [-2.65477262e-02, -1.14146948e+00,  0.00000000e+00],\n",
       "             [-3.92567843e-01, -1.21340084e+00,  0.00000000e+00],\n",
       "             [-6.69590294e-01, -1.16267562e+00,  0.00000000e+00],\n",
       "             [-8.81475389e-01, -9.57140148e-01,  0.00000000e+00],\n",
       "             [-1.00102794e+00, -6.87260687e-01,  0.00000000e+00],\n",
       "             [-1.02620006e+00, -3.97362977e-01,  0.00000000e+00],\n",
       "             [-9.53941166e-01, -8.19489807e-02,  0.00000000e+00],\n",
       "             [-8.10568035e-01,  1.95410222e-01,  0.00000000e+00],\n",
       "             [-5.82075655e-01,  4.17931885e-01,  0.00000000e+00],\n",
       "             [-2.84788519e-01,  5.35175562e-01,  0.00000000e+00],\n",
       "             [ 9.00018290e-02,  5.81030607e-01,  0.00000000e+00],\n",
       "             [ 5.31837702e-01,  5.69520414e-01,  0.00000000e+00],\n",
       "             [ 9.50928688e-01,  5.44926465e-01,  0.00000000e+00],\n",
       "             [ 1.31393671e+00,  5.43347776e-01,  0.00000000e+00],\n",
       "             [ 1.63441455e+00,  5.75386941e-01,  0.00000000e+00],\n",
       "             [ 1.86774981e+00,  6.04288936e-01,  0.00000000e+00],\n",
       "             [ 1.98029876e+00,  6.22510791e-01,  0.00000000e+00],\n",
       "             [ 1.99914610e+00,  6.42969608e-01,  0.00000000e+00],\n",
       "             [ 1.96350801e+00,  6.73849344e-01,  0.00000000e+00],\n",
       "             [ 1.87403345e+00,  7.26070464e-01,  0.00000000e+00],\n",
       "             [ 1.70311785e+00,  8.00364196e-01,  0.00000000e+00],\n",
       "             [ 1.45172906e+00,  8.77507567e-01,  0.00000000e+00],\n",
       "             [ 1.13853860e+00,  9.36590075e-01,  0.00000000e+00],\n",
       "             [ 7.51398265e-01,  9.63143289e-01,  0.00000000e+00],\n",
       "             [ 3.22711110e-01,  9.20603991e-01,  0.00000000e+00],\n",
       "             [-1.19632401e-01,  7.67536759e-01,  0.00000000e+00],\n",
       "             [-5.34258604e-01,  4.94597226e-01,  0.00000000e+00],\n",
       "             [-8.40597093e-01,  1.68543845e-01,  0.00000000e+00],\n",
       "             [-1.04540145e+00, -1.83062345e-01,  0.00000000e+00],\n",
       "             [-1.16108084e+00, -5.48790276e-01,  0.00000000e+00],\n",
       "             [-1.17783844e+00, -9.58576262e-01,  0.00000000e+00],\n",
       "             [-1.09949791e+00, -1.36339474e+00,  0.00000000e+00],\n",
       "             [-9.09468055e-01, -1.66696739e+00,  0.00000000e+00],\n",
       "             [-6.38070524e-01, -1.80927944e+00,  0.00000000e+00],\n",
       "             [-3.45431268e-01, -1.82834303e+00,  0.00000000e+00],\n",
       "             [-2.32149214e-02, -1.72700191e+00,  0.00000000e+00],\n",
       "             [ 3.21689606e-01, -1.49493706e+00,  0.00000000e+00],\n",
       "             [ 6.88849866e-01, -1.12210226e+00,  0.00000000e+00],\n",
       "             [ 1.06432188e+00, -7.93307066e-01,  0.00000000e+00],\n",
       "             [ 1.48418820e+00, -4.20743823e-01,  0.00000000e+00],\n",
       "             [ 1.89811015e+00, -1.40300468e-02,  0.00000000e+00],\n",
       "             [ 2.21906662e+00,  3.91403317e-01,  0.00000000e+00],\n",
       "             [ 2.35737491e+00,  7.48991370e-01,  1.00000000e+00],\n",
       "             [ 4.84699249e+00,  2.91868424e+00,  0.00000000e+00],\n",
       "             [-3.55451167e-01,  3.37195843e-02,  0.00000000e+00],\n",
       "             [-4.28901821e-01,  4.77968939e-02,  0.00000000e+00],\n",
       "             [-5.60449779e-01, -3.36844213e-02,  0.00000000e+00],\n",
       "             [-7.05150306e-01, -1.62923545e-01,  0.00000000e+00],\n",
       "             [-8.53978574e-01, -3.59265894e-01,  0.00000000e+00],\n",
       "             [-9.43288028e-01, -6.18460298e-01,  0.00000000e+00],\n",
       "             [-9.39308524e-01, -8.80467176e-01,  0.00000000e+00],\n",
       "             [-7.55304039e-01, -1.07115543e+00,  0.00000000e+00],\n",
       "             [-4.45131570e-01, -1.09084904e+00,  0.00000000e+00],\n",
       "             [-3.52417007e-02, -9.23392773e-01,  0.00000000e+00],\n",
       "             [ 3.76176178e-01, -6.17718518e-01,  0.00000000e+00],\n",
       "             [ 7.25546658e-01, -2.99894840e-01,  0.00000000e+00],\n",
       "             [ 9.72480655e-01,  7.62659609e-02,  0.00000000e+00],\n",
       "             [ 1.15884912e+00,  5.03504217e-01,  0.00000000e+00],\n",
       "             [ 1.26025319e+00,  9.63394225e-01,  0.00000000e+00],\n",
       "             [ 1.28829753e+00,  1.36720991e+00,  0.00000000e+00],\n",
       "             [ 1.28907502e+00,  1.76129353e+00,  0.00000000e+00],\n",
       "             [ 1.25096929e+00,  2.07258368e+00,  0.00000000e+00],\n",
       "             [ 1.17010367e+00,  2.18851519e+00,  0.00000000e+00],\n",
       "             [ 1.03213751e+00,  2.17040348e+00,  0.00000000e+00],\n",
       "             [ 8.77494872e-01,  2.05559254e+00,  0.00000000e+00],\n",
       "             [ 7.07135022e-01,  1.86215675e+00,  0.00000000e+00],\n",
       "             [ 5.01269042e-01,  1.56219637e+00,  0.00000000e+00],\n",
       "             [ 2.70323634e-01,  1.15693450e+00,  0.00000000e+00],\n",
       "             [ 3.36827822e-02,  6.74937963e-01,  0.00000000e+00],\n",
       "             [-1.72174513e-01,  2.02458784e-01,  0.00000000e+00],\n",
       "             [-2.60294050e-01, -1.25353619e-01,  0.00000000e+00],\n",
       "             [-4.35360700e-01, -5.67379117e-01,  0.00000000e+00],\n",
       "             [-5.41939795e-01, -1.10097301e+00,  0.00000000e+00],\n",
       "             [-6.18331909e-01, -1.59565806e+00,  0.00000000e+00],\n",
       "             [-6.35377407e-01, -1.95992982e+00,  0.00000000e+00],\n",
       "             [-5.64204156e-01, -2.17751527e+00,  0.00000000e+00],\n",
       "             [-4.19962823e-01, -2.22682381e+00,  0.00000000e+00],\n",
       "             [-2.31326163e-01, -2.11672163e+00,  0.00000000e+00],\n",
       "             [-2.56367046e-02, -1.88352871e+00,  0.00000000e+00],\n",
       "             [ 1.83136910e-01, -1.61610413e+00,  0.00000000e+00],\n",
       "             [ 4.49342251e-01, -1.26117659e+00,  0.00000000e+00],\n",
       "             [ 7.40234733e-01, -8.67621362e-01,  0.00000000e+00],\n",
       "             [ 1.03077483e+00, -4.60418880e-01,  0.00000000e+00],\n",
       "             [ 1.26958334e+00, -6.51068389e-02,  0.00000000e+00],\n",
       "             [ 1.52525938e+00,  3.32662702e-01,  0.00000000e+00],\n",
       "             [ 1.76094282e+00,  7.09017515e-01,  0.00000000e+00],\n",
       "             [ 1.89922726e+00,  1.03396595e+00,  0.00000000e+00],\n",
       "             [ 1.91320312e+00,  1.26439130e+00,  0.00000000e+00],\n",
       "             [ 1.81442595e+00,  1.40746200e+00,  0.00000000e+00],\n",
       "             [ 1.62212527e+00,  1.44235373e+00,  0.00000000e+00],\n",
       "             [ 1.34796321e+00,  1.39536917e+00,  0.00000000e+00],\n",
       "             [ 1.01691997e+00,  1.30048418e+00,  0.00000000e+00],\n",
       "             [ 6.84451163e-01,  1.18803585e+00,  0.00000000e+00],\n",
       "             [ 3.22560102e-01,  1.01896632e+00,  0.00000000e+00],\n",
       "             [-4.32251990e-02,  7.79989362e-01,  0.00000000e+00],\n",
       "             [-3.57753336e-01,  4.60669607e-01,  0.00000000e+00],\n",
       "             [-5.66493273e-01,  1.58474118e-01,  0.00000000e+00],\n",
       "             [-7.90329576e-01, -1.88983783e-01,  0.00000000e+00],\n",
       "             [-9.55206335e-01, -6.29408002e-01,  0.00000000e+00],\n",
       "             [-1.04922986e+00, -1.07070768e+00,  0.00000000e+00],\n",
       "             [-1.05547965e+00, -1.48501289e+00,  0.00000000e+00],\n",
       "             [-9.74312961e-01, -1.80992842e+00,  0.00000000e+00],\n",
       "             [-7.84117758e-01, -2.01696634e+00,  0.00000000e+00],\n",
       "             [-5.40100276e-01, -2.07069230e+00,  0.00000000e+00],\n",
       "             [-2.48839453e-01, -1.97672904e+00,  0.00000000e+00],\n",
       "             [ 6.84332177e-02, -1.76630235e+00,  0.00000000e+00],\n",
       "             [ 4.24535990e-01, -1.45745671e+00,  0.00000000e+00],\n",
       "             [ 8.25789511e-01, -1.05117452e+00,  0.00000000e+00],\n",
       "             [ 1.26634717e+00, -6.20727360e-01,  0.00000000e+00],\n",
       "             [ 1.64028275e+00, -1.54999241e-01,  0.00000000e+00],\n",
       "             [ 1.95824707e+00,  2.73160011e-01,  0.00000000e+00],\n",
       "             [ 2.14858890e+00,  6.78464830e-01,  0.00000000e+00],\n",
       "             [ 2.18311071e+00,  1.00667202e+00,  0.00000000e+00],\n",
       "             [ 2.14472914e+00,  1.21343231e+00,  0.00000000e+00],\n",
       "             [ 2.03269267e+00,  1.30208194e+00,  0.00000000e+00],\n",
       "             [ 1.85520208e+00,  1.30535877e+00,  0.00000000e+00],\n",
       "             [ 1.59843576e+00,  1.22258317e+00,  0.00000000e+00],\n",
       "             [ 1.27468109e+00,  1.06174612e+00,  0.00000000e+00],\n",
       "             [ 9.14032817e-01,  8.76397014e-01,  0.00000000e+00],\n",
       "             [ 5.08779168e-01,  6.61112249e-01,  0.00000000e+00],\n",
       "             [ 5.75713292e-02,  4.30774778e-01,  0.00000000e+00],\n",
       "             [-3.12974155e-01,  2.29707494e-01,  0.00000000e+00],\n",
       "             [-6.24683261e-01,  3.52846384e-02,  0.00000000e+00],\n",
       "             [-8.91593516e-01, -2.52273917e-01,  0.00000000e+00],\n",
       "             [-1.05475354e+00, -5.74866116e-01,  0.00000000e+00],\n",
       "             [-1.13932717e+00, -9.15774643e-01,  0.00000000e+00],\n",
       "             [-1.11858416e+00, -1.25494349e+00,  0.00000000e+00],\n",
       "             [-9.68994379e-01, -1.52226412e+00,  0.00000000e+00],\n",
       "             [-6.93813860e-01, -1.64099967e+00,  0.00000000e+00],\n",
       "             [-3.44873548e-01, -1.59280288e+00,  0.00000000e+00],\n",
       "             [ 4.15649191e-02, -1.40195644e+00,  0.00000000e+00],\n",
       "             [ 4.42217052e-01, -1.04649925e+00,  0.00000000e+00],\n",
       "             [ 8.13290656e-01, -6.28925800e-01,  0.00000000e+00],\n",
       "             [ 1.17755783e+00, -2.10431844e-01,  0.00000000e+00],\n",
       "             [ 1.50006151e+00,  2.57700324e-01,  0.00000000e+00],\n",
       "             [ 1.78829610e+00,  7.82216907e-01,  0.00000000e+00],\n",
       "             [ 1.93924129e+00,  1.26145792e+00,  0.00000000e+00],\n",
       "             [ 2.16164637e+00,  1.73081112e+00,  0.00000000e+00],\n",
       "             [ 2.27004409e+00,  2.10658312e+00,  0.00000000e+00],\n",
       "             [ 2.24869609e+00,  2.34772253e+00,  0.00000000e+00],\n",
       "             [ 2.15683579e+00,  2.49815965e+00,  0.00000000e+00],\n",
       "             [ 1.99695182e+00,  2.54855919e+00,  0.00000000e+00],\n",
       "             [ 1.80348492e+00,  2.50483084e+00,  0.00000000e+00],\n",
       "             [ 1.59007061e+00,  2.36748123e+00,  0.00000000e+00],\n",
       "             [ 1.23096478e+00,  2.01045704e+00,  0.00000000e+00],\n",
       "             [ 9.28014934e-01,  1.67951858e+00,  0.00000000e+00],\n",
       "             [ 6.32584393e-01,  1.33054519e+00,  0.00000000e+00],\n",
       "             [ 3.00278425e-01,  9.07679260e-01,  0.00000000e+00],\n",
       "             [-6.04965612e-02,  4.86175090e-01,  0.00000000e+00],\n",
       "             [-3.41139317e-01,  1.46821797e-01,  0.00000000e+00],\n",
       "             [-5.52652657e-01, -2.07324952e-01,  0.00000000e+00],\n",
       "             [-8.48129988e-01, -7.94067025e-01,  0.00000000e+00],\n",
       "             [-1.01587677e+00, -1.29392970e+00,  0.00000000e+00],\n",
       "             [-1.09149921e+00, -1.77397966e+00,  0.00000000e+00],\n",
       "             [-1.08641887e+00, -2.12788081e+00,  0.00000000e+00],\n",
       "             [-9.48870063e-01, -2.30899167e+00,  0.00000000e+00],\n",
       "             [-7.38104641e-01, -2.29384160e+00,  0.00000000e+00],\n",
       "             [-4.70592380e-01, -2.11051941e+00,  0.00000000e+00],\n",
       "             [-2.04980388e-01, -1.86338353e+00,  0.00000000e+00],\n",
       "             [ 4.68707755e-02, -1.53890240e+00,  0.00000000e+00],\n",
       "             [ 3.25082749e-01, -1.11417818e+00,  0.00000000e+00],\n",
       "             [ 5.90782702e-01, -6.81380332e-01,  0.00000000e+00],\n",
       "             [ 8.46304595e-01, -2.98497170e-01,  0.00000000e+00],\n",
       "             [ 1.10047281e+00,  8.36310163e-02,  0.00000000e+00],\n",
       "             [ 1.34465766e+00,  5.33110857e-01,  0.00000000e+00],\n",
       "             [ 1.54689825e+00,  9.61585283e-01,  1.00000000e+00],\n",
       "             [-2.97156954e+00,  2.60515070e+00,  0.00000000e+00],\n",
       "             [-2.17904329e-01, -3.73547494e-01,  0.00000000e+00],\n",
       "             [ 3.64251167e-01, -4.60016578e-01,  0.00000000e+00],\n",
       "             [ 6.64305389e-01, -4.25681502e-01,  0.00000000e+00],\n",
       "             [ 1.09819758e+00, -3.19406152e-01,  0.00000000e+00],\n",
       "             [ 1.49204051e+00, -1.38086706e-01,  0.00000000e+00],\n",
       "             [ 1.80892491e+00,  5.30550443e-02,  0.00000000e+00],\n",
       "             [ 1.98388612e+00,  2.11857632e-01,  0.00000000e+00],\n",
       "             [ 2.00991821e+00,  3.25806379e-01,  0.00000000e+00],\n",
       "             [ 1.90741789e+00,  4.08238530e-01,  0.00000000e+00],\n",
       "             [ 1.68430912e+00,  4.79745209e-01,  0.00000000e+00],\n",
       "             [ 1.33152926e+00,  5.51375628e-01,  0.00000000e+00],\n",
       "             [ 9.54938710e-01,  6.03280425e-01,  0.00000000e+00],\n",
       "             [ 5.14721930e-01,  6.14002466e-01,  0.00000000e+00],\n",
       "             [ 6.02031313e-02,  5.94799757e-01,  0.00000000e+00],\n",
       "             [-3.36582780e-01,  5.08497596e-01,  0.00000000e+00],\n",
       "             [-6.12769604e-01,  3.62570256e-01,  0.00000000e+00],\n",
       "             [-7.89929509e-01,  2.01723158e-01,  0.00000000e+00],\n",
       "             [-8.46543372e-01,  2.07457948e-03,  0.00000000e+00],\n",
       "             [-8.51482987e-01, -2.32627958e-01,  0.00000000e+00],\n",
       "             [-7.95932531e-01, -4.90289330e-01,  0.00000000e+00],\n",
       "             [-7.11549282e-01, -7.70774007e-01,  0.00000000e+00],\n",
       "             [-5.73606074e-01, -1.05464780e+00,  0.00000000e+00],\n",
       "             [-3.84840429e-01, -1.25701272e+00,  0.00000000e+00],\n",
       "             [-1.32701844e-01, -1.36821043e+00,  0.00000000e+00],\n",
       "             [ 1.27229318e-01, -1.39538419e+00,  0.00000000e+00],\n",
       "             [ 3.95100445e-01, -1.32380939e+00,  0.00000000e+00],\n",
       "             [ 6.89466000e-01, -1.17535639e+00,  0.00000000e+00],\n",
       "             [ 9.68305707e-01, -9.37634587e-01,  0.00000000e+00],\n",
       "             [ 1.25218761e+00, -6.01869464e-01,  0.00000000e+00],\n",
       "             [ 1.46612263e+00, -2.43001685e-01,  0.00000000e+00],\n",
       "             [ 1.62619638e+00,  1.41901791e-01,  0.00000000e+00],\n",
       "             [ 1.71524107e+00,  5.47706246e-01,  0.00000000e+00],\n",
       "             [ 1.73539686e+00,  9.46108878e-01,  0.00000000e+00],\n",
       "             [ 1.68484700e+00,  1.31589496e+00,  0.00000000e+00],\n",
       "             [ 1.61482382e+00,  1.62219810e+00,  0.00000000e+00],\n",
       "             [ 1.52047157e+00,  1.88034105e+00,  0.00000000e+00],\n",
       "             [ 1.42839622e+00,  2.08587956e+00,  0.00000000e+00],\n",
       "             [ 1.32608902e+00,  2.18969607e+00,  0.00000000e+00],\n",
       "             [ 1.20960796e+00,  2.21412635e+00,  0.00000000e+00],\n",
       "             [ 1.05949771e+00,  2.16300678e+00,  0.00000000e+00],\n",
       "             [ 8.91823828e-01,  2.03147745e+00,  0.00000000e+00],\n",
       "             [ 6.85560703e-01,  1.81356311e+00,  0.00000000e+00]],            dtype=float32)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_stroke(results[0, :250, :], one_hot_sentence[0, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('mltests-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8efb2c21bab5202d09f40f8b1f8eaf82b86e2dc2c9c82b8a82cff7f696c4161c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
