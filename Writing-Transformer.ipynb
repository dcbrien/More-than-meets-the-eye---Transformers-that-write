{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/don/ML-tests/lib/python3.8/site-packages/haiku/_src/data_structures.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.\n",
      "  PyTreeDef = type(jax.tree_structure(None))\n",
      "/home/don/ML-tests/lib/python3.8/site-packages/chex/_src/pytypes.py:37: FutureWarning: jax.tree_structure is deprecated, and will be removed in a future release. Use jax.tree_util.tree_structure instead.\n",
      "  PyTreeDef = type(jax.tree_structure(None))\n",
      "2022-09-19 09:42:14.781472: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-19 09:42:15.383655: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64\n",
      "2022-09-19 09:42:15.383762: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64\n",
      "2022-09-19 09:42:15.383770: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# First going to work out some ideas in a jupyter notebook - Later I will clean this\n",
    "# up and and create some proper code.\n",
    "\n",
    "from typing import Iterator, NamedTuple\n",
    "\n",
    "# Getting some tensorflow warnings, but don't care about those right now\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import pathlib\n",
    "import string\n",
    "import glob2\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import keras\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dataclasses\n",
    "\n",
    "tf.config.experimental.set_visible_devices([], \"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The IAM On-Line Handwriting Database (IAM-OnDB) is required for this project, so you will need to ask for\n",
    "# permission for that and download. https://fki.tic.heia-fr.ch/databases/iam-on-line-handwriting-database \n",
    "# Here we will create a wrapper class to give us some tensorflow dataset summaries of just the writing portion.\n",
    "\n",
    "class WritingGenerator():\n",
    "    def __init__(self, f_name, batch_size=32):   \n",
    "        self.all_x = []\n",
    "        self.all_y = []\n",
    "\n",
    "        # How we might pad each stroke to a consistent length and batch it for fitting - With the amount \n",
    "        # of data in the database and the complexity of a Transformer, this should probably be kept to\n",
    "        # under 400 strokes and 20 characters. There is an issue here though that different people use \n",
    "        # different amounts of strokes/char and this can confuse the network. I am not sure there is an\n",
    "        # elegant way to handle that problem with a Transformer network, as we would need about 1500\n",
    "        # tokens to read in every writing sample fully with padding and this is beyond a standard \n",
    "        # Transformer. Perhaps a PerceiverAR is next?\n",
    "        self.MAX_STROKE_LEN = 500\n",
    "        self.MAX_CHAR_SEQ_LEN = 100         \n",
    "        \n",
    "        self.f_name = f_name\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.padding_value = -1.\n",
    "        self.char_padding_value = 0.\n",
    "\n",
    "        # You will need to change this and point to your own database where you unzipped all of the \n",
    "        # IAM-OnDB strokes and corresponding ascii\n",
    "        base_dir_strokes='../IamONDB/lineStrokes'\n",
    "        base_dir_ascii='../IamONDB/ascii'\n",
    "        \n",
    "        try:\n",
    "            f = open(f_name, 'r')\n",
    "        except IOError:\n",
    "            print(\"Error opening file\")\n",
    "            return 0\n",
    "      \n",
    "        f_train = list(f)\n",
    "        f.close()\n",
    "        \n",
    "        self.n_samp = len(f_train)\n",
    "\n",
    "        print('Reading ' + str(self.n_samp) + ' files')\n",
    "\n",
    "        # This will contain a list of all stroke files\n",
    "        self.f_sub_list_strokes = []\n",
    "        # This will contain a list of corresponding ascii line files\n",
    "        self.f_sub_list_ascii = []\n",
    "\n",
    "        # First create a list of all subfiles in the .txt list - we are going to treat each line as a separate sample here\n",
    "        for i, fname in enumerate(f_train):\n",
    "            path_stroke = glob2.glob(base_dir_strokes + '/' + fname.strip()[0:3] + '/' + fname.strip()[0:7] + '/' + fname.strip() + '-*.xml')    \n",
    "              \n",
    "            self.f_sub_list_strokes += path_stroke\n",
    "\n",
    "            path_line = glob2.glob(base_dir_ascii + '/' + fname.strip()[0:3] + '/' + fname.strip()[0:7] + '/' + fname.strip() + '.txt')   \n",
    "            # We want a 1 to 1 matching of strokes to ascii.  We will pull out the appropriate line when we create the dataset\n",
    "            self.f_sub_list_ascii += path_line * len(path_stroke)\n",
    "                        \n",
    "        # list datasets\n",
    "        self.list_ds_strokes = tf.data.Dataset.from_tensor_slices(self.f_sub_list_strokes)\n",
    "        self.list_ds_ascii = tf.data.Dataset.from_tensor_slices(self.f_sub_list_ascii)\n",
    "\n",
    "        # Text helper functions and variables.  All lines of text need to be one-hot-encoded for proper integration into the attention\n",
    "        # mechanism of the model\n",
    "        self.vocab = string.printable\n",
    "\n",
    "        # I am adding 1 to all character enumerations so that 0 is reserved for padding only and can be ignored in the model\n",
    "        self.char2idx = {u: i+1 for i, u in enumerate(self.vocab)}\n",
    "        self.idx2char = {i+1: u for i, u in enumerate(self.vocab)}\n",
    "\n",
    "        self.invert_one_hot = lambda x: tf.argmax(x, -1).numpy()\n",
    "\n",
    "        self.text_to_int = lambda x: np.array([self.char2idx[c] for c in x])\n",
    "        self.int_to_text = lambda x: ''.join(np.array([self.idx2char[i] for i in x]))\n",
    "\n",
    "        # Combine\n",
    "        self.list_ds = tf.data.Dataset.zip((self.list_ds_strokes, self.list_ds_ascii))\n",
    "        \n",
    "        # Create a datbase of tuples (strokes, matching ascii)\n",
    "        self.labeled_ds = self.list_ds.map(lambda x, y: tf.py_function(self.process_stroke, (x, y), (tf.float32, tf.float32)))\n",
    "        \n",
    "        self.cached_example_dataset = self.labeled_ds.shuffle(buffer_size=1024).cache().take(1024)\n",
    "        \n",
    "    # Create a dataset of strokes and matching lines - As mentioned before, each line is a training sample in this version\n",
    "    def process_stroke(self, file_path_stroke, file_path_lines):\n",
    "        line_num = int(file_path_stroke.numpy()[-6:-4])\n",
    "        strokes = self.get_strokes(file_path_stroke.numpy())\n",
    "        # Not sure the best way to combine two files\n",
    "        lines = self.get_ascii(file_path_lines)\n",
    "\n",
    "        U = lines[line_num-1]\n",
    "        U = U[:self.MAX_CHAR_SEQ_LEN]\n",
    "        U_conv = tf.keras.backend.one_hot(self.text_to_int(U), len(self.vocab)+1)\n",
    "\n",
    "        return strokes, U_conv\n",
    "\n",
    "    # Returns only the strokes of the dataset as a tuple with a label of the same data 1 timestamp ahead\n",
    "    @property\n",
    "    def batched_set(self):\n",
    "        # We don't care about the line data for this version, so remove that first\n",
    "\n",
    "        stroke_only_ds=self.labeled_ds.map(lambda x, y: x)\n",
    "        \n",
    "        # All sequences will be strictly right padded so that tensorflow will run them on a GPU\n",
    "        batched_dataset = stroke_only_ds.padded_batch(self.batch_size, padded_shapes=([self.MAX_STROKE_LEN, 3]), \n",
    "                                                      drop_remainder=True, padding_values=self.padding_value)\n",
    "\n",
    "        return batched_dataset.map(self.dense_1_step).cache()\n",
    "        return batched_dataset.cache()\n",
    "\n",
    "    # Returns only the strokes of the dataset as a tuple with a label of the same data 1 timestamp ahead\n",
    "    # This one also returns the character sequence U being written as a one-hot-encoded tensor\n",
    "    @property\n",
    "    def batched_onehot_set(self):\n",
    "        batched_dataset_one_hot = self.labeled_ds.padded_batch(\n",
    "            self.batch_size, padded_shapes=([self.MAX_STROKE_LEN, 3], \n",
    "                                            [self.MAX_CHAR_SEQ_LEN, len(self.vocab)+1]), \n",
    "                                            drop_remainder=False, padding_values=(self.padding_value, self.char_padding_value))        \n",
    "\n",
    "        return batched_dataset_one_hot.map(self.dense_1_step)\n",
    "\n",
    "    # We will make our prediction 1 step ahead\n",
    "    def dense_1_step(self, batch_stroke, batch_char_seq):\n",
    "        # Shift features and labels one step relative to each other.\n",
    "        return (batch_stroke[:, :, :], batch_char_seq ), batch_stroke[:, :, :]\n",
    "    \n",
    "    def get_examples(self, num_examples):\n",
    "        example_dataset = self.labeled_ds.shuffle(100).take(num_examples)\n",
    "        \n",
    "        #example_dataset = self.labeled_ds.batch(1).shuffle(100).take(num_examples)\n",
    "        \n",
    "        return example_dataset\n",
    "        \n",
    "    def get_strokes(self, fname):\n",
    "        root = ET.parse(fname).getroot()\n",
    "\n",
    "        # Parse one xml file\n",
    "        strokeset = root.find('StrokeSet')\n",
    "\n",
    "        x_samp = []\n",
    "\n",
    "        for stroke in strokeset.iter('Stroke'):\n",
    "            for child in stroke:\n",
    "                x_samp.append([float(child.attrib.get('x')), -1*float(child.attrib.get('y')), 0.])\n",
    "\n",
    "            # As in Graves, 2013, we add a binary vector indicating the end of a stroke\n",
    "            x_samp[-1][-1]=1.0\n",
    "\n",
    "        x_samp = np.asarray(x_samp)\n",
    "        x_samp = x_samp[:self.MAX_STROKE_LEN, :]\n",
    "\n",
    "        # We want the data as offsets though, not raw strokes - easier to train a network to predict small changes in the next timestamp\n",
    "        x_off = np.hstack(([x_samp[1:, :2]-x_samp[:-1, :2], x_samp[1:, 2:3]]))\n",
    "        x_off = np.vstack(([0, 0, 0], x_off))\n",
    "\n",
    "        x_off[:, 0] /= np.std(x_off[:, 0])\n",
    "        x_off[:, 1] /= np.std(x_off[:, 1])\n",
    "\n",
    "        return x_off\n",
    " \n",
    "    # Read an ascii file form the iamONDB and return all of the lines as strings\n",
    "    def get_ascii(self, fname):\n",
    "        text_file = open(fname.numpy(), \"r\")\n",
    "        lines = text_file.read()\n",
    "        lines = lines.split('CSR:')\n",
    "\n",
    "        return lines[1].strip().split('\\n')       \n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([\n",
    "            f'Writing Dataset for: {self.f_name}'\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing function for strokes\n",
    "# TODO: This should be in the writing class\n",
    "\n",
    "def plot_stroke(offsets, lines):\n",
    "    plt.figure(num=None, figsize=(15, 4))\n",
    "    strokes=np.array([np.cumsum(offsets[:,0]), np.cumsum(offsets[:,1]), offsets[:,2]]);    \n",
    "    stroke=[]\n",
    "\n",
    "    strokes[-1, -1] = 1\n",
    "\n",
    "    for x, y, eos in strokes.T:\n",
    "        stroke.append([x, y])\n",
    "        if eos > 0.1:\n",
    "            stroke=np.asarray(stroke);\n",
    "            #print(stroke.shape)\n",
    "            plt.plot(stroke[:,0], stroke[:,1], 'k')\n",
    "            stroke = []\n",
    "\n",
    "    clean_txt = lines\n",
    "\n",
    "    clean_txt = np.delete(clean_txt, np.argmax(clean_txt, -1) == 0.0, axis=0)\n",
    "\n",
    "    # TODO: This should be passed in\n",
    "    plt.title(train.int_to_text(train.invert_one_hot(clean_txt)))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1726 files\n"
     ]
    }
   ],
   "source": [
    "# I concatenated all data into one set as we just want the maximum amount of data to train the \n",
    "# network to write and don't really care about evaluation or test sets for this project\n",
    "train = WritingGenerator('../IamONDB/trainset_d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect our dataset.  It returns a tuple of stroke offsets and matching ascii strings\n",
    "# You can see that there isn't an exact match for every subject for strokes to ascii \n",
    "# because of the token limit in Transformers\n",
    "SUB = 10\n",
    "\n",
    "for s, l in train.batched_onehot_set.take(2).cache():\n",
    "    plot_stroke(s[0][SUB, :, :], s[1][SUB, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Much of the following code is from the excellent Tensorflow tutorial on Transformers: https://www.tensorflow.org/text/tutorials/transformer\n",
    "\n",
    "# STEP 1 - Positional Embeddings from the original paper. Although, you can also just add a randomized vector and I may try that next\n",
    "# TODO: Switch to a random vector and see if performance suffers vs this complex embedding.\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / jnp.power(10000, (2 * (i//2)) / jnp.float32(d_model))\n",
    "  return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(jnp.arange(position)[:, jnp.newaxis],\n",
    "                          jnp.arange(d_model)[jnp.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads = angle_rads.at[:, 0::2].set(jnp.sin(angle_rads[:, 0::2]))\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads = angle_rads.at[:, 1::2].set(jnp.cos(angle_rads[:, 1::2]))\n",
    "\n",
    "  pos_encoding = angle_rads[jnp.newaxis, ...]\n",
    "\n",
    "  return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up some pieces in haiku. See: https://github.com/deepmind/dm-haiku/tree/main/examples/transformer\n",
    "\n",
    "def layer_norm(x: jnp.ndarray) -> jnp.ndarray:\n",
    "  \"\"\"Applies a unique LayerNorm to x with default settings.\"\"\"\n",
    "  ln = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)\n",
    "  return ln(x)\n",
    "\n",
    "def point_wise_feed_forward(x: jnp.ndarray, d_model: int, dff: int) -> jnp.ndarray:\n",
    "  mlp = hk.Sequential([\n",
    "      hk.Linear(dff, name='Lin1'), jax.nn.relu, # (batch_size, seq_len, dff)\n",
    "      hk.Linear(d_model, name='Lin2'),          # (batch_size, seq_len, d_model)\n",
    "  ])\n",
    "  return mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out the point_wise_feed_forward network\n",
    "network = hk.transform(point_wise_feed_forward)\n",
    "params = network.init(rng=jax.random.PRNGKey(42), x=jnp.zeros((32, 100)), d_model=128, dff=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Encoder - Self attention over the input characters - The only mask needed is padded characters\n",
    "class Encoder_Layer(hk.Module):\n",
    "    # The Encoder Layer is one stack of the Encoder, putting the multihead together \n",
    "    # with the point wise network and some normalization layers\n",
    "    def __init__(self, key_size, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__(name='EncoderLayer')\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        # In haiku the key_size is specified manually instead of d_model/num_heads. Internally, it \n",
    "        # will project Q, K, and V to dimensions (*leading_dims, num_heads, head_size) before \n",
    "        # computing attention logits. After that you can futher modify it to project to d_model.\n",
    "        # TODO: Define an initializer here?\n",
    "        self.mha = hk.MultiHeadAttention(num_heads=num_heads, key_size=key_size, model_size=d_model, w_init_scale=1)\n",
    "\n",
    "    # I don't think haiku has any method for dealing with removing dropout automatically, so we will need\n",
    "    # to always pass in a training flag to remove it if necessary during inference\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "        mask,\n",
    "        training=True\n",
    "    ) -> jnp.ndarray:\n",
    "        # Need to format the mask properly across q_vals * k_vals\n",
    "        mask1 = mask[:, None, :, None] \n",
    "        mask2 = mask[:, None, None, :] \n",
    "        mask = mask1 & mask2  # [B, H=1, T, T]\n",
    "\n",
    "        attn_out = self.mha(x, x, x, mask)\n",
    "        if training:\n",
    "            attn_out = hk.dropout(hk.next_rng_key(), self.dropout_rate, attn_out)\n",
    "\n",
    "        # residual 1\n",
    "        attn_out = x + attn_out\n",
    "        attn_out1 = layer_norm(attn_out)\n",
    "\n",
    "        ffn_out = point_wise_feed_forward(attn_out1, self.d_model, self.dff)\n",
    "        if training:\n",
    "            ffn_out = hk.dropout(hk.next_rng_key(), self.dropout_rate, ffn_out)\n",
    "        \n",
    "        # residual 2\n",
    "        ffn_out = attn_out1 + ffn_out\n",
    "        attn_out2 = layer_norm(ffn_out)\n",
    "\n",
    "        return attn_out2\n",
    "\n",
    "# The Encoder module handles the pre-processing of the character data - embedding + positional encoding\n",
    "# and looping over the requested number of encoder attention layers\n",
    "class Encoder(hk.Module):\n",
    "    def __init__(self, num_layers, key_size, d_model, num_heads, dff, maximum_positional_encoding, \n",
    "        dropout_rate=0.1):\n",
    "        super().__init__(name='Encoder')\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.enc_layers = [Encoder_Layer(key_size, d_model, num_heads, dff, dropout_rate) \n",
    "                       for _ in range(num_layers)]\n",
    "\n",
    "        # Postional encodings - enocodings are static in this case and not learned parameters\n",
    "        # TODO: Compare this to random\n",
    "        self.positional_embeddings = positional_encoding(maximum_positional_encoding, d_model)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "        mask,\n",
    "        training=True\n",
    "    ) -> jnp.ndarray:\n",
    "        # The mask for the encoder needs to be broadcastable to the last 2 dimensions (1, 1, T, T)\n",
    "        # because the multihead attention is parallel - See https://www.tensorflow.org/text/tutorials/transformer\n",
    "        \n",
    "        seq_len = jnp.shape(x)[1]\n",
    "        \n",
    "        # We are using one-hot encoded characters and not embedded words, so we will just use a\n",
    "        # prenet to connect that to our model of depth d_model instead\n",
    "        x = hk.Linear(self.d_model, name='prenet')(x)\n",
    "\n",
    "        x = x + self.positional_embeddings[:, :seq_len, :]\n",
    "\n",
    "        if training:\n",
    "            x = hk.dropout(hk.next_rng_key(), self.dropout_rate, x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, mask, training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Encoder - x input will be [B, T, d_model] embedded characters with positional encoding added\n",
    "x=s[1].numpy()\n",
    "\n",
    "mask = jnp.not_equal(jnp.sum(x, -1), 0)\n",
    "\n",
    "def encoder(x: jnp.ndarray, mask: jnp.ndarray) -> jnp.ndarray:\n",
    "    enc = Encoder(4, 32, 128, 4, 128, 200)\n",
    "\n",
    "    return enc(x, mask)\n",
    "\n",
    "network = hk.transform(encoder)\n",
    "key = jax.random.PRNGKey(42) \n",
    "params = network.init(rng=key, x=jnp.ones((32, train.MAX_CHAR_SEQ_LEN, 101)), mask=mask)\n",
    "\n",
    "out = network.apply(params, key, x=x, mask=mask)\n",
    "\n",
    "#params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Decoder - Very similar to the Encoder with a self-attention mechanism, but there is a second cross-attention mechanism with\n",
    "# the output of the Encoder as K, V and the outputs of the self-attention mechanism as Q. That is, as the Decoder attemps to draw\n",
    "# hand written text based on the Encoder characters, it asks what parts of the encoding are important. Hopefully it learns this \n",
    "# relationship and we should see that reflected in the attention weights. It uses additional causal-masking to prevent future tokens\n",
    "# from being attended to as it attempts to predict the next token.\n",
    "\n",
    "class Decoder_Layer(hk.Module):\n",
    "    # The Decoder Layer is one stack of the Decoder, putting the 2 multihead attention blocks together \n",
    "    # with the point wise network and some normalization layers\n",
    "    def __init__(self, key_size, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__(name='DecoderLayer')\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.mha_self = hk.MultiHeadAttention(num_heads=num_heads, key_size=key_size, model_size=d_model, w_init_scale=1)\n",
    "        self.mha_cross = hk.MultiHeadAttention(num_heads=num_heads, key_size=key_size, model_size=d_model, w_init_scale=1)\n",
    "\n",
    "    # Mask here will only deal with the padding mask. We will compute the causal mask as needed in the calling function\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "        enc_output: jnp.ndarray,\n",
    "        mask,       # Mask of stroke padding\n",
    "        enc_mask,   # Mask of character padding\n",
    "        training=True\n",
    "    ) -> jnp.ndarray:\n",
    "        seq_len = jnp.shape(x)[1]\n",
    "\n",
    "        # Need to format the mask properly across q_vals * k_vals\n",
    "        mask1 = mask[:, None, :, None] \n",
    "        mask2 = mask[:, None, None, :] \n",
    "        mask2 = mask1 & mask2  # [B, H=1, T, T]\n",
    "\n",
    "        # Compute the causal mask and combine with the padding mask for the strokes\n",
    "        causal_mask = np.tril(np.ones((1, 1, seq_len, seq_len)))  # [B=1, H=1, T, T]\n",
    "        self_mask = mask2 * causal_mask  # [B, H=1, T, T]\n",
    "\n",
    "        # Self-attention\n",
    "        attn_out = self.mha_self(x, x, x, self_mask)\n",
    "        if training:\n",
    "            attn_out = hk.dropout(hk.next_rng_key(), self.dropout_rate, attn_out)\n",
    "\n",
    "        # residual 1\n",
    "        attn_out = x + attn_out\n",
    "        attn_out1 = layer_norm(attn_out)\n",
    "\n",
    "        # Cross-attention\n",
    "        # Combine the 2 padding masks. We don't need to attend to encodings that are padded or \n",
    "        # query decodings that are padded\n",
    "\n",
    "        # Need to format the mask properly across q_vals * k_vals\n",
    "        # TODO: This code is repeated a lot. This needs to be refactored.\n",
    "        mask1 = mask[:, None, :, None] \n",
    "        mask2 = enc_mask[:, None, None, :] \n",
    "        cross_mask = mask1 & mask2  # [B, H=1, T, T]\n",
    "        attn_out2 = self.mha_cross(attn_out1, enc_output, enc_output, cross_mask)\n",
    "        if training:\n",
    "            attn_out2 = hk.dropout(hk.next_rng_key(), self.dropout_rate, attn_out2)\n",
    "\n",
    "        # residual 2\n",
    "        attn_out2 = attn_out1 + attn_out2\n",
    "        attn_out2 = layer_norm(attn_out2)\n",
    "\n",
    "        attn_out3 = point_wise_feed_forward(attn_out2, self.d_model, self.dff)\n",
    "        if training:\n",
    "            attn_out3 = hk.dropout(hk.next_rng_key(), self.dropout_rate, attn_out3)\n",
    "        \n",
    "        # residual 3\n",
    "        attn_out3 = attn_out2 + attn_out3\n",
    "        out_all = layer_norm(attn_out3)\n",
    "\n",
    "        # TODO: It looks like the haiku transformer does not allow the return of the attent weights,\n",
    "        # only the final projection. I am going to fork my own repo and add that (maybe pull request as well)\n",
    "        return out_all\n",
    "\n",
    "def decoder_prenet(x: jnp.ndarray, d_model: int) -> jnp.ndarray:\n",
    "  mlp = hk.Sequential([\n",
    "      hk.Linear(d_model, name='D_Prenet1'), jax.nn.relu,    # (batch_size, seq_len, d_model)\n",
    "      hk.Linear(d_model, name='D_Prenet2'), jax.nn.relu,   # (batch_size, seq_len, d_model)\n",
    "      hk.Linear(d_model, name='D_Prenet3')                 # (batch_size, seq_len, d_model)\n",
    "  ])\n",
    "  return mlp(x)\n",
    "\n",
    "# The Decoder module handles the pre-processing of the stroke data - embedding + positional encoding\n",
    "# and looping over the requested number of decoder attention layers\n",
    "class Decoder(hk.Module):\n",
    "    def __init__(self, num_layers, key_size, d_model, num_heads, dff, maximum_positional_encoding, \n",
    "        dropout_rate=0.1):\n",
    "        super().__init__(name='Decoder')\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.dec_layers = [Decoder_Layer(key_size, d_model, num_heads, dff, dropout_rate) \n",
    "                       for _ in range(num_layers)]\n",
    "\n",
    "        # Postional encodings - enocodings are static in this case and not learned parameters\n",
    "        # TODO: Compare this to random\n",
    "        self.positional_embeddings = positional_encoding(maximum_positional_encoding, d_model)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        x: jnp.ndarray,\n",
    "        enc_output: jnp.ndarray,\n",
    "        enc_mask,\n",
    "        training=True\n",
    "    ) -> jnp.ndarray:\n",
    "        # The mask for the encoder needs to be broadcastable to the last 2 dimensions (1, 1, T, T)\n",
    "        # because the multihead attention is parallel - See https://www.tensorflow.org/text/tutorials/transformer\n",
    "        # We just check the pen up for a negative value to indicate a masked stroke (otherwise is should be \n",
    "        # 0 or 1)\n",
    "        # TODO: padding_value should be passed in or made global\n",
    "        mask = jnp.not_equal(x[:,:,2], train.padding_value)\n",
    "\n",
    "        seq_len = jnp.shape(x)[1]\n",
    "        \n",
    "        # Adding a small MLP here to give the network an opportunity to construct filters and non-linear relationships\n",
    "        # among the raw stroke data\n",
    "        x = decoder_prenet(x, self.d_model)\n",
    "\n",
    "        x = x + self.positional_embeddings[:, :seq_len, :]\n",
    "\n",
    "        if training:\n",
    "            x = hk.dropout(hk.next_rng_key(), self.dropout_rate, x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, enc_output, mask, enc_mask, training)\n",
    "\n",
    "        # TODO: Modify the mha from haiku to output attention_weights as well\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 200, 128)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Decoder - x input will be [B, T, d_model] embedded characters with positional encoding added\n",
    "x=s[1].numpy()\n",
    "\n",
    "mask = jnp.not_equal(jnp.sum(x, -1), 0)\n",
    "\n",
    "x=s[0].numpy()\n",
    "\n",
    "def decoder(x: jnp.ndarray, enc_output: jnp.ndarray, enc_mask: jnp.ndarray) -> jnp.ndarray:\n",
    "    dec = Decoder(4, 32, 128, 4, 128, 200)\n",
    "\n",
    "    return dec(x, enc_output, mask)\n",
    "\n",
    "network = hk.transform(decoder)\n",
    "key = jax.random.PRNGKey(42) \n",
    "params = network.init(rng=key, x=jnp.ones((32, train.MAX_STROKE_LEN, 3)), enc_output=out, enc_mask=mask)\n",
    "\n",
    "out2 = network.apply(params, key, x=x, enc_output=out, enc_mask=mask)\n",
    "\n",
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Writing Transformer\n",
    "\n",
    "# Output space - number of parameters in the mixture model\n",
    "NUM_MIX_COM = 20\n",
    "# weights + means (x + y) + std. devs. (x + y) + correlations + end_of_stroke\n",
    "# Unlike the Mixture Density Network notebook we are going to add cross correlation\n",
    "# terms to our loss and sampling functions for added complexity of the density \n",
    "# estimations\n",
    "NUM_PARAMS = NUM_MIX_COM + NUM_MIX_COM*2 + NUM_MIX_COM*2 + NUM_MIX_COM + 1\n",
    "\n",
    "class Writing_Transformer(hk.Module):\n",
    "    def __init__(self, num_layers, key_size, d_model, num_heads, dff, pe_encoding, pe_target, \n",
    "        dropout_rate=0.1):\n",
    "        super().__init__(name='Writing_Transformer')\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "        self.enc = Encoder(1, key_size, d_model, num_heads, dff, pe_encoding, dropout_rate)\n",
    "        self.dec = Decoder(num_layers, key_size, d_model, num_heads, dff, pe_target, dropout_rate)\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        inp: jnp.ndarray,\n",
    "        tar: jnp.ndarray,\n",
    "        training=True\n",
    "    ) -> jnp.ndarray:\n",
    "        enc_mask = jnp.not_equal(jnp.sum(inp, -1), 0)\n",
    "\n",
    "        # The Encoder\n",
    "        enc_output = self.enc(inp, enc_mask, training)\n",
    "\n",
    "        # The Decoder\n",
    "        dec_output = self.dec(tar, enc_output, enc_mask, training)\n",
    "\n",
    "        # The final layer to give us our logits\n",
    "        final_output = hk.Linear(NUM_PARAMS, name='final_layer')(dec_output)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 200, 121)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test The full network\n",
    "inp=s[1].numpy()\n",
    "tar=s[0].numpy()\n",
    "\n",
    "def writing_transformer(inp: jnp.ndarray, tar: jnp.ndarray) -> jnp.ndarray:\n",
    "    tra = Writing_Transformer(4, 32, 128, 4, 128, 200, 1000, 0.2)\n",
    "\n",
    "    return tra(inp, tar)\n",
    "\n",
    "network = hk.transform(writing_transformer)\n",
    "key = jax.random.PRNGKey(42)\n",
    "\n",
    "rng, init_rng = jax.random.split(key)\n",
    "params = network.init(rng=key, inp=inp, tar=tar)\n",
    "\n",
    "out3 = network.apply(params, key, inp=inp, tar=tar)\n",
    "\n",
    "out3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for the loss function\n",
    "# TODO: I think there are a lot of edge cases here that will result in NaNs when training.\n",
    "\n",
    "EPS = 0.000001\n",
    "\n",
    "@hk.transform\n",
    "def my_loss_fun_MDN(batch: tuple, training: bool) -> jnp.ndarray:\n",
    "    # Predict the next strokes\n",
    "\n",
    "    # We split the input and then use 1 sample ahead as the target (y_true)\n",
    "    inp = batch[0][:, :-1]\n",
    "    y_true = batch[0][:, 1:]\n",
    "\n",
    "    logits = writing_transformer(batch[1], inp, training)\n",
    "\n",
    "    pis, mu, sig, rho, eos = jnp.array_split(logits, [NUM_MIX_COM, NUM_MIX_COM*3, NUM_MIX_COM*5, NUM_MIX_COM*6], axis=-1)\n",
    "    \n",
    "    #print(eos.shape)\n",
    "\n",
    "    # weights - must be a probability distribution so softmax over all components\n",
    "    pis = jax.nn.softmax(pis)\n",
    "    \n",
    "    # means - no transformation needed\n",
    "    mu_x1, mu_x2 = jnp.array_split(mu, 2, axis=-1)\n",
    "    \n",
    "    # standard deviations - must be strictly positive so exponent\n",
    "    sig = jnp.exp(sig)\n",
    "    \n",
    "    sig = jnp.clip(sig, EPS, np.inf)\n",
    "    \n",
    "    sig_x1, sig_x2 = jnp.array_split(sig, 2, axis=-1)\n",
    "    \n",
    "    x1, x2, eos_true = jnp.array_split(y_true, 3, axis=-1)\n",
    "    \n",
    "    eos_true = jnp.squeeze(eos_true)\n",
    "        \n",
    "    # correlations - squish to -1 to 1 with tanh activation\n",
    "    rho = jnp.tanh(rho)\n",
    "    \n",
    "    rho = jnp.clip(rho, -1.+EPS, 1.-EPS)\n",
    "    \n",
    "    # Define Z as in Graves, 2013\n",
    "    Z = jnp.square( ( x1-mu_x1 ) / sig_x1 ) + jnp.square( ( x2-mu_x2 ) / sig_x2 ) - ( 2 * rho * (x1-mu_x1) * (x2-mu_x2) ) / ( sig_x1*sig_x2 )\n",
    "    \n",
    "    one_minus_rho_square = 1. - jnp.square(rho)\n",
    "    \n",
    "    # Now form Gaussian mixtures\n",
    "    term1 = jnp.divide(1., ( 2. * np.pi * sig_x1 * sig_x2 * jnp.sqrt( one_minus_rho_square ) ))\n",
    "    term2 = jnp.exp( jnp.divide ( (-1. * Z) , (2.*( one_minus_rho_square )) ))\n",
    "    \n",
    "    mix_loss = jnp.sum(pis * term1 * term2, axis=-1)       \n",
    "    \n",
    "    mix_loss = jnp.clip(mix_loss, EPS, np.inf)\n",
    "\n",
    "    # end of stroke loss\n",
    "    eos = jnp.squeeze(jax.nn.sigmoid(eos))\n",
    "    \n",
    "    eos = jnp.clip(eos, EPS, 1.-EPS)\n",
    "\n",
    "    eos_loss = jnp.where(jnp.equal(eos_true, 1.), eos, 1.-eos)\n",
    "    \n",
    "    # Only the valid parts of the sequence should count towards the loss.  The invalid parts are tagged with -2200\n",
    "    val_seq = jnp.squeeze(jnp.not_equal(eos_true, train.padding_value))\n",
    "\n",
    "    # This is the total loss for each element (batch * num_timepoints)\n",
    "    tot_loss = -(jnp.log(mix_loss) + jnp.log(eos_loss))\n",
    "\n",
    "    # The sequence loss is the sum of only the valid timepoints\n",
    "    \n",
    "    tot_loss = jnp.where(val_seq, tot_loss, 0.)   \n",
    "    \n",
    "    seq_tot = jnp.sum(val_seq, axis=-1, dtype=float)\n",
    "\n",
    "    tot_loss = jnp.sum(tot_loss, axis=-1) / seq_tot \n",
    "\n",
    "    return jnp.mean(tot_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Transformed' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [158], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test out the loss function\u001b[39;00m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mbatched_onehot_set\u001b[38;5;241m.\u001b[39mprefetch(tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mAUTOTUNE)\u001b[38;5;241m.\u001b[39mas_numpy_iterator()\n\u001b[0;32m----> 4\u001b[0m out4 \u001b[38;5;241m=\u001b[39m \u001b[43mmy_loss_fun_MDN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m out4\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Transformed' object is not callable"
     ]
    }
   ],
   "source": [
    "# Test out the loss function\n",
    "data = train.batched_onehot_set.prefetch(tf.data.experimental.AUTOTUNE).as_numpy_iterator()\n",
    "\n",
    "out4 = my_loss_fun_MDN(params, data.next()[0])\n",
    "\n",
    "out4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "num_layers = 4\n",
    "key_size = 64\n",
    "d_model = 256\n",
    "dff = 512\n",
    "num_heads = 4\n",
    "dropout_rate = 0.1\n",
    "\n",
    "SEED = 0\n",
    "\n",
    "EPOCHS = 500\n",
    "\n",
    "GRAD_CLIP_VALUE = 1\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "total_steps = EPOCHS*350 + EPOCHS\n",
    "warmup_cosine_decay_scheduler = optax.warmup_cosine_decay_schedule(init_value=0.00001, peak_value=0.0001,\n",
    "                                                                   warmup_steps=int(total_steps*0.2),\n",
    "                                                                   decay_steps=total_steps, end_value=0.000001)\n",
    "\n",
    "optimiser = optax.chain(\n",
    "      optax.clip_by_global_norm(GRAD_CLIP_VALUE),\n",
    "      optax.adam(LEARNING_RATE, b1=0.9, b2=0.99),\n",
    "  )\n",
    "\n",
    "def writing_transformer(inp: jnp.ndarray, tar: jnp.ndarray, training: bool) -> jnp.ndarray:\n",
    "    tra = Writing_Transformer(num_layers, key_size, d_model, num_heads, dff, pe_encoding=250, pe_target=1000, dropout_rate=dropout_rate)\n",
    "\n",
    "    return tra(inp, tar, training)\n",
    "\n",
    "#network = hk.transform(writing_transformer)\n",
    "\n",
    "@jax.jit\n",
    "def update(params: hk.Params, rng, opt_state: optax.OptState, batch: tuple):\n",
    "  rng, new_rng = jax.random.split(rng)\n",
    "  loss_and_grad_fn = jax.value_and_grad(my_loss_fun_MDN.apply)\n",
    "  #grad = jax.grad(my_loss_fun_MDN)(params, batch)\n",
    "  loss, gradients = loss_and_grad_fn(params, rng, batch, True)\n",
    "  updates, opt_state = optimiser.update(gradients, opt_state)\n",
    "  params = optax.apply_updates(params, updates)\n",
    "\n",
    "  return params, opt_state, loss, new_rng\n",
    "\n",
    "# TODO: make fetching the iterator more elegant and does the conversion from numpy to jax slow things down?\n",
    "#b = train.batched_onehot_set.prefetch(tf.data.experimental.AUTOTUNE).as_numpy_iterator()\n",
    "#key = jax.random.PRNGKey(42) \n",
    "#s = next(b)[0]\n",
    "\n",
    "data_iter = train.batched_onehot_set.prefetch(tf.data.experimental.AUTOTUNE).as_numpy_iterator()\n",
    "\n",
    "rng = jax.random.PRNGKey(SEED)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "params = my_loss_fun_MDN.init(rng=init_rng, batch=data_iter.next()[0], training=True)\n",
    "opt_state = optimiser.init(params)\n",
    "\n",
    "  # Average loss?\n",
    "  #data_iter = train.batched_onehot_set.prefetch(tf.data.experimental.AUTOTUNE).as_numpy_iterator()\n",
    "  #total_loss = 0\n",
    "  #for b, _ in data_iter:\n",
    "  #  total_loss = total_loss + my_loss_fun_MDN(params, b)\n",
    "\n",
    "  #print(\"   loss {:0.4f}\".format(total_loss*BATCH/20000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.00001\n",
    "\n",
    "optimiser = optax.chain(\n",
    "      optax.clip_by_global_norm(GRAD_CLIP_VALUE),\n",
    "      optax.adam(LEARNING_RATE, b1=0.9, b2=0.99),\n",
    "  )\n",
    "\n",
    "opt_state = optimiser.init(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Epoch': 0}\n",
      "Epoch 1 Batch 0 Loss 4.2691\n",
      "Epoch 1 Batch 50 Loss 1.8825\n",
      "Epoch 1 Batch 100 Loss 1.7124\n",
      "Epoch 1 Batch 150 Loss 1.4697\n",
      "Epoch 1 Batch 200 Loss 1.2281\n",
      "Epoch 1 Batch 250 Loss 1.0296\n",
      "Epoch 1 Batch 300 Loss 0.8880\n",
      "Epoch 1 Batch 350 Loss 0.7726\n",
      "Epoch 1 Loss 0.7114\n",
      "{'Epoch': 1}\n",
      "Epoch 2 Batch 0 Loss 0.5624\n",
      "Epoch 2 Batch 50 Loss 0.0760\n",
      "Epoch 2 Batch 100 Loss 0.0840\n",
      "Epoch 2 Batch 150 Loss 0.0696\n",
      "Epoch 2 Batch 200 Loss 0.0336\n",
      "Epoch 2 Batch 250 Loss -0.0063\n",
      "Epoch 2 Batch 300 Loss -0.0240\n",
      "Epoch 2 Batch 350 Loss -0.0449\n",
      "Epoch 2 Loss -0.0606\n",
      "{'Epoch': 2}\n",
      "Epoch 3 Batch 0 Loss 0.4247\n",
      "Epoch 3 Batch 50 Loss -0.1303\n",
      "Epoch 3 Batch 100 Loss -0.1084\n",
      "Epoch 3 Batch 150 Loss -0.1182\n",
      "Epoch 3 Batch 200 Loss -0.1603\n",
      "Epoch 3 Batch 250 Loss -0.1991\n",
      "Epoch 3 Batch 300 Loss -0.2147\n",
      "Epoch 3 Batch 350 Loss -0.2346\n",
      "Epoch 3 Loss -0.2506\n",
      "{'Epoch': 3}\n",
      "Epoch 4 Batch 0 Loss 0.3206\n",
      "Epoch 4 Batch 50 Loss -0.3008\n",
      "Epoch 4 Batch 100 Loss -0.2699\n",
      "Epoch 4 Batch 150 Loss -0.2745\n",
      "Epoch 4 Batch 200 Loss -0.3102\n",
      "Epoch 4 Batch 250 Loss -0.3466\n",
      "Epoch 4 Batch 300 Loss -0.3582\n",
      "Epoch 4 Batch 350 Loss -0.3725\n",
      "Epoch 4 Loss -0.3859\n",
      "{'Epoch': 4}\n",
      "Epoch 5 Batch 0 Loss 0.2746\n",
      "Epoch 5 Batch 50 Loss -0.4002\n",
      "Epoch 5 Batch 100 Loss -0.3542\n",
      "Epoch 5 Batch 150 Loss -0.3646\n",
      "Epoch 5 Batch 200 Loss -0.3993\n",
      "Epoch 5 Batch 250 Loss -0.4375\n",
      "Epoch 5 Batch 300 Loss -0.4461\n",
      "Epoch 5 Batch 350 Loss -0.4602\n",
      "Epoch 5 Loss -0.4722\n",
      "{'Epoch': 5}\n",
      "Epoch 6 Batch 0 Loss 0.2564\n",
      "Epoch 6 Batch 50 Loss -0.4611\n",
      "Epoch 6 Batch 100 Loss -0.4199\n",
      "Epoch 6 Batch 150 Loss -0.4219\n",
      "Epoch 6 Batch 200 Loss -0.4531\n",
      "Epoch 6 Batch 250 Loss -0.4902\n",
      "Epoch 6 Batch 300 Loss -0.4922\n",
      "Epoch 6 Batch 350 Loss -0.5075\n",
      "Epoch 6 Loss -0.5197\n",
      "{'Epoch': 6}\n",
      "Epoch 7 Batch 0 Loss 0.2422\n",
      "Epoch 7 Batch 50 Loss -0.4858\n",
      "Epoch 7 Batch 100 Loss -0.4405\n",
      "Epoch 7 Batch 150 Loss -0.4489\n",
      "Epoch 7 Batch 200 Loss -0.4890\n",
      "Epoch 7 Batch 250 Loss -0.5274\n",
      "Epoch 7 Batch 300 Loss -0.5289\n",
      "Epoch 7 Batch 350 Loss -0.5445\n",
      "Epoch 7 Loss -0.5577\n",
      "{'Epoch': 7}\n",
      "Epoch 8 Batch 0 Loss 0.1838\n",
      "Epoch 8 Batch 50 Loss -0.5407\n",
      "Epoch 8 Batch 100 Loss -0.4889\n",
      "Epoch 8 Batch 150 Loss -0.4940\n",
      "Epoch 8 Batch 200 Loss -0.5334\n",
      "Epoch 8 Batch 250 Loss -0.5713\n",
      "Epoch 8 Batch 300 Loss -0.5772\n",
      "Epoch 8 Batch 350 Loss -0.5911\n",
      "Epoch 8 Loss -0.6029\n",
      "{'Epoch': 8}\n",
      "Epoch 9 Batch 0 Loss 0.1526\n",
      "Epoch 9 Batch 50 Loss -0.5779\n",
      "Epoch 9 Batch 100 Loss -0.5226\n",
      "Epoch 9 Batch 150 Loss -0.5223\n",
      "Epoch 9 Batch 200 Loss -0.5601\n",
      "Epoch 9 Batch 250 Loss -0.5986\n",
      "Epoch 9 Batch 300 Loss -0.6028\n",
      "Epoch 9 Batch 350 Loss -0.6172\n",
      "Epoch 9 Loss -0.6296\n",
      "{'Epoch': 9}\n",
      "Epoch 10 Batch 0 Loss 0.1165\n",
      "Epoch 10 Batch 50 Loss -0.6026\n",
      "Epoch 10 Batch 100 Loss -0.5507\n",
      "Epoch 10 Batch 150 Loss -0.5561\n",
      "Epoch 10 Batch 200 Loss -0.5956\n",
      "Epoch 10 Batch 250 Loss -0.6293\n",
      "Epoch 10 Batch 300 Loss -0.6285\n",
      "Epoch 10 Batch 350 Loss -0.6433\n",
      "Epoch 10 Loss -0.6548\n",
      "{'Epoch': 10}\n",
      "Epoch 11 Batch 0 Loss 0.0559\n",
      "Epoch 11 Batch 50 Loss -0.6269\n",
      "Epoch 11 Batch 100 Loss -0.5685\n",
      "Epoch 11 Batch 150 Loss -0.5793\n",
      "Epoch 11 Batch 200 Loss -0.6154\n",
      "Epoch 11 Batch 250 Loss -0.6477\n",
      "Epoch 11 Batch 300 Loss -0.6523\n",
      "Epoch 11 Batch 350 Loss -0.6651\n",
      "Epoch 11 Loss -0.6768\n",
      "{'Epoch': 11}\n",
      "Epoch 12 Batch 0 Loss 0.0578\n",
      "Epoch 12 Batch 50 Loss -0.6486\n",
      "Epoch 12 Batch 100 Loss -0.5916\n",
      "Epoch 12 Batch 150 Loss -0.6023\n",
      "Epoch 12 Batch 200 Loss -0.6446\n",
      "Epoch 12 Batch 250 Loss -0.6783\n",
      "Epoch 12 Batch 300 Loss -0.6811\n",
      "Epoch 12 Batch 350 Loss -0.6947\n",
      "Epoch 12 Loss -0.7048\n",
      "{'Epoch': 12}\n",
      "Epoch 13 Batch 0 Loss 0.0859\n",
      "Epoch 13 Batch 50 Loss -0.6505\n",
      "Epoch 13 Batch 100 Loss -0.6078\n",
      "Epoch 13 Batch 150 Loss -0.6178\n",
      "Epoch 13 Batch 200 Loss -0.6593\n",
      "Epoch 13 Batch 250 Loss -0.6916\n",
      "Epoch 13 Batch 300 Loss -0.6986\n",
      "Epoch 13 Batch 350 Loss -0.7121\n",
      "Epoch 13 Loss -0.7226\n",
      "{'Epoch': 13}\n",
      "Epoch 14 Batch 0 Loss 0.0373\n",
      "Epoch 14 Batch 50 Loss -0.6841\n",
      "Epoch 14 Batch 100 Loss -0.6244\n",
      "Epoch 14 Batch 150 Loss -0.6380\n",
      "Epoch 14 Batch 200 Loss -0.6798\n",
      "Epoch 14 Batch 250 Loss -0.7167\n",
      "Epoch 14 Batch 300 Loss -0.7217\n",
      "Epoch 14 Batch 350 Loss -0.7347\n",
      "Epoch 14 Loss -0.7455\n",
      "{'Epoch': 14}\n",
      "Epoch 15 Batch 0 Loss 0.0120\n",
      "Epoch 15 Batch 50 Loss -0.6979\n",
      "Epoch 15 Batch 100 Loss -0.6464\n",
      "Epoch 15 Batch 150 Loss -0.6587\n",
      "Epoch 15 Batch 200 Loss -0.6980\n",
      "Epoch 15 Batch 250 Loss -0.7350\n",
      "Epoch 15 Batch 300 Loss -0.7408\n",
      "Epoch 15 Batch 350 Loss -0.7537\n",
      "Epoch 15 Loss -0.7642\n",
      "{'Epoch': 15}\n",
      "Epoch 16 Batch 0 Loss -0.0338\n",
      "Epoch 16 Batch 50 Loss -0.7172\n",
      "Epoch 16 Batch 100 Loss -0.6622\n",
      "Epoch 16 Batch 150 Loss -0.6746\n",
      "Epoch 16 Batch 200 Loss -0.7156\n",
      "Epoch 16 Batch 250 Loss -0.7484\n",
      "Epoch 16 Batch 300 Loss -0.7549\n",
      "Epoch 16 Batch 350 Loss -0.7684\n",
      "Epoch 16 Loss -0.7790\n",
      "{'Epoch': 16}\n",
      "Epoch 17 Batch 0 Loss -0.0509\n",
      "Epoch 17 Batch 50 Loss -0.7360\n",
      "Epoch 17 Batch 100 Loss -0.6771\n",
      "Epoch 17 Batch 150 Loss -0.6887\n",
      "Epoch 17 Batch 200 Loss -0.7303\n",
      "Epoch 17 Batch 250 Loss -0.7692\n",
      "Epoch 17 Batch 300 Loss -0.7737\n",
      "Epoch 17 Batch 350 Loss -0.7868\n",
      "Epoch 17 Loss -0.7968\n",
      "{'Epoch': 17}\n",
      "Epoch 18 Batch 0 Loss -0.0633\n",
      "Epoch 18 Batch 50 Loss -0.7522\n",
      "Epoch 18 Batch 100 Loss -0.6946\n",
      "Epoch 18 Batch 150 Loss -0.7086\n",
      "Epoch 18 Batch 200 Loss -0.7495\n",
      "Epoch 18 Batch 250 Loss -0.7896\n",
      "Epoch 18 Batch 300 Loss -0.7953\n",
      "Epoch 18 Batch 350 Loss -0.8056\n",
      "Epoch 18 Loss -0.8158\n",
      "{'Epoch': 18}\n",
      "Epoch 19 Batch 0 Loss -0.0955\n",
      "Epoch 19 Batch 50 Loss -0.7614\n",
      "Epoch 19 Batch 100 Loss -0.7127\n",
      "Epoch 19 Batch 150 Loss -0.7228\n",
      "Epoch 19 Batch 200 Loss -0.7633\n",
      "Epoch 19 Batch 250 Loss -0.7990\n",
      "Epoch 19 Batch 300 Loss -0.8031\n",
      "Epoch 19 Batch 350 Loss -0.8157\n",
      "Epoch 19 Loss -0.8250\n",
      "{'Epoch': 19}\n",
      "Epoch 20 Batch 0 Loss -0.1146\n",
      "Epoch 20 Batch 50 Loss -0.7769\n",
      "Epoch 20 Batch 100 Loss -0.7197\n",
      "Epoch 20 Batch 150 Loss -0.7314\n",
      "Epoch 20 Batch 200 Loss -0.7734\n",
      "Epoch 20 Batch 250 Loss -0.8117\n",
      "Epoch 20 Batch 300 Loss -0.8182\n",
      "Epoch 20 Batch 350 Loss -0.8320\n",
      "Epoch 20 Loss -0.8412\n",
      "{'Epoch': 20}\n",
      "Epoch 21 Batch 0 Loss -0.1192\n",
      "Epoch 21 Batch 50 Loss -0.7898\n",
      "Epoch 21 Batch 100 Loss -0.7403\n",
      "Epoch 21 Batch 150 Loss -0.7521\n",
      "Epoch 21 Batch 200 Loss -0.7946\n",
      "Epoch 21 Batch 250 Loss -0.8326\n",
      "Epoch 21 Batch 300 Loss -0.8386\n",
      "Epoch 21 Batch 350 Loss -0.8496\n",
      "Epoch 21 Loss -0.8599\n",
      "{'Epoch': 21}\n",
      "Epoch 22 Batch 0 Loss -0.1430\n",
      "Epoch 22 Batch 50 Loss -0.7991\n",
      "Epoch 22 Batch 100 Loss -0.7507\n",
      "Epoch 22 Batch 150 Loss -0.7635\n",
      "Epoch 22 Batch 200 Loss -0.8039\n",
      "Epoch 22 Batch 250 Loss -0.8408\n",
      "Epoch 22 Batch 300 Loss -0.8466\n",
      "Epoch 22 Batch 350 Loss -0.8598\n",
      "Epoch 22 Loss -0.8694\n",
      "{'Epoch': 22}\n",
      "Epoch 23 Batch 0 Loss -0.1614\n",
      "Epoch 23 Batch 50 Loss -0.8119\n",
      "Epoch 23 Batch 100 Loss -0.7616\n",
      "Epoch 23 Batch 150 Loss -0.7748\n",
      "Epoch 23 Batch 200 Loss -0.8176\n",
      "Epoch 23 Batch 250 Loss -0.8525\n",
      "Epoch 23 Batch 300 Loss -0.8582\n",
      "Epoch 23 Batch 350 Loss -0.8717\n",
      "Epoch 23 Loss -0.8810\n",
      "{'Epoch': 23}\n",
      "Epoch 24 Batch 0 Loss -0.1814\n",
      "Epoch 24 Batch 50 Loss -0.8320\n",
      "Epoch 24 Batch 100 Loss -0.7766\n",
      "Epoch 24 Batch 150 Loss -0.7897\n",
      "Epoch 24 Batch 200 Loss -0.8325\n",
      "Epoch 24 Batch 250 Loss -0.8717\n",
      "Epoch 24 Batch 300 Loss -0.8773\n",
      "Epoch 24 Batch 350 Loss -0.8896\n",
      "Epoch 24 Loss -0.8987\n",
      "{'Epoch': 24}\n",
      "Epoch 25 Batch 0 Loss -0.1965\n",
      "Epoch 25 Batch 50 Loss -0.8411\n",
      "Epoch 25 Batch 100 Loss -0.7843\n",
      "Epoch 25 Batch 150 Loss -0.7967\n",
      "Epoch 25 Batch 200 Loss -0.8406\n",
      "Epoch 25 Batch 250 Loss -0.8801\n",
      "Epoch 25 Batch 300 Loss -0.8863\n",
      "Epoch 25 Batch 350 Loss -0.8986\n",
      "Epoch 25 Loss -0.9086\n",
      "{'Epoch': 25}\n",
      "Epoch 26 Batch 0 Loss -0.2062\n",
      "Epoch 26 Batch 50 Loss -0.8564\n",
      "Epoch 26 Batch 100 Loss -0.8011\n",
      "Epoch 26 Batch 150 Loss -0.8124\n",
      "Epoch 26 Batch 200 Loss -0.8544\n",
      "Epoch 26 Batch 250 Loss -0.8918\n",
      "Epoch 26 Batch 300 Loss -0.8960\n",
      "Epoch 26 Batch 350 Loss -0.9083\n",
      "Epoch 26 Loss -0.9188\n",
      "{'Epoch': 26}\n",
      "Epoch 27 Batch 0 Loss -0.2319\n",
      "Epoch 27 Batch 50 Loss -0.8640\n",
      "Epoch 27 Batch 100 Loss -0.8083\n",
      "Epoch 27 Batch 150 Loss -0.8213\n",
      "Epoch 27 Batch 200 Loss -0.8642\n",
      "Epoch 27 Batch 250 Loss -0.9031\n",
      "Epoch 27 Batch 300 Loss -0.9093\n",
      "Epoch 27 Batch 350 Loss -0.9205\n",
      "Epoch 27 Loss -0.9307\n",
      "{'Epoch': 27}\n",
      "Epoch 28 Batch 0 Loss -0.2498\n",
      "Epoch 28 Batch 50 Loss -0.8673\n",
      "Epoch 28 Batch 100 Loss -0.8183\n",
      "Epoch 28 Batch 150 Loss -0.8314\n",
      "Epoch 28 Batch 200 Loss -0.8751\n",
      "Epoch 28 Batch 250 Loss -0.9142\n",
      "Epoch 28 Batch 300 Loss -0.9203\n",
      "Epoch 28 Batch 350 Loss -0.9311\n",
      "Epoch 28 Loss -0.9408\n",
      "{'Epoch': 28}\n",
      "Epoch 29 Batch 0 Loss -0.2450\n",
      "Epoch 29 Batch 50 Loss -0.8870\n",
      "Epoch 29 Batch 100 Loss -0.8264\n",
      "Epoch 29 Batch 150 Loss -0.8382\n",
      "Epoch 29 Batch 200 Loss -0.8823\n",
      "Epoch 29 Batch 250 Loss -0.9217\n",
      "Epoch 29 Batch 300 Loss -0.9267\n",
      "Epoch 29 Batch 350 Loss -0.9387\n",
      "Epoch 29 Loss -0.9490\n",
      "{'Epoch': 29}\n",
      "Epoch 30 Batch 0 Loss -0.2669\n",
      "Epoch 30 Batch 50 Loss -0.8983\n",
      "Epoch 30 Batch 100 Loss -0.8426\n",
      "Epoch 30 Batch 150 Loss -0.8548\n",
      "Epoch 30 Batch 200 Loss -0.8988\n",
      "Epoch 30 Batch 250 Loss -0.9347\n",
      "Epoch 30 Batch 300 Loss -0.9379\n",
      "Epoch 30 Batch 350 Loss -0.9488\n",
      "Epoch 30 Loss -0.9589\n",
      "{'Epoch': 30}\n",
      "Epoch 31 Batch 0 Loss -0.2776\n",
      "Epoch 31 Batch 50 Loss -0.9032\n",
      "Epoch 31 Batch 100 Loss -0.8470\n",
      "Epoch 31 Batch 150 Loss -0.8612\n",
      "Epoch 31 Batch 200 Loss -0.9058\n",
      "Epoch 31 Batch 250 Loss -0.9413\n",
      "Epoch 31 Batch 300 Loss -0.9467\n",
      "Epoch 31 Batch 350 Loss -0.9591\n",
      "Epoch 31 Loss -0.9695\n",
      "{'Epoch': 31}\n",
      "Epoch 32 Batch 0 Loss -0.2815\n",
      "Epoch 32 Batch 50 Loss -0.9150\n",
      "Epoch 32 Batch 100 Loss -0.8593\n",
      "Epoch 32 Batch 150 Loss -0.8719\n",
      "Epoch 32 Batch 200 Loss -0.9152\n",
      "Epoch 32 Batch 250 Loss -0.9559\n",
      "Epoch 32 Batch 300 Loss -0.9616\n",
      "Epoch 32 Batch 350 Loss -0.9716\n",
      "Epoch 32 Loss -0.9813\n",
      "{'Epoch': 32}\n",
      "Epoch 33 Batch 0 Loss -0.2755\n",
      "Epoch 33 Batch 50 Loss -0.9178\n",
      "Epoch 33 Batch 100 Loss -0.8663\n",
      "Epoch 33 Batch 150 Loss -0.8797\n",
      "Epoch 33 Batch 200 Loss -0.9239\n",
      "Epoch 33 Batch 250 Loss -0.9632\n",
      "Epoch 33 Batch 300 Loss -0.9693\n",
      "Epoch 33 Batch 350 Loss -0.9798\n",
      "Epoch 33 Loss -0.9894\n",
      "{'Epoch': 33}\n",
      "Epoch 34 Batch 0 Loss -0.2955\n",
      "Epoch 34 Batch 50 Loss -0.9341\n",
      "Epoch 34 Batch 100 Loss -0.8763\n",
      "Epoch 34 Batch 150 Loss -0.8876\n",
      "Epoch 34 Batch 200 Loss -0.9284\n",
      "Epoch 34 Batch 250 Loss -0.9663\n",
      "Epoch 34 Batch 300 Loss -0.9738\n",
      "Epoch 34 Batch 350 Loss -0.9854\n",
      "Epoch 34 Loss -0.9958\n",
      "{'Epoch': 34}\n",
      "Epoch 35 Batch 0 Loss -0.3012\n",
      "Epoch 35 Batch 50 Loss -0.9356\n",
      "Epoch 35 Batch 100 Loss -0.8838\n",
      "Epoch 35 Batch 150 Loss -0.8971\n",
      "Epoch 35 Batch 200 Loss -0.9430\n",
      "Epoch 35 Batch 250 Loss -0.9831\n",
      "Epoch 35 Batch 300 Loss -0.9898\n",
      "Epoch 35 Batch 350 Loss -0.9986\n",
      "Epoch 35 Loss -1.0076\n",
      "{'Epoch': 35}\n",
      "Epoch 36 Batch 0 Loss -0.3094\n",
      "Epoch 36 Batch 50 Loss -0.9454\n",
      "Epoch 36 Batch 100 Loss -0.8927\n",
      "Epoch 36 Batch 150 Loss -0.9043\n",
      "Epoch 36 Batch 200 Loss -0.9472\n",
      "Epoch 36 Batch 250 Loss -0.9864\n",
      "Epoch 36 Batch 300 Loss -0.9923\n",
      "Epoch 36 Batch 350 Loss -1.0028\n",
      "Epoch 36 Loss -1.0126\n",
      "{'Epoch': 36}\n",
      "Epoch 37 Batch 0 Loss -0.3141\n",
      "Epoch 37 Batch 50 Loss -0.9540\n",
      "Epoch 37 Batch 100 Loss -0.8976\n",
      "Epoch 37 Batch 150 Loss -0.9124\n",
      "Epoch 37 Batch 200 Loss -0.9567\n",
      "Epoch 37 Batch 250 Loss -0.9966\n",
      "Epoch 37 Batch 300 Loss -1.0017\n",
      "Epoch 37 Batch 350 Loss -1.0135\n",
      "Epoch 37 Loss -1.0233\n",
      "{'Epoch': 37}\n",
      "Epoch 38 Batch 0 Loss -0.3335\n",
      "Epoch 38 Batch 50 Loss -0.9525\n",
      "Epoch 38 Batch 100 Loss -0.9035\n",
      "Epoch 38 Batch 150 Loss -0.9154\n",
      "Epoch 38 Batch 200 Loss -0.9625\n",
      "Epoch 38 Batch 250 Loss -1.0031\n",
      "Epoch 38 Batch 300 Loss -1.0082\n",
      "Epoch 38 Batch 350 Loss -1.0196\n",
      "Epoch 38 Loss -1.0295\n",
      "{'Epoch': 38}\n",
      "Epoch 39 Batch 0 Loss -0.3477\n",
      "Epoch 39 Batch 50 Loss -0.9617\n",
      "Epoch 39 Batch 100 Loss -0.9087\n",
      "Epoch 39 Batch 150 Loss -0.9242\n",
      "Epoch 39 Batch 200 Loss -0.9694\n",
      "Epoch 39 Batch 250 Loss -1.0082\n",
      "Epoch 39 Batch 300 Loss -1.0143\n",
      "Epoch 39 Batch 350 Loss -1.0259\n",
      "Epoch 39 Loss -1.0360\n",
      "{'Epoch': 39}\n",
      "Epoch 40 Batch 0 Loss -0.3382\n",
      "Epoch 40 Batch 50 Loss -0.9754\n",
      "Epoch 40 Batch 100 Loss -0.9234\n",
      "Epoch 40 Batch 150 Loss -0.9359\n",
      "Epoch 40 Batch 200 Loss -0.9813\n",
      "Epoch 40 Batch 250 Loss -1.0227\n",
      "Epoch 40 Batch 300 Loss -1.0303\n",
      "Epoch 40 Batch 350 Loss -1.0417\n",
      "Epoch 40 Loss -1.0506\n",
      "{'Epoch': 40}\n",
      "Epoch 41 Batch 0 Loss -0.3485\n",
      "Epoch 41 Batch 50 Loss -0.9961\n",
      "Epoch 41 Batch 100 Loss -0.9340\n",
      "Epoch 41 Batch 150 Loss -0.9442\n",
      "Epoch 41 Batch 200 Loss -0.9872\n",
      "Epoch 41 Batch 250 Loss -1.0278\n",
      "Epoch 41 Batch 300 Loss -1.0339\n",
      "Epoch 41 Batch 350 Loss -1.0432\n",
      "Epoch 41 Loss -1.0518\n",
      "{'Epoch': 41}\n",
      "Epoch 42 Batch 0 Loss -0.3669\n",
      "Epoch 42 Batch 50 Loss -0.9842\n",
      "Epoch 42 Batch 100 Loss -0.9312\n",
      "Epoch 42 Batch 150 Loss -0.9465\n",
      "Epoch 42 Batch 200 Loss -0.9898\n",
      "Epoch 42 Batch 250 Loss -1.0291\n",
      "Epoch 42 Batch 300 Loss -1.0347\n",
      "Epoch 42 Batch 350 Loss -1.0447\n",
      "Epoch 42 Loss -1.0544\n",
      "{'Epoch': 42}\n",
      "Epoch 43 Batch 0 Loss -0.3722\n",
      "Epoch 43 Batch 50 Loss -0.9879\n",
      "Epoch 43 Batch 100 Loss -0.9339\n",
      "Epoch 43 Batch 150 Loss -0.9489\n",
      "Epoch 43 Batch 200 Loss -0.9946\n",
      "Epoch 43 Batch 250 Loss -1.0372\n",
      "Epoch 43 Batch 300 Loss -1.0439\n",
      "Epoch 43 Batch 350 Loss -1.0534\n",
      "Epoch 43 Loss -1.0612\n",
      "{'Epoch': 43}\n",
      "Epoch 44 Batch 0 Loss -0.3523\n",
      "Epoch 44 Batch 50 Loss -0.9927\n",
      "Epoch 44 Batch 100 Loss -0.9441\n",
      "Epoch 44 Batch 150 Loss -0.9552\n",
      "Epoch 44 Batch 200 Loss -0.9996\n",
      "Epoch 44 Batch 250 Loss -1.0397\n",
      "Epoch 44 Batch 300 Loss -1.0445\n",
      "Epoch 44 Batch 350 Loss -1.0563\n",
      "Epoch 44 Loss -1.0647\n",
      "{'Epoch': 44}\n",
      "Epoch 45 Batch 0 Loss -0.4117\n",
      "Epoch 45 Batch 50 Loss -1.0099\n",
      "Epoch 45 Batch 100 Loss -0.9526\n",
      "Epoch 45 Batch 150 Loss -0.9630\n",
      "Epoch 45 Batch 200 Loss -1.0065\n",
      "Epoch 45 Batch 250 Loss -1.0465\n",
      "Epoch 45 Batch 300 Loss -1.0535\n",
      "Epoch 45 Batch 350 Loss -1.0615\n",
      "Epoch 45 Loss -1.0703\n",
      "{'Epoch': 45}\n",
      "Epoch 46 Batch 0 Loss -0.3951\n",
      "Epoch 46 Batch 50 Loss -1.0221\n",
      "Epoch 46 Batch 100 Loss -0.9593\n",
      "Epoch 46 Batch 150 Loss -0.9712\n",
      "Epoch 46 Batch 200 Loss -1.0146\n",
      "Epoch 46 Batch 250 Loss -1.0548\n",
      "Epoch 46 Batch 300 Loss -1.0607\n",
      "Epoch 46 Batch 350 Loss -1.0720\n",
      "Epoch 46 Loss -1.0805\n",
      "{'Epoch': 46}\n",
      "Epoch 47 Batch 0 Loss -0.4155\n",
      "Epoch 47 Batch 50 Loss -1.0153\n",
      "Epoch 47 Batch 100 Loss -0.9647\n",
      "Epoch 47 Batch 150 Loss -0.9765\n",
      "Epoch 47 Batch 200 Loss -1.0199\n",
      "Epoch 47 Batch 250 Loss -1.0607\n",
      "Epoch 47 Batch 300 Loss -1.0670\n",
      "Epoch 47 Batch 350 Loss -1.0757\n",
      "Epoch 47 Loss -1.0849\n",
      "{'Epoch': 47}\n",
      "Epoch 48 Batch 0 Loss -0.4079\n",
      "Epoch 48 Batch 50 Loss -1.0243\n",
      "Epoch 48 Batch 100 Loss -0.9691\n",
      "Epoch 48 Batch 150 Loss -0.9805\n",
      "Epoch 48 Batch 200 Loss -1.0241\n",
      "Epoch 48 Batch 250 Loss -1.0646\n",
      "Epoch 48 Batch 300 Loss -1.0716\n",
      "Epoch 48 Batch 350 Loss -1.0832\n",
      "Epoch 48 Loss -1.0914\n",
      "{'Epoch': 48}\n",
      "Epoch 49 Batch 0 Loss -0.4471\n",
      "Epoch 49 Batch 50 Loss -1.0371\n",
      "Epoch 49 Batch 100 Loss -0.9775\n",
      "Epoch 49 Batch 150 Loss -0.9883\n",
      "Epoch 49 Batch 200 Loss -1.0320\n",
      "Epoch 49 Batch 250 Loss -1.0726\n",
      "Epoch 49 Batch 300 Loss -1.0774\n",
      "Epoch 49 Batch 350 Loss -1.0883\n",
      "Epoch 49 Loss -1.0972\n",
      "{'Epoch': 49}\n",
      "Epoch 50 Batch 0 Loss -0.4213\n",
      "Epoch 50 Batch 50 Loss -1.0305\n",
      "Epoch 50 Batch 100 Loss -0.9746\n",
      "Epoch 50 Batch 150 Loss -0.9877\n",
      "Epoch 50 Batch 200 Loss -1.0315\n",
      "Epoch 50 Batch 250 Loss -1.0727\n",
      "Epoch 50 Batch 300 Loss -1.0781\n",
      "Epoch 50 Batch 350 Loss -1.0889\n",
      "Epoch 50 Loss -1.0976\n",
      "{'Epoch': 50}\n",
      "Epoch 51 Batch 0 Loss -0.4458\n",
      "Epoch 51 Batch 50 Loss -1.0421\n",
      "Epoch 51 Batch 100 Loss -0.9858\n",
      "Epoch 51 Batch 150 Loss -0.9982\n",
      "Epoch 51 Batch 200 Loss -1.0398\n",
      "Epoch 51 Batch 250 Loss -1.0789\n",
      "Epoch 51 Batch 300 Loss -1.0841\n",
      "Epoch 51 Batch 350 Loss -1.0956\n",
      "Epoch 51 Loss -1.1049\n",
      "{'Epoch': 51}\n",
      "Epoch 52 Batch 0 Loss -0.4440\n",
      "Epoch 52 Batch 50 Loss -1.0510\n",
      "Epoch 52 Batch 100 Loss -0.9945\n",
      "Epoch 52 Batch 150 Loss -1.0035\n",
      "Epoch 52 Batch 200 Loss -1.0475\n",
      "Epoch 52 Batch 250 Loss -1.0876\n",
      "Epoch 52 Batch 300 Loss -1.0907\n",
      "Epoch 52 Batch 350 Loss -1.1008\n",
      "Epoch 52 Loss -1.1098\n",
      "{'Epoch': 52}\n",
      "Epoch 53 Batch 0 Loss -0.4477\n",
      "Epoch 53 Batch 50 Loss -1.0507\n",
      "Epoch 53 Batch 100 Loss -0.9963\n",
      "Epoch 53 Batch 150 Loss -1.0072\n",
      "Epoch 53 Batch 200 Loss -1.0518\n",
      "Epoch 53 Batch 250 Loss -1.0935\n",
      "Epoch 53 Batch 300 Loss -1.1000\n",
      "Epoch 53 Batch 350 Loss -1.1094\n",
      "Epoch 53 Loss -1.1182\n",
      "{'Epoch': 53}\n",
      "Epoch 54 Batch 0 Loss -0.4383\n",
      "Epoch 54 Batch 50 Loss -1.0508\n",
      "Epoch 54 Batch 100 Loss -1.0019\n",
      "Epoch 54 Batch 150 Loss -1.0115\n",
      "Epoch 54 Batch 200 Loss -1.0547\n",
      "Epoch 54 Batch 250 Loss -1.0962\n",
      "Epoch 54 Batch 300 Loss -1.1025\n",
      "Epoch 54 Batch 350 Loss -1.1105\n",
      "Epoch 54 Loss -1.1194\n",
      "{'Epoch': 54}\n",
      "Epoch 55 Batch 0 Loss -0.4443\n",
      "Epoch 55 Batch 50 Loss -1.0533\n",
      "Epoch 55 Batch 100 Loss -1.0037\n",
      "Epoch 55 Batch 150 Loss -1.0154\n",
      "Epoch 55 Batch 200 Loss -1.0568\n",
      "Epoch 55 Batch 250 Loss -1.0976\n",
      "Epoch 55 Batch 300 Loss -1.0989\n",
      "Epoch 55 Batch 350 Loss -1.1094\n",
      "Epoch 55 Loss -1.1186\n",
      "{'Epoch': 55}\n",
      "Epoch 56 Batch 0 Loss -0.4654\n",
      "Epoch 56 Batch 50 Loss -1.0640\n",
      "Epoch 56 Batch 100 Loss -1.0126\n",
      "Epoch 56 Batch 150 Loss -1.0231\n",
      "Epoch 56 Batch 200 Loss -1.0663\n",
      "Epoch 56 Batch 250 Loss -1.1063\n",
      "Epoch 56 Batch 300 Loss -1.1098\n",
      "Epoch 56 Batch 350 Loss -1.1204\n",
      "Epoch 56 Loss -1.1290\n",
      "{'Epoch': 56}\n",
      "Epoch 57 Batch 0 Loss -0.4568\n",
      "Epoch 57 Batch 50 Loss -1.0684\n",
      "Epoch 57 Batch 100 Loss -1.0174\n",
      "Epoch 57 Batch 150 Loss -1.0255\n",
      "Epoch 57 Batch 200 Loss -1.0705\n",
      "Epoch 57 Batch 250 Loss -1.1100\n",
      "Epoch 57 Batch 300 Loss -1.1108\n",
      "Epoch 57 Batch 350 Loss -1.1201\n",
      "Epoch 57 Loss -1.1288\n",
      "{'Epoch': 57}\n",
      "Epoch 58 Batch 0 Loss -0.4512\n",
      "Epoch 58 Batch 50 Loss -1.0699\n",
      "Epoch 58 Batch 100 Loss -1.0206\n",
      "Epoch 58 Batch 150 Loss -1.0310\n",
      "Epoch 58 Batch 200 Loss -1.0745\n",
      "Epoch 58 Batch 250 Loss -1.1134\n",
      "Epoch 58 Batch 300 Loss -1.1160\n",
      "Epoch 58 Batch 350 Loss -1.1254\n",
      "Epoch 58 Loss -1.1341\n",
      "{'Epoch': 58}\n",
      "Epoch 59 Batch 0 Loss -0.4579\n",
      "Epoch 59 Batch 50 Loss -1.0723\n",
      "Epoch 59 Batch 100 Loss -1.0253\n",
      "Epoch 59 Batch 150 Loss -1.0347\n",
      "Epoch 59 Batch 200 Loss -1.0757\n",
      "Epoch 59 Batch 250 Loss -1.1159\n",
      "Epoch 59 Batch 300 Loss -1.1195\n",
      "Epoch 59 Batch 350 Loss -1.1307\n",
      "Epoch 59 Loss -1.1397\n",
      "{'Epoch': 59}\n",
      "Epoch 60 Batch 0 Loss -0.4880\n",
      "Epoch 60 Batch 50 Loss -1.0820\n",
      "Epoch 60 Batch 100 Loss -1.0296\n",
      "Epoch 60 Batch 150 Loss -1.0405\n",
      "Epoch 60 Batch 200 Loss -1.0837\n",
      "Epoch 60 Batch 250 Loss -1.1242\n",
      "Epoch 60 Batch 300 Loss -1.1281\n",
      "Epoch 60 Batch 350 Loss -1.1371\n",
      "Epoch 60 Loss -1.1464\n",
      "{'Epoch': 60}\n",
      "Epoch 61 Batch 0 Loss -0.4799\n",
      "Epoch 61 Batch 50 Loss -1.0909\n",
      "Epoch 61 Batch 100 Loss -1.0358\n",
      "Epoch 61 Batch 150 Loss -1.0453\n",
      "Epoch 61 Batch 200 Loss -1.0850\n",
      "Epoch 61 Batch 250 Loss -1.1238\n",
      "Epoch 61 Batch 300 Loss -1.1281\n",
      "Epoch 61 Batch 350 Loss -1.1378\n",
      "Epoch 61 Loss -1.1476\n",
      "{'Epoch': 61}\n",
      "Epoch 62 Batch 0 Loss -0.5047\n",
      "Epoch 62 Batch 50 Loss -1.0946\n",
      "Epoch 62 Batch 100 Loss -1.0403\n",
      "Epoch 62 Batch 150 Loss -1.0496\n",
      "Epoch 62 Batch 200 Loss -1.0913\n",
      "Epoch 62 Batch 250 Loss -1.1321\n",
      "Epoch 62 Batch 300 Loss -1.1349\n",
      "Epoch 62 Batch 350 Loss -1.1421\n",
      "Epoch 62 Loss -1.1511\n",
      "{'Epoch': 62}\n",
      "Epoch 63 Batch 0 Loss -0.4872\n",
      "Epoch 63 Batch 50 Loss -1.0910\n",
      "Epoch 63 Batch 100 Loss -1.0376\n",
      "Epoch 63 Batch 150 Loss -1.0469\n",
      "Epoch 63 Batch 200 Loss -1.0912\n",
      "Epoch 63 Batch 250 Loss -1.1329\n",
      "Epoch 63 Batch 300 Loss -1.1383\n",
      "Epoch 63 Batch 350 Loss -1.1469\n",
      "Epoch 63 Loss -1.1564\n",
      "{'Epoch': 63}\n",
      "Epoch 64 Batch 0 Loss -0.5042\n",
      "Epoch 64 Batch 50 Loss -1.0871\n",
      "Epoch 64 Batch 100 Loss -1.0382\n",
      "Epoch 64 Batch 150 Loss -1.0498\n",
      "Epoch 64 Batch 200 Loss -1.0932\n",
      "Epoch 64 Batch 250 Loss -1.1350\n",
      "Epoch 64 Batch 300 Loss -1.1423\n",
      "Epoch 64 Batch 350 Loss -1.1520\n",
      "Epoch 64 Loss -1.1613\n",
      "{'Epoch': 64}\n",
      "Epoch 65 Batch 0 Loss -0.5112\n",
      "Epoch 65 Batch 50 Loss -1.1023\n",
      "Epoch 65 Batch 100 Loss -1.0473\n",
      "Epoch 65 Batch 150 Loss -1.0587\n",
      "Epoch 65 Batch 200 Loss -1.1012\n",
      "Epoch 65 Batch 250 Loss -1.1429\n",
      "Epoch 65 Batch 300 Loss -1.1492\n",
      "Epoch 65 Batch 350 Loss -1.1585\n",
      "Epoch 65 Loss -1.1672\n",
      "{'Epoch': 65}\n",
      "Epoch 66 Batch 0 Loss -0.5223\n",
      "Epoch 66 Batch 50 Loss -1.1058\n",
      "Epoch 66 Batch 100 Loss -1.0528\n",
      "Epoch 66 Batch 150 Loss -1.0622\n",
      "Epoch 66 Batch 200 Loss -1.1036\n",
      "Epoch 66 Batch 250 Loss -1.1456\n",
      "Epoch 66 Batch 300 Loss -1.1503\n",
      "Epoch 66 Batch 350 Loss -1.1593\n",
      "Epoch 66 Loss -1.1685\n",
      "{'Epoch': 66}\n",
      "Epoch 67 Batch 0 Loss -0.5100\n",
      "Epoch 67 Batch 50 Loss -1.1033\n",
      "Epoch 67 Batch 100 Loss -1.0539\n",
      "Epoch 67 Batch 150 Loss -1.0644\n",
      "Epoch 67 Batch 200 Loss -1.1080\n",
      "Epoch 67 Batch 250 Loss -1.1498\n",
      "Epoch 67 Batch 300 Loss -1.1557\n",
      "Epoch 67 Batch 350 Loss -1.1644\n",
      "Epoch 67 Loss -1.1735\n",
      "{'Epoch': 67}\n",
      "Epoch 68 Batch 0 Loss -0.5147\n",
      "Epoch 68 Batch 50 Loss -1.0901\n",
      "Epoch 68 Batch 100 Loss -1.0411\n",
      "Epoch 68 Batch 150 Loss -1.0554\n",
      "Epoch 68 Batch 200 Loss -1.1022\n",
      "Epoch 68 Batch 250 Loss -1.1447\n",
      "Epoch 68 Batch 300 Loss -1.1500\n",
      "Epoch 68 Batch 350 Loss -1.1596\n",
      "Epoch 68 Loss -1.1691\n",
      "{'Epoch': 68}\n",
      "Epoch 69 Batch 0 Loss -0.5343\n",
      "Epoch 69 Batch 50 Loss -1.1202\n",
      "Epoch 69 Batch 100 Loss -1.0682\n",
      "Epoch 69 Batch 150 Loss -1.0751\n",
      "Epoch 69 Batch 200 Loss -1.1143\n",
      "Epoch 69 Batch 250 Loss -1.1532\n",
      "Epoch 69 Batch 300 Loss -1.1574\n",
      "Epoch 69 Batch 350 Loss -1.1670\n",
      "Epoch 69 Loss -1.1766\n",
      "{'Epoch': 69}\n",
      "Epoch 70 Batch 0 Loss -0.5305\n",
      "Epoch 70 Batch 50 Loss -1.1176\n",
      "Epoch 70 Batch 100 Loss -1.0638\n",
      "Epoch 70 Batch 150 Loss -1.0732\n",
      "Epoch 70 Batch 200 Loss -1.1160\n",
      "Epoch 70 Batch 250 Loss -1.1541\n",
      "Epoch 70 Batch 300 Loss -1.1588\n",
      "Epoch 70 Batch 350 Loss -1.1689\n",
      "Epoch 70 Loss -1.1789\n",
      "{'Epoch': 70}\n",
      "Epoch 71 Batch 0 Loss -0.5319\n",
      "Epoch 71 Batch 50 Loss -1.1211\n",
      "Epoch 71 Batch 100 Loss -1.0673\n",
      "Epoch 71 Batch 150 Loss -1.0775\n",
      "Epoch 71 Batch 200 Loss -1.1197\n",
      "Epoch 71 Batch 250 Loss -1.1615\n",
      "Epoch 71 Batch 300 Loss -1.1653\n",
      "Epoch 71 Batch 350 Loss -1.1750\n",
      "Epoch 71 Loss -1.1838\n",
      "{'Epoch': 71}\n",
      "Epoch 72 Batch 0 Loss -0.5550\n",
      "Epoch 72 Batch 50 Loss -1.1299\n",
      "Epoch 72 Batch 100 Loss -1.0759\n",
      "Epoch 72 Batch 150 Loss -1.0842\n",
      "Epoch 72 Batch 200 Loss -1.1270\n",
      "Epoch 72 Batch 250 Loss -1.1687\n",
      "Epoch 72 Batch 300 Loss -1.1745\n",
      "Epoch 72 Batch 350 Loss -1.1838\n",
      "Epoch 72 Loss -1.1926\n",
      "{'Epoch': 72}\n",
      "Epoch 73 Batch 0 Loss -0.5693\n",
      "Epoch 73 Batch 50 Loss -1.1322\n",
      "Epoch 73 Batch 100 Loss -1.0775\n",
      "Epoch 73 Batch 150 Loss -1.0842\n",
      "Epoch 73 Batch 200 Loss -1.1271\n",
      "Epoch 73 Batch 250 Loss -1.1686\n",
      "Epoch 73 Batch 300 Loss -1.1742\n",
      "Epoch 73 Batch 350 Loss -1.1830\n",
      "Epoch 73 Loss -1.1912\n",
      "{'Epoch': 73}\n",
      "Epoch 74 Batch 0 Loss -0.5624\n",
      "Epoch 74 Batch 50 Loss -1.1336\n",
      "Epoch 74 Batch 100 Loss -1.0801\n",
      "Epoch 74 Batch 150 Loss -1.0881\n",
      "Epoch 74 Batch 200 Loss -1.1305\n",
      "Epoch 74 Batch 250 Loss -1.1726\n",
      "Epoch 74 Batch 300 Loss -1.1780\n",
      "Epoch 74 Batch 350 Loss -1.1880\n",
      "Epoch 74 Loss -1.1963\n",
      "{'Epoch': 74}\n",
      "Epoch 75 Batch 0 Loss -0.5544\n",
      "Epoch 75 Batch 50 Loss -1.1389\n",
      "Epoch 75 Batch 100 Loss -1.0864\n",
      "Epoch 75 Batch 150 Loss -1.0929\n",
      "Epoch 75 Batch 200 Loss -1.1360\n",
      "Epoch 75 Batch 250 Loss -1.1761\n",
      "Epoch 75 Batch 300 Loss -1.1805\n",
      "Epoch 75 Batch 350 Loss -1.1886\n",
      "Epoch 75 Loss -1.1977\n",
      "{'Epoch': 75}\n",
      "Epoch 76 Batch 0 Loss -0.5464\n",
      "Epoch 76 Batch 50 Loss -1.1427\n",
      "Epoch 76 Batch 100 Loss -1.0907\n",
      "Epoch 76 Batch 150 Loss -1.0989\n",
      "Epoch 76 Batch 200 Loss -1.1412\n",
      "Epoch 76 Batch 250 Loss -1.1807\n",
      "Epoch 76 Batch 300 Loss -1.1842\n",
      "Epoch 76 Batch 350 Loss -1.1940\n",
      "Epoch 76 Loss -1.2032\n",
      "{'Epoch': 76}\n",
      "Epoch 77 Batch 0 Loss -0.5565\n",
      "Epoch 77 Batch 50 Loss -1.1365\n",
      "Epoch 77 Batch 100 Loss -1.0884\n",
      "Epoch 77 Batch 150 Loss -1.0969\n",
      "Epoch 77 Batch 200 Loss -1.1414\n",
      "Epoch 77 Batch 250 Loss -1.1827\n",
      "Epoch 77 Batch 300 Loss -1.1878\n",
      "Epoch 77 Batch 350 Loss -1.1971\n",
      "Epoch 77 Loss -1.2059\n",
      "{'Epoch': 77}\n",
      "Epoch 78 Batch 0 Loss -0.5707\n",
      "Epoch 78 Batch 50 Loss -1.1447\n",
      "Epoch 78 Batch 100 Loss -1.0924\n",
      "Epoch 78 Batch 150 Loss -1.0999\n",
      "Epoch 78 Batch 200 Loss -1.1440\n",
      "Epoch 78 Batch 250 Loss -1.1844\n",
      "Epoch 78 Batch 300 Loss -1.1894\n",
      "Epoch 78 Batch 350 Loss -1.2000\n",
      "Epoch 78 Loss -1.2088\n",
      "{'Epoch': 78}\n",
      "Epoch 79 Batch 0 Loss -0.5813\n",
      "Epoch 79 Batch 50 Loss -1.1547\n",
      "Epoch 79 Batch 100 Loss -1.1006\n",
      "Epoch 79 Batch 150 Loss -1.1074\n",
      "Epoch 79 Batch 200 Loss -1.1511\n",
      "Epoch 79 Batch 250 Loss -1.1922\n",
      "Epoch 79 Batch 300 Loss -1.1962\n",
      "Epoch 79 Batch 350 Loss -1.2048\n",
      "Epoch 79 Loss -1.2136\n",
      "{'Epoch': 79}\n",
      "Epoch 80 Batch 0 Loss -0.5843\n",
      "Epoch 80 Batch 50 Loss -1.1575\n",
      "Epoch 80 Batch 100 Loss -1.1020\n",
      "Epoch 80 Batch 150 Loss -1.1107\n",
      "Epoch 80 Batch 200 Loss -1.1533\n",
      "Epoch 80 Batch 250 Loss -1.1963\n",
      "Epoch 80 Batch 300 Loss -1.2023\n",
      "Epoch 80 Batch 350 Loss -1.2115\n",
      "Epoch 80 Loss -1.2187\n",
      "{'Epoch': 80}\n",
      "Epoch 81 Batch 0 Loss -0.5785\n",
      "Epoch 81 Batch 50 Loss -1.1513\n",
      "Epoch 81 Batch 100 Loss -1.1021\n",
      "Epoch 81 Batch 150 Loss -1.1115\n",
      "Epoch 81 Batch 200 Loss -1.1543\n",
      "Epoch 81 Batch 250 Loss -1.1961\n",
      "Epoch 81 Batch 300 Loss -1.2021\n",
      "Epoch 81 Batch 350 Loss -1.2116\n",
      "Epoch 81 Loss -1.2206\n",
      "{'Epoch': 81}\n",
      "Epoch 82 Batch 0 Loss -0.5642\n",
      "Epoch 82 Batch 50 Loss -1.1553\n",
      "Epoch 82 Batch 100 Loss -1.1056\n",
      "Epoch 82 Batch 150 Loss -1.1147\n",
      "Epoch 82 Batch 200 Loss -1.1590\n",
      "Epoch 82 Batch 250 Loss -1.2013\n",
      "Epoch 82 Batch 300 Loss -1.2063\n",
      "Epoch 82 Batch 350 Loss -1.2156\n",
      "Epoch 82 Loss -1.2247\n",
      "{'Epoch': 82}\n",
      "Epoch 83 Batch 0 Loss -0.5640\n",
      "Epoch 83 Batch 50 Loss -1.1618\n",
      "Epoch 83 Batch 100 Loss -1.1085\n",
      "Epoch 83 Batch 150 Loss -1.1153\n",
      "Epoch 83 Batch 200 Loss -1.1602\n",
      "Epoch 83 Batch 250 Loss -1.2019\n",
      "Epoch 83 Batch 300 Loss -1.2051\n",
      "Epoch 83 Batch 350 Loss -1.2137\n",
      "Epoch 83 Loss -1.2228\n",
      "{'Epoch': 83}\n",
      "Epoch 84 Batch 0 Loss -0.5661\n",
      "Epoch 84 Batch 50 Loss -1.1712\n",
      "Epoch 84 Batch 100 Loss -1.1194\n",
      "Epoch 84 Batch 150 Loss -1.1267\n",
      "Epoch 84 Batch 200 Loss -1.1695\n",
      "Epoch 84 Batch 250 Loss -1.2111\n",
      "Epoch 84 Batch 300 Loss -1.2150\n",
      "Epoch 84 Batch 350 Loss -1.2231\n",
      "Epoch 84 Loss -1.2324\n",
      "{'Epoch': 84}\n",
      "Epoch 85 Batch 0 Loss -0.6003\n",
      "Epoch 85 Batch 50 Loss -1.1680\n",
      "Epoch 85 Batch 100 Loss -1.1172\n",
      "Epoch 85 Batch 150 Loss -1.1251\n",
      "Epoch 85 Batch 200 Loss -1.1700\n",
      "Epoch 85 Batch 250 Loss -1.2116\n",
      "Epoch 85 Batch 300 Loss -1.2164\n",
      "Epoch 85 Batch 350 Loss -1.2256\n",
      "Epoch 85 Loss -1.2343\n",
      "{'Epoch': 85}\n",
      "Epoch 86 Batch 0 Loss -0.6058\n",
      "Epoch 86 Batch 50 Loss -1.1796\n",
      "Epoch 86 Batch 100 Loss -1.1234\n",
      "Epoch 86 Batch 150 Loss -1.1297\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/don/Writing-Transformer/Writing-Transformer.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/don/Writing-Transformer/Writing-Transformer.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m (batch, (b, _)) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data_iter):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/don/Writing-Transformer/Writing-Transformer.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m   params, opt_state, loss, rng \u001b[39m=\u001b[39m update(params, rng, opt_state, b)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/don/Writing-Transformer/Writing-Transformer.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m   train_loss(loss)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/don/Writing-Transformer/Writing-Transformer.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m   \u001b[39mif\u001b[39;00m batch \u001b[39m%\u001b[39m \u001b[39m50\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/don/Writing-Transformer/Writing-Transformer.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mstep \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m Batch \u001b[39m\u001b[39m{\u001b[39;00mbatch\u001b[39m}\u001b[39;00m\u001b[39m Loss \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m.\u001b[39mresult()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/keras/metrics/base_metric.py:213\u001b[0m, in \u001b[0;36mMetric.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[39mreturn\u001b[39;00m result_t\n\u001b[1;32m    209\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistribute\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m    210\u001b[0m     distributed_training_utils,\n\u001b[1;32m    211\u001b[0m )\n\u001b[0;32m--> 213\u001b[0m \u001b[39mreturn\u001b[39;00m distributed_training_utils\u001b[39m.\u001b[39;49mcall_replica_local_fn(\n\u001b[1;32m    214\u001b[0m     replica_local_fn, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    215\u001b[0m )\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/keras/distribute/distributed_training_utils.py:60\u001b[0m, in \u001b[0;36mcall_replica_local_fn\u001b[0;34m(fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[39mwith\u001b[39;00m strategy\u001b[39m.\u001b[39mscope():\n\u001b[1;32m     59\u001b[0m         \u001b[39mreturn\u001b[39;00m strategy\u001b[39m.\u001b[39mextended\u001b[39m.\u001b[39mcall_for_each_replica(fn, args, kwargs)\n\u001b[0;32m---> 60\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/keras/metrics/base_metric.py:190\u001b[0m, in \u001b[0;36mMetric.__call__.<locals>.replica_local_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m     update_op \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     update_op \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_state(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    191\u001b[0m update_ops \u001b[39m=\u001b[39m []\n\u001b[1;32m    192\u001b[0m \u001b[39mif\u001b[39;00m update_op \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/keras/utils/metrics_utils.py:77\u001b[0m, in \u001b[0;36mupdate_state_wrapper.<locals>.decorated\u001b[0;34m(metric_obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     70\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mTrying to run metric.update_state in replica context when \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mthe metric was not created in TPUStrategy scope. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMake sure the keras Metric is created in TPUstrategy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mscope. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m         )\n\u001b[1;32m     76\u001b[0m \u001b[39mwith\u001b[39;00m tf_utils\u001b[39m.\u001b[39mgraph_context_for_symbolic_tensors(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 77\u001b[0m     update_op \u001b[39m=\u001b[39m update_state_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     78\u001b[0m \u001b[39mif\u001b[39;00m update_op \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# update_op will be None in eager execution.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     metric_obj\u001b[39m.\u001b[39madd_update(update_op)\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/keras/metrics/base_metric.py:143\u001b[0m, in \u001b[0;36mMetric.__new__.<locals>.update_state_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    139\u001b[0m control_status \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mcontrol_status_ctx()\n\u001b[1;32m    140\u001b[0m ag_update_state \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mautograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m    141\u001b[0m     obj_update_state, control_status\n\u001b[1;32m    142\u001b[0m )\n\u001b[0;32m--> 143\u001b[0m \u001b[39mreturn\u001b[39;00m ag_update_state(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:689\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 689\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m    690\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    691\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:331\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[39mif\u001b[39;00m conversion\u001b[39m.\u001b[39mis_in_allowlist_cache(f, options):\n\u001b[1;32m    330\u001b[0m   logging\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAllowlisted \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: from cache\u001b[39m\u001b[39m'\u001b[39m, f)\n\u001b[0;32m--> 331\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m ag_ctx\u001b[39m.\u001b[39mcontrol_status_ctx()\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m ag_ctx\u001b[39m.\u001b[39mStatus\u001b[39m.\u001b[39mDISABLED:\n\u001b[1;32m    334\u001b[0m   logging\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAllowlisted: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: AutoGraph is disabled in context\u001b[39m\u001b[39m'\u001b[39m, f)\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py:458\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    455\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    457\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 458\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    459\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/keras/metrics/base_metric.py:462\u001b[0m, in \u001b[0;36mReduce.update_state\u001b[0;34m(self, values, sample_weight)\u001b[0m\n\u001b[1;32m    456\u001b[0m [\n\u001b[1;32m    457\u001b[0m     values\n\u001b[1;32m    458\u001b[0m ], sample_weight \u001b[39m=\u001b[39m metrics_utils\u001b[39m.\u001b[39mragged_assert_compatible_and_get_flat_values(  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[1;32m    459\u001b[0m     [values], sample_weight\n\u001b[1;32m    460\u001b[0m )\n\u001b[1;32m    461\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 462\u001b[0m     values \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mcast(values, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dtype)\n\u001b[1;32m    463\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[1;32m    464\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    465\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe output of a metric function can only be a single Tensor. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    466\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReceived: \u001b[39m\u001b[39m{\u001b[39;00mvalues\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    467\u001b[0m     )\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/tensorflow/python/ops/math_ops.py:1001\u001b[0m, in \u001b[0;36mcast\u001b[0;34m(x, dtype, name)\u001b[0m\n\u001b[1;32m    995\u001b[0m   x \u001b[39m=\u001b[39m indexed_slices\u001b[39m.\u001b[39mIndexedSlices(values_cast, x\u001b[39m.\u001b[39mindices, x\u001b[39m.\u001b[39mdense_shape)\n\u001b[1;32m    996\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    997\u001b[0m   \u001b[39m# TODO(josh11b): If x is not already a Tensor, we could return\u001b[39;00m\n\u001b[1;32m    998\u001b[0m   \u001b[39m# ops.convert_to_tensor(x, dtype=dtype, ...)  here, but that\u001b[39;00m\n\u001b[1;32m    999\u001b[0m   \u001b[39m# allows some conversions that cast() can't do, e.g. casting numbers to\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m   \u001b[39m# strings.\u001b[39;00m\n\u001b[0;32m-> 1001\u001b[0m   x \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(x, name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1002\u001b[0m   \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m base_type:\n\u001b[1;32m   1003\u001b[0m     x \u001b[39m=\u001b[39m gen_math_ops\u001b[39m.\u001b[39mcast(x, base_type, name\u001b[39m=\u001b[39mname)\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1638\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1629\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1630\u001b[0m           _add_error_prefix(\n\u001b[1;32m   1631\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1634\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1635\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[1;32m   1637\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1638\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[1;32m   1640\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1641\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:343\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    341\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    342\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[0;32m--> 343\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[1;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    172\u001b[0m   \u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \n\u001b[1;32m    174\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    268\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39m\"\u001b[39m\u001b[39mtf.constant\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    278\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 279\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m    281\u001b[0m g \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_default_graph()\n\u001b[1;32m    282\u001b[0m tensor_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    303\u001b[0m   \u001b[39m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m   t \u001b[39m=\u001b[39m convert_to_eager_tensor(value, ctx, dtype)\n\u001b[1;32m    305\u001b[0m   \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "File \u001b[0;32m~/ML-tests/lib/python3.8/site-packages/jax/_src/device_array.py:266\u001b[0m, in \u001b[0;36m__array__\u001b[0;34m(self, dtype, context)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, context\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 266\u001b[0m   \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_value, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#EPOCHS = 200\n",
    "\n",
    "#LEARNING_RATE = 0.00001\n",
    "#optimiser = optax.chain(\n",
    "#      optax.clip_by_global_norm(GRAD_CLIP_VALUE),\n",
    " #     optax.adam(LEARNING_RATE, b1=0.9, b2=0.99),\n",
    " # )\n",
    "#opt_state = optimiser.init(params)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "for step in range(EPOCHS):\n",
    "  data_iter = train.batched_onehot_set.prefetch(tf.data.experimental.AUTOTUNE).as_numpy_iterator()\n",
    "\n",
    "  train_loss.reset_states()\n",
    "\n",
    "  print({\"Epoch\": step})\n",
    "  for (batch, (b, _)) in enumerate(data_iter):\n",
    "    params, opt_state, loss, rng = update(params, rng, opt_state, b)\n",
    "\n",
    "    train_loss(loss)\n",
    "\n",
    "    if batch % 50 == 0:\n",
    "      print(f'Epoch {step + 1} Batch {batch} Loss {train_loss.result():.4f}')\n",
    "\n",
    "  print(f'Epoch {step + 1} Loss {train_loss.result():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need our own sampling function in this case\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "\n",
    "def fill_diagonal(a, val):\n",
    "  assert a.ndim >= 2\n",
    "  i, j = jnp.diag_indices(min(a.shape[-2:]))\n",
    "  return a.at[..., i, j].set(val)\n",
    "\n",
    "# sample the mixture model\n",
    "# input: res (mixture_components)\n",
    "#        b (temperature)\n",
    "# output: sample, pis, mean, variance\n",
    "def sample_mix_model(res, rng, b):\n",
    "      #print(res.shape)\n",
    "      \n",
    "      pis, mu, sig, rho, eos = jnp.array_split(res, [NUM_MIX_COM, NUM_MIX_COM*3, NUM_MIX_COM*5, NUM_MIX_COM*6], axis=-1)\n",
    "\n",
    "      # weights - must be a probability distribution so softmax over all components\n",
    "      pis = jax.nn.softmax(pis * (1.+b))\n",
    "\n",
    "\n",
    "      # means - no transformation needed\n",
    "      mu_x1, mu_x2 = jnp.array_split(mu, 2, axis=-1)\n",
    "      \n",
    "      # standard deviations - must be strictly positive so exponent\n",
    "      sig = jnp.exp(sig - b)\n",
    "\n",
    "      sig_x1, sig_x2 = jnp.array_split(sig, 2, axis=-1)\n",
    "            \n",
    "      # correlations - squish to -1 to 1 with tanh activation\n",
    "      rho = jnp.tanh(rho)\n",
    "\n",
    "      a = jnp.zeros((NUM_MIX_COM, 2, 2))\n",
    "\n",
    "      S = fill_diagonal(a, jnp.stack([sig_x1, sig_x2], axis=-1))\n",
    "\n",
    "      #print(S)\n",
    "\n",
    "      #print(S.shape)\n",
    "\n",
    "      #E = jnp.eye(2, batch_shape=[NUM_MIX_COM])\n",
    "      E = jnp.repeat(jnp.eye(2)[None, :], NUM_MIX_COM, axis=0)\n",
    "\n",
    "      rho_exp = jnp.reshape(jnp.repeat(rho, 4), [NUM_MIX_COM, 2, 2])\n",
    "    \n",
    "      corr_mat = jnp.where(jnp.equal(E, 1.), E, rho_exp)\n",
    "      \n",
    "      cov_mat = jnp.matmul(S, corr_mat)\n",
    "      cov_mat = jnp.matmul(cov_mat, S)\n",
    "\n",
    "      #print(cov_mat)\n",
    "\n",
    "      #print(cov_mat.shape)\n",
    "\n",
    "      # The distribution is a mixture of gaussians\n",
    "      gm = tfp.distributions.MixtureSameFamily(mixture_distribution=tfp.distributions.Categorical(probs=pis),\n",
    "            components_distribution=tfp.distributions.MultivariateNormalTriL(loc=jnp.stack([mu_x1, mu_x2], axis=-1),\n",
    "                                                                    scale_tril=jax.lax.linalg.cholesky(cov_mat)))\n",
    "\n",
    "      # End of stroke\n",
    "      eos = jax.nn.sigmoid(eos)\n",
    "      \n",
    "      bd = tfp.distributions.Bernoulli(probs=eos, dtype=float)\n",
    "\n",
    "      #print(bd.sample(seed=jax.random.PRNGKey(seed=1)))\n",
    "      \n",
    "      rng, new_rng = jax.random.split(rng)\n",
    "\n",
    "      bd_samp = bd.sample(seed=new_rng)\n",
    "      \n",
    "      #print(tf.concat([gm.sample(), eos], axis=-1))\n",
    "\n",
    "      rng, new_rng = jax.random.split(rng)\n",
    "      \n",
    "      gm_samp = gm.sample(seed=new_rng)\n",
    "\n",
    "      return jnp.hstack((gm_samp, bd_samp, gm.mean(), gm.covariance().ravel(), pis)), rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([-1.8900048e-02, -3.3042753e-01,  0.0000000e+00,\n",
       "             -1.8898826e-02, -3.2954726e-01,  2.7309941e-05,\n",
       "             -6.3517196e-05, -6.3517196e-05,  3.7825466e-04,\n",
       "              4.3236162e-04,  1.2162946e-10,  1.2430813e-13,\n",
       "              5.7854582e-20,  4.3210040e-11,  9.7065145e-10,\n",
       "              1.6838317e-27,  2.7926050e-09,  9.9632746e-01,\n",
       "              1.1438686e-16,  2.2631118e-04,  6.9324026e-12,\n",
       "              1.0700577e-06,  5.8040485e-05,  1.0115902e-20,\n",
       "              2.9548528e-03,  5.8673964e-15,  8.0648543e-21,\n",
       "              8.7738051e-14,  5.4580181e-20], dtype=float32)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=10\n",
    "\n",
    "rng = jax.random.PRNGKey(5)\n",
    "rng, new_rng = jax.random.split(rng)\n",
    "dec_input = jnp.zeros((1, 1, 3))\n",
    "predictions_all = writing_transformer.apply(params, new_rng, one_hot_sentence, dec_input, False)\n",
    "\n",
    "predictions_all[0, -1, :]\n",
    "\n",
    "predictions, rng = sample_mix_model(predictions_all[0, -1, :], rng, b)\n",
    "\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sample_mix_model() missing 1 required positional argument: 'b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [257], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m42\u001b[39m) \n\u001b[1;32m      6\u001b[0m predictions_all \u001b[38;5;241m=\u001b[39m network\u001b[38;5;241m.\u001b[39mapply(params, key, one_hot_sentence, dec_input)\n\u001b[0;32m----> 8\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43msample_mix_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions_all\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m pred_strokes \u001b[38;5;241m=\u001b[39m predictions[:\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m     11\u001b[0m pred_strokes \u001b[38;5;241m=\u001b[39m pred_strokes[jnp\u001b[38;5;241m.\u001b[39mnewaxis, jnp\u001b[38;5;241m.\u001b[39mnewaxis, :]\n",
      "\u001b[0;31mTypeError\u001b[0m: sample_mix_model() missing 1 required positional argument: 'b'"
     ]
    }
   ],
   "source": [
    "# Test the sample function\n",
    "dec_input = jnp.zeros((1, 1, 3))\n",
    "one_hot_sentence = convert_sentence(encoding_sent)\n",
    "\n",
    "key = jax.random.PRNGKey(42) \n",
    "predictions_all = network.apply(params, key, one_hot_sentence, dec_input)\n",
    "\n",
    "predictions = sample_mix_model(predictions_all[0, -1, :], 1)\n",
    "\n",
    "pred_strokes = predictions[:3]\n",
    "pred_strokes = pred_strokes[jnp.newaxis, jnp.newaxis, :]\n",
    "\n",
    "pred_strokes.shape\n",
    "\n",
    "dec_input = jax.lax.concatenate([dec_input, pred_strokes], 1)\n",
    "#dec_input = jnp.stack([dec_input, pred_strokes])\n",
    "\n",
    "dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def fill_diagonal(a, val):\n",
    "  assert a.ndim >= 2\n",
    "  i, j = jnp.diag_indices(min(a.shape[-2:]))\n",
    "  return a.at[..., i, j].set(val)\n",
    "\n",
    "a = jnp.zeros((2, 3, 4, 4))\n",
    "\n",
    "# works for scalars\n",
    "a1 = fill_diagonal(a, 2)\n",
    "\n",
    "# or for batched vectors\n",
    "a2 = fill_diagonal(a, jnp.arange(24).reshape(2, 3, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4, 4)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need an evaluate function that will take in a character sequence and \n",
    "# generate some writing\n",
    "\n",
    "# Convert a sentence to a one-hot-encoded vector\n",
    "def convert_sentence(sentence):\n",
    "  # Convert it to a one-hot encoded vector for the encoder\n",
    "  U_conv = tf.keras.backend.one_hot(train.text_to_int(sentence), len(train.vocab)+1)\n",
    "  #U_conv = train.text_to_int(sentence)\n",
    "  # Pad it to match the original data that was input into the encoder\n",
    "  U_conv = tf.keras.preprocessing.sequence.pad_sequences([U_conv],\n",
    "                                                         maxlen=train.MAX_CHAR_SEQ_LEN,\n",
    "                                                         padding='post',\n",
    "                                                         value=train.char_padding_value);\n",
    "  #U_conv = tf.convert_to_tensor(U_conv, dtype='float32')\n",
    "\n",
    "  return jnp.asarray(U_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 100, 101)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_sent = 'Donald Brien'\n",
    "\n",
    "one_hot_sentence = convert_sentence(encoding_sent)\n",
    "\n",
    "one_hot_sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "@hk.transform\n",
    "def writing_transformer(inp: jnp.ndarray, tar: jnp.ndarray, training: bool) -> jnp.ndarray:\n",
    "    tra = Writing_Transformer(num_layers, key_size, d_model, num_heads, dff, pe_encoding=250, pe_target=1000, dropout_rate=dropout_rate)\n",
    "\n",
    "    return tra(inp, tar, training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "output_name = \"test_attention.mp4\"\n",
    "\n",
    "b = 5.0\n",
    "\n",
    "def evaluate(U_conv):\n",
    "  #gen_sequence = np.zeros((1, 3))\n",
    "\n",
    "  dec_input = jnp.zeros((1, 1, 3))\n",
    "\n",
    "  MAX_LEN = 500\n",
    "\n",
    " # transformer.reset_states()\n",
    "\n",
    "  #fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    " # ims = []\n",
    "\n",
    "  rng = jax.random.PRNGKey(50) \n",
    "\n",
    "#  for t in range(int(train.MAX_STROKE_LEN)):\n",
    "  for t in range(int(MAX_LEN)):\n",
    "    # Create masks.  Even in the inference stage we may create input that is \n",
    "    # padded, such as the one-hot_sentence, and we always need a look-ahead \n",
    "    # mask\n",
    "    #enc_padding_mask, combined_mask, dec_padding_mask = create_masks(U_conv, dec_input)\n",
    "\n",
    "    rng, new_rng = jax.random.split(rng)\n",
    "\n",
    "    predictions_all = writing_transformer.apply(params, new_rng, U_conv, dec_input, False)\n",
    "\n",
    "    #rng = new_rng\n",
    "\n",
    "    #predictions_all = transformer(U_conv,\n",
    "    #                                                     dec_input,\n",
    "     #                                                    False,\n",
    "     #                                                    enc_padding_mask,\n",
    "     #                                                    combined_mask,\n",
    "     #                                                    dec_padding_mask)\n",
    "\n",
    "    #data = tf.squeeze(attention_weights['decoder_layer1_block2'], 0)[0]\n",
    "  \n",
    "    #data_all = np.zeros((MAX_LEN, 20))\n",
    "\n",
    "    #data_all[:data.shape[0], :] = data\n",
    "\n",
    "    #ax = fig.add_subplot(1, 1, 1)\n",
    "    #im = plt.imshow(data_all, cmap='viridis', interpolation='nearest', aspect='auto', animated=True)\n",
    "\n",
    "    #ax = plt.gca()\n",
    "\n",
    "    #labels = 'Eye tracking....'\n",
    "\n",
    "    #ax.set_xticks(range(0, train.MAX_CHAR_SEQ_LEN-1))\n",
    "    #ax.set_xticklabels(labels)\n",
    "\n",
    "    #ax.set_xlabel('Characters to be Written')\n",
    "    #ax.set_ylabel('Stroke Number')\n",
    "\n",
    "    #ims.append([im])\n",
    "\n",
    "    predictions, rng = sample_mix_model(predictions_all[0, -1, :], rng, b)\n",
    "\n",
    "    pred_strokes = predictions[:3]\n",
    "    pred_strokes = pred_strokes[jnp.newaxis, jnp.newaxis, :]\n",
    "\n",
    "    #pred_strokes.shape\n",
    "\n",
    "    dec_input = jax.lax.concatenate([dec_input, pred_strokes], 1)\n",
    "\n",
    "    #print(dec_input.shape)\n",
    "\n",
    "    #print(dec_input.shape)\n",
    "\n",
    "  #ani = animation.ArtistAnimation(fig, ims, interval=50, blit=True,\n",
    "   #                             repeat_delay=1000)\n",
    "  \n",
    "  #ani.save(output_name)\n",
    "\n",
    "  #plt.show()\n",
    "\n",
    "  #return dec_input.numpy(), attention_weights\n",
    "  return dec_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate(one_hot_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "             [-1.68487933e-02, -1.51628200e-02,  0.00000000e+00],\n",
       "             [ 2.67114304e-03, -9.75223072e-03,  0.00000000e+00],\n",
       "             [ 8.46487284e-03, -2.21872795e-02,  0.00000000e+00],\n",
       "             [ 1.70229923e-03, -1.77664254e-02,  0.00000000e+00],\n",
       "             [-6.53663708e-04, -1.44730303e-02,  0.00000000e+00],\n",
       "             [-3.49887909e-04, -1.42878834e-02,  0.00000000e+00],\n",
       "             [-1.14641536e-03, -1.41803417e-02,  0.00000000e+00],\n",
       "             [-1.45932660e-03, -1.06922667e-02,  0.00000000e+00],\n",
       "             [-4.47778060e-04, -8.39947723e-03,  0.00000000e+00],\n",
       "             [ 1.78421004e-04, -6.85079955e-03,  0.00000000e+00],\n",
       "             [-8.73343961e-04, -6.41875155e-03,  0.00000000e+00],\n",
       "             [-1.17087550e-03, -7.55912997e-03,  0.00000000e+00],\n",
       "             [-6.96228875e-04, -9.50282253e-03,  0.00000000e+00],\n",
       "             [-3.63635161e-04, -9.92705300e-03,  0.00000000e+00],\n",
       "             [ 4.89076599e-04, -1.01954434e-02,  0.00000000e+00],\n",
       "             [ 1.07247825e-03, -9.95431282e-03,  0.00000000e+00],\n",
       "             [ 1.06854551e-03, -8.48473050e-03,  0.00000000e+00],\n",
       "             [ 9.29230766e-04, -7.94303603e-03,  0.00000000e+00],\n",
       "             [ 8.69163312e-04, -8.15217942e-03,  0.00000000e+00],\n",
       "             [ 7.23471865e-04, -8.29164125e-03,  0.00000000e+00],\n",
       "             [ 6.74235809e-04, -7.59854168e-03,  0.00000000e+00],\n",
       "             [ 7.66201003e-04, -7.26563670e-03,  0.00000000e+00],\n",
       "             [ 7.90303573e-04, -7.24614598e-03,  0.00000000e+00],\n",
       "             [ 8.89448449e-04, -7.77590461e-03,  0.00000000e+00],\n",
       "             [ 7.29327963e-04, -7.02379085e-03,  0.00000000e+00],\n",
       "             [ 4.28816275e-04, -6.47888519e-03,  0.00000000e+00],\n",
       "             [ 5.10511862e-04, -5.79281338e-03,  0.00000000e+00],\n",
       "             [ 2.69388052e-04, -4.52262722e-03,  0.00000000e+00],\n",
       "             [-1.24113678e-04, -2.70525552e-03,  0.00000000e+00],\n",
       "             [ 2.04285592e-04, -2.04486586e-03,  0.00000000e+00],\n",
       "             [-6.46207554e-05, -1.93102087e-03,  0.00000000e+00],\n",
       "             [-7.91711733e-04, -2.16166116e-03,  0.00000000e+00],\n",
       "             [-1.73613615e-03, -1.15570612e-03,  0.00000000e+00],\n",
       "             [-2.00414844e-03,  4.10517619e-04,  0.00000000e+00],\n",
       "             [-1.56063400e-03,  1.34055875e-03,  0.00000000e+00],\n",
       "             [-9.75312665e-04,  1.11024641e-03,  0.00000000e+00],\n",
       "             [-4.48597537e-04,  1.28941052e-03,  0.00000000e+00],\n",
       "             [ 2.34072955e-04,  1.24015473e-03,  0.00000000e+00],\n",
       "             [ 7.55654648e-04,  1.83079951e-03,  0.00000000e+00],\n",
       "             [ 5.74017002e-04,  3.48626636e-03,  0.00000000e+00],\n",
       "             [-3.10586824e-04,  4.53816913e-03,  0.00000000e+00],\n",
       "             [-3.26578098e-04,  3.20143625e-03,  0.00000000e+00],\n",
       "             [-2.08111553e-04,  1.56414323e-03,  0.00000000e+00],\n",
       "             [ 1.75414592e-04,  6.73962757e-04,  0.00000000e+00],\n",
       "             [ 1.16351806e-03,  4.72856627e-04,  0.00000000e+00],\n",
       "             [ 2.13733874e-03,  1.25720166e-03,  0.00000000e+00],\n",
       "             [ 2.39131413e-03,  2.47984193e-03,  0.00000000e+00],\n",
       "             [ 2.44857185e-03,  2.38650106e-03,  0.00000000e+00],\n",
       "             [ 1.97486021e-03,  8.45944509e-04,  0.00000000e+00],\n",
       "             [ 1.33518199e-03, -8.91810341e-04,  0.00000000e+00],\n",
       "             [ 1.01307151e-03, -1.24671496e-03,  0.00000000e+00],\n",
       "             [ 1.06049876e-03, -4.94720414e-04,  0.00000000e+00],\n",
       "             [ 2.18221731e-03,  4.97860892e-04,  0.00000000e+00],\n",
       "             [ 2.87432782e-03,  2.21579336e-03,  0.00000000e+00],\n",
       "             [ 2.80155055e-03,  3.10214423e-03,  0.00000000e+00],\n",
       "             [ 2.94845365e-03,  3.13690118e-03,  0.00000000e+00],\n",
       "             [ 2.54232995e-03,  2.96108238e-03,  0.00000000e+00],\n",
       "             [ 2.17028148e-03,  3.78852151e-03,  0.00000000e+00],\n",
       "             [ 2.17154995e-03,  5.44219743e-03,  0.00000000e+00],\n",
       "             [ 2.68496014e-03,  5.97842596e-03,  0.00000000e+00],\n",
       "             [ 3.64324264e-03,  5.25446050e-03,  0.00000000e+00],\n",
       "             [ 3.84145044e-03,  4.72874008e-03,  0.00000000e+00],\n",
       "             [ 3.05112265e-03,  4.37575392e-03,  0.00000000e+00],\n",
       "             [ 1.90577470e-03,  5.53750060e-03,  0.00000000e+00],\n",
       "             [ 1.13441620e-03,  7.67232291e-03,  0.00000000e+00],\n",
       "             [ 4.82415868e-04,  9.64420848e-03,  0.00000000e+00],\n",
       "             [ 7.45709840e-05,  9.83764790e-03,  0.00000000e+00],\n",
       "             [ 4.51968983e-04,  8.00709240e-03,  0.00000000e+00],\n",
       "             [ 7.68771453e-04,  6.22140430e-03,  0.00000000e+00],\n",
       "             [ 9.48587433e-04,  4.65639494e-03,  0.00000000e+00],\n",
       "             [ 3.88165907e-04,  4.73239087e-03,  0.00000000e+00],\n",
       "             [ 1.02518054e-04,  5.68284653e-03,  0.00000000e+00],\n",
       "             [-1.22217039e-04,  6.58639334e-03,  0.00000000e+00],\n",
       "             [-1.19713135e-03,  7.60385208e-03,  0.00000000e+00],\n",
       "             [-1.32976659e-03,  7.82990269e-03,  0.00000000e+00],\n",
       "             [-7.29585066e-04,  6.64683245e-03,  0.00000000e+00],\n",
       "             [-8.52348785e-06,  5.91380708e-03,  0.00000000e+00],\n",
       "             [ 5.65422641e-04,  6.86385296e-03,  0.00000000e+00],\n",
       "             [-2.45984702e-04,  8.80338810e-03,  0.00000000e+00],\n",
       "             [-5.82890585e-04,  9.08583961e-03,  0.00000000e+00],\n",
       "             [-6.30415336e-04,  8.03299062e-03,  0.00000000e+00],\n",
       "             [-7.22309574e-04,  7.06551038e-03,  0.00000000e+00],\n",
       "             [-2.43360162e-04,  7.08447956e-03,  0.00000000e+00],\n",
       "             [ 4.39770607e-04,  7.06584565e-03,  0.00000000e+00],\n",
       "             [ 7.59167597e-04,  6.49680756e-03,  0.00000000e+00],\n",
       "             [ 1.10941567e-03,  6.46727718e-03,  0.00000000e+00],\n",
       "             [ 2.46606860e-03,  6.09984435e-03,  0.00000000e+00],\n",
       "             [ 3.24860774e-03,  5.94008155e-03,  0.00000000e+00],\n",
       "             [ 3.81853245e-03,  5.97123615e-03,  0.00000000e+00],\n",
       "             [ 4.25212458e-03,  6.77774660e-03,  0.00000000e+00],\n",
       "             [ 4.45065089e-03,  8.31608661e-03,  0.00000000e+00],\n",
       "             [ 4.00404446e-03,  9.70624946e-03,  0.00000000e+00],\n",
       "             [ 2.78203189e-03,  1.03457514e-02,  0.00000000e+00],\n",
       "             [ 2.26153294e-03,  9.63919982e-03,  0.00000000e+00],\n",
       "             [ 2.89008580e-03,  8.88909958e-03,  0.00000000e+00],\n",
       "             [ 3.71676218e-03,  8.33042059e-03,  0.00000000e+00],\n",
       "             [ 4.74153273e-03,  8.16932879e-03,  0.00000000e+00],\n",
       "             [ 5.15058450e-03,  8.66772793e-03,  0.00000000e+00],\n",
       "             [ 4.88193147e-03,  9.41016339e-03,  0.00000000e+00],\n",
       "             [ 3.91362421e-03,  9.78454016e-03,  0.00000000e+00],\n",
       "             [ 2.65893899e-03,  1.02806259e-02,  0.00000000e+00],\n",
       "             [ 2.43680924e-03,  1.10925715e-02,  0.00000000e+00],\n",
       "             [ 2.96568312e-03,  1.18665900e-02,  0.00000000e+00],\n",
       "             [ 4.26974334e-03,  1.14604179e-02,  0.00000000e+00],\n",
       "             [ 5.96713088e-03,  1.07513163e-02,  0.00000000e+00],\n",
       "             [ 6.20594434e-03,  1.09335389e-02,  0.00000000e+00],\n",
       "             [ 5.43376617e-03,  1.17922481e-02,  0.00000000e+00],\n",
       "             [ 4.52140532e-03,  1.30658727e-02,  0.00000000e+00],\n",
       "             [ 3.36994790e-03,  1.36489179e-02,  0.00000000e+00],\n",
       "             [ 3.17467190e-03,  1.28914956e-02,  0.00000000e+00],\n",
       "             [ 3.37584876e-03,  1.16241183e-02,  0.00000000e+00],\n",
       "             [ 4.01305594e-03,  9.86327417e-03,  0.00000000e+00],\n",
       "             [ 3.64702009e-03,  8.74993391e-03,  0.00000000e+00],\n",
       "             [ 3.16616148e-03,  8.95422138e-03,  0.00000000e+00],\n",
       "             [ 2.88849883e-03,  1.01791788e-02,  0.00000000e+00],\n",
       "             [ 2.41215527e-03,  1.28050875e-02,  0.00000000e+00],\n",
       "             [ 1.97484903e-03,  1.46153774e-02,  0.00000000e+00],\n",
       "             [ 2.29690038e-03,  1.46299209e-02,  0.00000000e+00],\n",
       "             [ 2.98657827e-03,  1.33740809e-02,  0.00000000e+00],\n",
       "             [ 3.19037400e-03,  1.26229320e-02,  0.00000000e+00],\n",
       "             [ 2.98958085e-03,  1.33526828e-02,  0.00000000e+00],\n",
       "             [ 2.54770182e-03,  1.48163047e-02,  0.00000000e+00],\n",
       "             [ 1.82208233e-03,  1.55358519e-02,  0.00000000e+00],\n",
       "             [ 8.80843087e-04,  1.56991985e-02,  0.00000000e+00],\n",
       "             [ 2.45718285e-04,  1.61580015e-02,  0.00000000e+00],\n",
       "             [ 6.75318763e-04,  1.68433096e-02,  0.00000000e+00],\n",
       "             [ 1.18411146e-03,  1.69785824e-02,  0.00000000e+00],\n",
       "             [ 1.50486641e-03,  1.69872548e-02,  0.00000000e+00],\n",
       "             [ 1.70592405e-03,  1.60468835e-02,  0.00000000e+00],\n",
       "             [ 1.71621703e-03,  1.54531468e-02,  0.00000000e+00],\n",
       "             [ 1.86478160e-03,  1.55427661e-02,  0.00000000e+00],\n",
       "             [ 2.01257132e-03,  1.58862974e-02,  0.00000000e+00],\n",
       "             [ 2.71852873e-03,  1.58084836e-02,  0.00000000e+00],\n",
       "             [ 3.13563459e-03,  1.60799939e-02,  0.00000000e+00],\n",
       "             [ 3.02752666e-03,  1.62634160e-02,  0.00000000e+00],\n",
       "             [ 2.58555077e-03,  1.59054007e-02,  0.00000000e+00],\n",
       "             [ 1.65620632e-03,  1.37510877e-02,  0.00000000e+00],\n",
       "             [ 4.69809427e-04,  1.21197607e-02,  0.00000000e+00],\n",
       "             [ 1.87156737e-04,  1.14539582e-02,  0.00000000e+00],\n",
       "             [-1.73912616e-04,  1.22616757e-02,  0.00000000e+00],\n",
       "             [-4.86457662e-04,  1.35970209e-02,  0.00000000e+00],\n",
       "             [-1.62098743e-03,  1.45774800e-02,  0.00000000e+00],\n",
       "             [-2.12717243e-03,  1.45649035e-02,  0.00000000e+00],\n",
       "             [-2.33120658e-03,  1.41813923e-02,  0.00000000e+00],\n",
       "             [-1.92468800e-03,  1.18308570e-02,  0.00000000e+00],\n",
       "             [-2.42697634e-03,  1.15873720e-02,  0.00000000e+00],\n",
       "             [-2.57508270e-03,  1.26260202e-02,  0.00000000e+00],\n",
       "             [-2.06645019e-03,  1.31456088e-02,  0.00000000e+00],\n",
       "             [-1.15746818e-03,  1.28194634e-02,  0.00000000e+00],\n",
       "             [-6.29993156e-04,  1.32612493e-02,  0.00000000e+00],\n",
       "             [-8.74755904e-04,  1.32846478e-02,  0.00000000e+00],\n",
       "             [-5.71640267e-04,  1.02659483e-02,  0.00000000e+00],\n",
       "             [-9.30948649e-04,  8.04346893e-03,  0.00000000e+00],\n",
       "             [-1.30019523e-03,  7.64333270e-03,  0.00000000e+00],\n",
       "             [-1.27381692e-03,  8.08811933e-03,  0.00000000e+00],\n",
       "             [-1.66052394e-03,  8.00790079e-03,  0.00000000e+00],\n",
       "             [-2.32676230e-03,  8.18906911e-03,  0.00000000e+00],\n",
       "             [-2.57737748e-03,  8.81934725e-03,  0.00000000e+00],\n",
       "             [-2.64560245e-03,  9.43817757e-03,  0.00000000e+00],\n",
       "             [-3.43598984e-03,  1.05957706e-02,  0.00000000e+00],\n",
       "             [-3.83112021e-03,  1.07377674e-02,  0.00000000e+00],\n",
       "             [-3.35130282e-03,  1.01649668e-02,  0.00000000e+00],\n",
       "             [-2.48320960e-03,  9.73667763e-03,  0.00000000e+00],\n",
       "             [-2.18080543e-03,  1.07610319e-02,  0.00000000e+00],\n",
       "             [-3.12897749e-03,  1.39210392e-02,  0.00000000e+00],\n",
       "             [-4.16739844e-03,  1.66790690e-02,  0.00000000e+00],\n",
       "             [-5.03195263e-03,  1.78654399e-02,  0.00000000e+00],\n",
       "             [-5.68459742e-03,  1.79123152e-02,  0.00000000e+00],\n",
       "             [-6.74908049e-03,  1.76050309e-02,  0.00000000e+00],\n",
       "             [-7.18600862e-03,  1.58668477e-02,  0.00000000e+00],\n",
       "             [-6.85715862e-03,  1.43404212e-02,  0.00000000e+00],\n",
       "             [-6.03915937e-03,  1.35452393e-02,  0.00000000e+00],\n",
       "             [-4.81001846e-03,  1.32181253e-02,  0.00000000e+00],\n",
       "             [-3.33685614e-03,  1.40094403e-02,  0.00000000e+00],\n",
       "             [-2.98018195e-03,  1.51934233e-02,  0.00000000e+00],\n",
       "             [-3.07569839e-03,  1.54428985e-02,  0.00000000e+00],\n",
       "             [-3.92969884e-03,  1.54828634e-02,  0.00000000e+00],\n",
       "             [-4.58664354e-03,  1.55152278e-02,  0.00000000e+00],\n",
       "             [-5.37799485e-03,  1.55902412e-02,  0.00000000e+00],\n",
       "             [-5.51826693e-03,  1.50765311e-02,  0.00000000e+00],\n",
       "             [-5.23222424e-03,  1.48897376e-02,  0.00000000e+00],\n",
       "             [-4.41137888e-03,  1.56530607e-02,  0.00000000e+00],\n",
       "             [-3.21892509e-03,  1.62379593e-02,  0.00000000e+00],\n",
       "             [-3.26334126e-03,  1.60209183e-02,  0.00000000e+00],\n",
       "             [-4.45302390e-03,  1.57430116e-02,  0.00000000e+00],\n",
       "             [-5.91654889e-03,  1.56411622e-02,  0.00000000e+00],\n",
       "             [-6.97277300e-03,  1.49518866e-02,  0.00000000e+00],\n",
       "             [-7.73851387e-03,  1.43650826e-02,  0.00000000e+00],\n",
       "             [-8.14802758e-03,  1.49841215e-02,  0.00000000e+00],\n",
       "             [-8.11993517e-03,  1.65324118e-02,  0.00000000e+00],\n",
       "             [-7.86906295e-03,  1.83555577e-02,  0.00000000e+00],\n",
       "             [-8.09801929e-03,  1.99089814e-02,  0.00000000e+00],\n",
       "             [-9.12624784e-03,  2.15129498e-02,  0.00000000e+00],\n",
       "             [-9.75306518e-03,  2.25796308e-02,  0.00000000e+00],\n",
       "             [-1.09339450e-02,  2.28912141e-02,  1.00000000e+00],\n",
       "             [ 1.31878114e+00,  3.97691274e+00,  0.00000000e+00],\n",
       "             [ 1.06548611e-02, -2.31785886e-03,  0.00000000e+00],\n",
       "             [ 2.83695385e-02, -6.44676536e-02,  0.00000000e+00],\n",
       "             [ 2.13259049e-02, -1.37979612e-01,  0.00000000e+00],\n",
       "             [ 1.08223669e-02, -2.01099604e-01,  0.00000000e+00],\n",
       "             [ 1.81597518e-03, -2.79430032e-01,  0.00000000e+00],\n",
       "             [-7.06507219e-03, -3.52876753e-01,  0.00000000e+00],\n",
       "             [-8.75443965e-03, -4.13334876e-01,  0.00000000e+00],\n",
       "             [-6.02507545e-03, -4.58454132e-01,  0.00000000e+00],\n",
       "             [ 5.71676181e-04, -4.89628285e-01,  0.00000000e+00],\n",
       "             [ 9.39575583e-03, -5.13225138e-01,  0.00000000e+00],\n",
       "             [ 1.86619852e-02, -5.34164727e-01,  0.00000000e+00],\n",
       "             [ 2.75882035e-02, -5.49677908e-01,  0.00000000e+00],\n",
       "             [ 3.88823077e-02, -5.53545535e-01,  0.00000000e+00],\n",
       "             [ 5.27505949e-02, -5.39402127e-01,  0.00000000e+00],\n",
       "             [ 6.74956664e-02, -4.99591291e-01,  0.00000000e+00],\n",
       "             [ 8.24902058e-02, -4.31578875e-01,  0.00000000e+00],\n",
       "             [ 9.96105447e-02, -3.38700294e-01,  0.00000000e+00],\n",
       "             [ 1.21382937e-01, -2.22649008e-01,  0.00000000e+00],\n",
       "             [ 1.44455895e-01, -8.99683759e-02,  0.00000000e+00],\n",
       "             [ 1.83088943e-01,  4.08962592e-02,  0.00000000e+00],\n",
       "             [ 2.44357064e-01,  2.28377864e-01,  0.00000000e+00],\n",
       "             [ 2.88286358e-01,  4.51247543e-01,  0.00000000e+00],\n",
       "             [ 3.14606607e-01,  6.54166222e-01,  0.00000000e+00],\n",
       "             [ 3.38046223e-01,  8.14845443e-01,  0.00000000e+00],\n",
       "             [ 3.32422465e-01,  9.11374211e-01,  0.00000000e+00],\n",
       "             [ 3.06001246e-01,  9.58232522e-01,  0.00000000e+00],\n",
       "             [ 2.60355026e-01,  9.53650713e-01,  0.00000000e+00],\n",
       "             [ 1.99582458e-01,  8.95785689e-01,  0.00000000e+00],\n",
       "             [ 1.21048175e-01,  8.00233841e-01,  0.00000000e+00],\n",
       "             [ 2.07744390e-02,  6.74179375e-01,  0.00000000e+00],\n",
       "             [-7.31836632e-02,  5.26292086e-01,  0.00000000e+00],\n",
       "             [-1.39711723e-01,  3.57843220e-01,  0.00000000e+00],\n",
       "             [-1.75452828e-01,  1.87121600e-01,  0.00000000e+00],\n",
       "             [-1.85098901e-01,  5.78795895e-02,  0.00000000e+00],\n",
       "             [-1.74701005e-01, -5.54766431e-02,  0.00000000e+00],\n",
       "             [-1.58657894e-01, -1.91786215e-01,  0.00000000e+00],\n",
       "             [-1.50594220e-01, -3.59010696e-01,  0.00000000e+00],\n",
       "             [-1.26184672e-01, -5.16809165e-01,  0.00000000e+00],\n",
       "             [-8.60238373e-02, -6.37671828e-01,  0.00000000e+00],\n",
       "             [-4.36499491e-02, -7.17697620e-01,  0.00000000e+00],\n",
       "             [-1.32458995e-03, -7.65079856e-01,  0.00000000e+00],\n",
       "             [ 4.73012365e-02, -7.83412695e-01,  0.00000000e+00],\n",
       "             [ 9.56876203e-02, -7.72015154e-01,  0.00000000e+00],\n",
       "             [ 1.41853243e-01, -7.16876328e-01,  0.00000000e+00],\n",
       "             [ 1.83900923e-01, -6.23000741e-01,  0.00000000e+00],\n",
       "             [ 2.24267393e-01, -4.86466318e-01,  0.00000000e+00],\n",
       "             [ 2.71595925e-01, -3.16248119e-01,  0.00000000e+00],\n",
       "             [ 3.30172151e-01, -1.33641347e-01,  0.00000000e+00],\n",
       "             [ 3.95955265e-01,  6.29582033e-02,  0.00000000e+00],\n",
       "             [ 4.71492767e-01,  3.03790629e-01,  0.00000000e+00],\n",
       "             [ 5.12972474e-01,  5.61632693e-01,  0.00000000e+00],\n",
       "             [ 5.36451161e-01,  7.49231100e-01,  0.00000000e+00],\n",
       "             [ 5.30616820e-01,  8.72476041e-01,  0.00000000e+00],\n",
       "             [ 5.04800200e-01,  9.36312318e-01,  0.00000000e+00]],            dtype=float32)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_stroke(results[0, :500, :], one_hot_sentence[0, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('ML-tests': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b163f0480e5ecb2ed0d5f04556597a429b2f653f97b785151c27d861fe015d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
